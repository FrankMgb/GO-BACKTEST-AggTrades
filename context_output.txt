PS Z:\AggTrade-GO> go run . test
>>> CONTINUOUS-TIME ALGO DISCOVERY (OOS REPORT) <<<
   Workers: 22 | Models: 4
Done. Processed 2167 days in 1m39.3798874s. OOS report saved to Continuous_Algo_Report_OOS.txt
PS Z:\AggTrade-GO> go run . probe
>>> GNC DATA PROBE <<<
BaseDir: Z:\DATA\data

SYMBOL     IDX_DAYS  SAMPLED  OK  FAIL  FIRST_DAY   LAST_DAY    MIN_ROWS  MAX_ROWS  AVG_ROWS   
------     --------  -------  --  ----  ---------   --------    --------  --------  --------   
ASTERUSDT  79        16       16  0     2025-09-19  2025-12-06  295050    3300036   1007964    
BNBUSDT    2127      16       16  0     2020-02-10  2025-12-06  29604     1470641   321734     
BTCUSDT    2167      16       16  0     2020-01-01  2025-12-06  71359     3143146   1304226    
DOGEUSDT   1976      16       16  0     2020-07-10  2025-12-06  27516     1538948   355863     
ETHUSDT    2167      16       16  0     2020-01-01  2025-12-06  24671     4050677   1128213    
HYPEUSDT   191       16       16  0     2025-05-30  2025-12-06  156754    744158    461923     
SOLUSDT    1910      16       16  0     2020-09-14  2025-12-06  51575     2017706   613836     
XRPUSDT    2162      16       16  0     2020-01-06  2025-12-06  24898     1062084   283625     

[probe] Finished in 2.3450962s
PS Z:\AggTrade-GO>


PS Z:\AggTrade-GO> go run . test
>>> CONTINUOUS-TIME ALGO DISCOVERY (OOS REPORT) <<<
   Workers: 22 | Models: 4
Done. Processed 2167 days in 1m39.3798874s. OOS report saved to Continuous_Algo_Report_OOS.txt
PS Z:\AggTrade-GO> go run . probe
>>> GNC DATA PROBE <<<
BaseDir: Z:\DATA\data

SYMBOL     IDX_DAYS  SAMPLED  OK  FAIL  FIRST_DAY   LAST_DAY    MIN_ROWS  MAX_ROWS  AVG_ROWS
------     --------  -------  --  ----  ---------   --------    --------  --------  --------
ASTERUSDT  79        16       16  0     2025-09-19  2025-12-06  295050    3300036   1007964
BNBUSDT    2127      16       16  0     2020-02-10  2025-12-06  29604     1470641   321734
BTCUSDT    2167      16       16  0     2020-01-01  2025-12-06  71359     3143146   1304226
DOGEUSDT   1976      16       16  0     2020-07-10  2025-12-06  27516     1538948   355863
ETHUSDT    2167      16       16  0     2020-01-01  2025-12-06  24671     4050677   1128213
HYPEUSDT   191       16       16  0     2025-05-30  2025-12-06  156754    744158    461923     
SOLUSDT    1910      16       16  0     2020-09-14  2025-12-06  51575     2017706   613836     
XRPUSDT    2162      16       16  0     2020-01-06  2025-12-06  24898     1062084   283625     

[probe] Finished in 2.3450962s
PS Z:\AggTrade-GO> 

MODEL            HORIZON TrainN  TestN  PearsonIC SpearmanIC  HitRate HitZ  Sharpe Spread(bps) TopDecile(bps) BotDecile(bps)  MI(bits) NMI   ΔLogLoss
-----            ------- ------  -----  --------- ----------- ------- ----  ------ ----------- -------------- --------------- -------- ---   --------
Hawkes_Intensity 15m     2091642 896419 0.0126    0.0030      0.502   3.92  0.004  +0.6        +0.7           +0.1            0.050    0.015 0.0000
Hawkes_Intensity 30m     2091642 896419 0.0128    0.0052      0.504   7.18  0.006  +1.0        +1.3           +0.3            0.045    0.014 0.0000
Hawkes_Intensity 1h      2091642 896419 0.0120    0.0070      0.508   15.67 0.009  +1.3        +1.8           +0.5            0.040    0.012 0.0000

Hawkes_OFI 15m 2091642 896419 -0.0135 -0.0421 0.482 -33.93 -0.013 -1.0 -0.3 +0.8 0.037 0.011 0.0005
Hawkes_OFI 30m 2091642 896419 -0.0105 -0.0355 0.486 -25.75 -0.010 -1.1 -0.1 +1.0 0.033 0.010 0.0002
Hawkes_OFI 1h  2091642 896419 -0.0142 -0.0352 0.486 -26.90 -0.011 -2.2 -0.4 +1.8 0.029 0.009 0.0004

Sig_LevyArea 15m 2091642 896419 0.0169 0.0066 0.502 3.95  0.004 +0.4 +0.6 +0.2 0.110 0.033 0.0000
Sig_LevyArea 30m 2091642 896419 0.0193 0.0080 0.504 7.17  0.006 +0.5 +0.9 +0.5 0.102 0.031 0.0000
Sig_LevyArea 1h  2091642 896419 0.0209 0.0102 0.508 15.64 0.009 +0.4 +1.5 +1.0 0.091 0.027 0.0000

Hilbert_Phase 15m 2091642 896419 -0.0096 -0.0299 0.483 -31.75 -0.009 -0.5 -0.1 +0.4 0.002 0.001 0.0003
Hilbert_Phase 30m 2091642 896419 -0.0047 -0.0249 0.486 -26.74 -0.005 -0.4 +0.2 +0.6 0.001 0.000 0.0000
Hilbert_Phase 1h  2091642 896419 -0.0081 -0.0251 0.485 -27.67 -0.008 -0.8 +0.2 +1.0 0.001 0.000 0.0003



# Rolling OOS metrics (test segment only)
MODEL            HORIZON WIN Count  PearsonIC SpearmanIC  HitRate Sharpe
-----            ------- --- -----  --------- ----------- ------- ------
Hawkes_Intensity 15m     0   112052 0.0143    0.0135      0.508   0.009
Hawkes_Intensity 15m     1   112052 0.0050    -0.0102     0.501   -0.006
Hawkes_Intensity 15m     2   112052 0.0100    0.0113      0.505   0.011
Hawkes_Intensity 15m     3   112052 0.0217    0.0127      0.506   0.019
Hawkes_Intensity 15m     4   112052 0.0043    -0.0108     0.495   -0.012
Hawkes_Intensity 15m     5   112052 0.0090    0.0074      0.504   0.019
Hawkes_Intensity 15m     6   112052 0.0117    -0.0074     0.500   0.010
Hawkes_Intensity 15m     7   112055 0.0266    -0.0008     0.497   -0.010
Hawkes_Intensity 30m     0   112052 0.0104    0.0080      0.510   0.013
Hawkes_Intensity 30m     1   112052 0.0156    -0.0062     0.503   -0.009
Hawkes_Intensity 30m     2   112052 0.0108    0.0110      0.502   0.016
Hawkes_Intensity 30m     3   112052 0.0281    0.0216      0.509   0.026
Hawkes_Intensity 30m     4   112052 -0.0006   -0.0105     0.496   -0.017
Hawkes_Intensity 30m     5   112052 0.0107    0.0123      0.509   0.028
Hawkes_Intensity 30m     6   112052 0.0042    -0.0031     0.507   0.013
Hawkes_Intensity 30m     7   112055 0.0180    0.0018      0.495   -0.014
Hawkes_Intensity 1h      0   112052 0.0007    0.0066      0.515   0.020
Hawkes_Intensity 1h      1   112052 0.0183    -0.0134     0.503   -0.014
Hawkes_Intensity 1h      2   112052 0.0023    0.0100      0.509   0.021
Hawkes_Intensity 1h      3   112052 0.0318    0.0269      0.519   0.036
Hawkes_Intensity 1h      4   112052 0.0084    0.0018      0.499   -0.020
Hawkes_Intensity 1h      5   112052 0.0095    0.0199      0.515   0.039
Hawkes_Intensity 1h      6   112052 0.0005    -0.0088     0.506   0.018
Hawkes_Intensity 1h      7   112055 0.0233    0.0021      0.500   -0.020

Hawkes_OFI 15m 0 112052 -0.0104 -0.0445 0.479 -0.003
Hawkes_OFI 15m 1 112052 -0.0313 -0.0605 0.473 -0.027
Hawkes_OFI 15m 2 112052 -0.0133 -0.0412 0.483 -0.014
Hawkes_OFI 15m 3 112052 -0.0003 -0.0282 0.489 -0.006
Hawkes_OFI 15m 4 112052 -0.0137 -0.0449 0.482 -0.017
Hawkes_OFI 15m 5 112052 -0.0156 -0.0390 0.482 -0.015
Hawkes_OFI 15m 6 112052 -0.0096 -0.0447 0.483 -0.021
Hawkes_OFI 15m 7 112055 -0.0191 -0.0341 0.486 -0.009
Hawkes_OFI 30m 0 112052 -0.0044 -0.0317 0.487 -0.001
Hawkes_OFI 30m 1 112052 -0.0210 -0.0451 0.479 -0.015
Hawkes_OFI 30m 2 112052 -0.0164 -0.0292 0.492 -0.005
Hawkes_OFI 30m 3 112052 0.0095  -0.0362 0.489 -0.005
Hawkes_OFI 30m 4 112052 -0.0299 -0.0304 0.486 -0.014
Hawkes_OFI 30m 5 112052 -0.0118 -0.0390 0.484 -0.018
Hawkes_OFI 30m 6 112052 -0.0070 -0.0485 0.485 -0.027
Hawkes_OFI 30m 7 112055 -0.0095 -0.0318 0.489 -0.008
Hawkes_OFI 1h  0 112052 -0.0126 -0.0458 0.480 -0.014
Hawkes_OFI 1h  1 112052 -0.0276 -0.0436 0.480 -0.015
Hawkes_OFI 1h  2 112052 -0.0236 -0.0342 0.492 -0.008
Hawkes_OFI 1h  3 112052 0.0036  -0.0330 0.484 -0.005
Hawkes_OFI 1h  4 112052 -0.0216 -0.0287 0.491 -0.007
Hawkes_OFI 1h  5 112052 -0.0165 -0.0392 0.483 -0.027
Hawkes_OFI 1h  6 112052 0.0030  -0.0263 0.489 -0.012
Hawkes_OFI 1h  7 112055 -0.0175 -0.0290 0.488 -0.008

Sig_LevyArea 15m 0 112052 -0.0033 0.0115  0.507 0.008
Sig_LevyArea 15m 1 112052 0.0159  -0.0024 0.501 -0.006
Sig_LevyArea 15m 2 112052 0.0231  0.0233  0.505 0.011
Sig_LevyArea 15m 3 112052 0.0426  0.0135  0.506 0.019
Sig_LevyArea 15m 4 112052 -0.0043 -0.0139 0.495 -0.012
Sig_LevyArea 15m 5 112052 0.0166  0.0163  0.504 0.019
Sig_LevyArea 15m 6 112052 0.0125  -0.0018 0.500 0.010
Sig_LevyArea 15m 7 112055 0.0498  0.0011  0.497 -0.010
Sig_LevyArea 30m 0 112052 -0.0136 0.0057  0.509 0.013
Sig_LevyArea 30m 1 112052 0.0200  -0.0060 0.503 -0.008
Sig_LevyArea 30m 2 112052 0.0250  0.0245  0.502 0.016
Sig_LevyArea 30m 3 112052 0.0562  0.0150  0.509 0.026
Sig_LevyArea 30m 4 112052 -0.0034 -0.0126 0.496 -0.017
Sig_LevyArea 30m 5 112052 0.0229  0.0251  0.509 0.027
Sig_LevyArea 30m 6 112052 0.0136  0.0031  0.507 0.013
Sig_LevyArea 30m 7 112055 0.0573  0.0053  0.495 -0.014
Sig_LevyArea 1h  0 112052 -0.0081 0.0046  0.514 0.019
Sig_LevyArea 1h  1 112052 0.0155  -0.0240 0.503 -0.014
Sig_LevyArea 1h  2 112052 0.0141  0.0294  0.509 0.021
Sig_LevyArea 1h  3 112052 0.0570  0.0165  0.519 0.036
Sig_LevyArea 1h  4 112052 0.0168  -0.0002 0.500 -0.020
Sig_LevyArea 1h  5 112052 0.0221  0.0355  0.515 0.039
Sig_LevyArea 1h  6 112052 0.0079  -0.0010 0.506 0.018
Sig_LevyArea 1h  7 112055 0.0431  0.0122  0.500 -0.020

Hilbert_Phase 15m 0 112052 -0.0065 -0.0344 0.477 -0.005
Hilbert_Phase 15m 1 112052 -0.0100 -0.0349 0.477 -0.012
Hilbert_Phase 15m 2 112052 -0.0103 -0.0346 0.481 -0.010
Hilbert_Phase 15m 3 112052 -0.0072 -0.0264 0.487 -0.006
Hilbert_Phase 15m 4 112052 -0.0086 -0.0233 0.487 -0.007
Hilbert_Phase 15m 5 112052 -0.0178 -0.0344 0.483 -0.019
Hilbert_Phase 15m 6 112052 -0.0136 -0.0242 0.489 -0.013
Hilbert_Phase 15m 7 112055 -0.0090 -0.0252 0.484 -0.008
Hilbert_Phase 30m 0 112052 0.0061  -0.0172 0.489 0.007
Hilbert_Phase 30m 1 112052 -0.0021 -0.0276 0.481 -0.003
Hilbert_Phase 30m 2 112052 -0.0038 -0.0242 0.488 -0.004
Hilbert_Phase 30m 3 112052 -0.0002 -0.0243 0.488 -0.001
Hilbert_Phase 30m 4 112052 -0.0059 -0.0215 0.487 -0.003
Hilbert_Phase 30m 5 112052 -0.0170 -0.0328 0.483 -0.018
Hilbert_Phase 30m 6 112052 -0.0174 -0.0257 0.489 -0.017
Hilbert_Phase 30m 7 112055 -0.0094 -0.0274 0.483 -0.011
Hilbert_Phase 1h  0 112052 -0.0010 -0.0259 0.483 -0.002
Hilbert_Phase 1h  1 112052 -0.0074 -0.0318 0.479 -0.008
Hilbert_Phase 1h  2 112052 -0.0128 -0.0294 0.485 -0.015
Hilbert_Phase 1h  3 112052 -0.0092 -0.0269 0.484 -0.010
Hilbert_Phase 1h  4 112052 -0.0034 -0.0159 0.492 -0.000
Hilbert_Phase 1h  5 112052 -0.0217 -0.0281 0.486 -0.023
Hilbert_Phase 1h  6 112052 -0.0117 -0.0208 0.489 -0.010
Hilbert_Phase 1h  7 112055 -0.0070 -0.0211 0.485 -0.006



# Volatility regime OOS metrics (test segment only)
MODEL            HORIZON REGIME  Count  PearsonIC SpearmanIC  HitRate Sharpe
-----            ------- ------  -----  --------- ----------- ------- ------
Hawkes_Intensity 15m     VolLow  298807 0.0005    0.0006      0.498   -0.003
Hawkes_Intensity 15m     VolMed  298806 0.0004    -0.0000     0.501   0.003
Hawkes_Intensity 15m     VolHigh 298806 0.0155    0.0067      0.507   0.007
Hawkes_Intensity 30m     VolLow  298807 0.0017    0.0037      0.499   -0.002
Hawkes_Intensity 30m     VolMed  298806 0.0008    0.0008      0.503   0.007
Hawkes_Intensity 30m     VolHigh 298806 0.0156    0.0094      0.510   0.009
Hawkes_Intensity 1h      VolLow  298807 0.0019    -0.0004     0.502   0.005
Hawkes_Intensity 1h      VolMed  298806 0.0018    0.0029      0.510   0.021
Hawkes_Intensity 1h      VolHigh 298806 0.0148    0.0109      0.513   0.010

Hawkes_OFI 15m VolLow  298807 -0.0201 -0.0270 0.490 -0.023
Hawkes_OFI 15m VolMed  298806 -0.0453 -0.0542 0.477 -0.047
Hawkes_OFI 15m VolHigh 298806 -0.0095 -0.0235 0.479 -0.009
Hawkes_OFI 30m VolLow  298807 -0.0115 -0.0109 0.498 -0.005
Hawkes_OFI 30m VolMed  298806 -0.0396 -0.0464 0.480 -0.041
Hawkes_OFI 30m VolHigh 298806 -0.0069 -0.0189 0.481 -0.007
Hawkes_OFI 1h  VolLow  298807 -0.0120 -0.0129 0.495 -0.011
Hawkes_OFI 1h  VolMed  298806 -0.0431 -0.0484 0.480 -0.042
Hawkes_OFI 1h  VolHigh 298806 -0.0114 -0.0193 0.483 -0.008

Sig_LevyArea 15m VolLow  298807 0.0033  0.0050  0.498 -0.003
Sig_LevyArea 15m VolMed  298806 0.0052  0.0066  0.501 0.003
Sig_LevyArea 15m VolHigh 298806 0.0200  0.0054  0.507 0.007
Sig_LevyArea 30m VolLow  298807 0.0053  0.0105  0.499 -0.002
Sig_LevyArea 30m VolMed  298806 0.0045  0.0074  0.503 0.007
Sig_LevyArea 30m VolHigh 298806 0.0228  0.0048  0.510 0.009
Sig_LevyArea 1h  VolLow  298807 -0.0002 -0.0020 0.502 0.005
Sig_LevyArea 1h  VolMed  298806 0.0059  0.0113  0.510 0.021
Sig_LevyArea 1h  VolHigh 298806 0.0259  0.0053  0.513 0.009

Hilbert_Phase 15m VolLow  298807 -0.0127 -0.0117 0.495 -0.013
Hilbert_Phase 15m VolMed  298806 -0.0453 -0.0408 0.476 -0.049
Hilbert_Phase 15m VolHigh 298806 -0.0039 -0.0155 0.479 -0.002
Hilbert_Phase 30m VolLow  298807 -0.0101 -0.0098 0.495 -0.011
Hilbert_Phase 30m VolMed  298806 -0.0359 -0.0337 0.481 -0.040
Hilbert_Phase 30m VolHigh 298806 0.0021  -0.0100 0.482 0.003
Hilbert_Phase 1h  VolLow  298807 -0.0094 -0.0088 0.494 -0.011
Hilbert_Phase 1h  VolMed  298806 -0.0421 -0.0390 0.478 -0.045
Hilbert_Phase 1h  VolHigh 298806 -0.0025 -0.0108 0.484 -0.002



# Time-of-day regime OOS metrics (test segment only)
MODEL            HORIZON REGIME    Count  PearsonIC SpearmanIC  HitRate Sharpe
-----            ------- ------    -----  --------- ----------- ------- ------
Hawkes_Intensity 15m     TOD_Early 311331 0.0067    0.0018      0.498   0.003
Hawkes_Intensity 15m     TOD_Mid   312000 0.0134    0.0017      0.500   0.003
Hawkes_Intensity 15m     TOD_Late  273088 0.0158    0.0062      0.509   0.007
Hawkes_Intensity 30m     TOD_Early 311331 0.0087    0.0020      0.500   0.006
Hawkes_Intensity 30m     TOD_Mid   312000 0.0150    0.0041      0.501   0.003
Hawkes_Intensity 30m     TOD_Late  273088 0.0134    0.0101      0.511   0.010
Hawkes_Intensity 1h      TOD_Early 311331 0.0056    0.0014      0.507   0.011
Hawkes_Intensity 1h      TOD_Mid   312000 0.0151    0.0054      0.503   0.002
Hawkes_Intensity 1h      TOD_Late  273088 0.0135    0.0159      0.515   0.014

Hawkes_OFI 15m TOD_Early 311331 0.0087  -0.0378 0.486 -0.007
Hawkes_OFI 15m TOD_Mid   312000 -0.0227 -0.0415 0.481 -0.018
Hawkes_OFI 15m TOD_Late  273088 -0.0209 -0.0470 0.479 -0.014
Hawkes_OFI 30m TOD_Early 311331 0.0176  -0.0341 0.487 -0.005
Hawkes_OFI 30m TOD_Mid   312000 -0.0162 -0.0283 0.488 -0.013
Hawkes_OFI 30m TOD_Late  273088 -0.0268 -0.0455 0.484 -0.012
Hawkes_OFI 1h  TOD_Early 311331 0.0140  -0.0269 0.491 -0.004
Hawkes_OFI 1h  TOD_Mid   312000 -0.0212 -0.0339 0.484 -0.016
Hawkes_OFI 1h  TOD_Late  273088 -0.0295 -0.0450 0.482 -0.013

Sig_LevyArea 15m TOD_Early 311331 0.0124 -0.0026 0.498 0.003
Sig_LevyArea 15m TOD_Mid   312000 0.0134 0.0041  0.500 0.003
Sig_LevyArea 15m TOD_Late  273088 0.0236 0.0175  0.509 0.007
Sig_LevyArea 30m TOD_Early 311331 0.0126 -0.0038 0.500 0.006
Sig_LevyArea 30m TOD_Mid   312000 0.0142 0.0042  0.501 0.003
Sig_LevyArea 30m TOD_Late  273088 0.0292 0.0230  0.511 0.010
Sig_LevyArea 1h  TOD_Early 311331 0.0061 -0.0061 0.507 0.011
Sig_LevyArea 1h  TOD_Mid   312000 0.0238 0.0051  0.503 0.002
Sig_LevyArea 1h  TOD_Late  273088 0.0290 0.0330  0.515 0.014

Hilbert_Phase 15m TOD_Early 311331 -0.0071 -0.0298 0.485 -0.005
Hilbert_Phase 15m TOD_Mid   312000 -0.0145 -0.0368 0.479 -0.015
Hilbert_Phase 15m TOD_Late  273088 -0.0066 -0.0225 0.486 -0.006
Hilbert_Phase 30m TOD_Early 311331 -0.0010 -0.0235 0.488 0.001
Hilbert_Phase 30m TOD_Mid   312000 -0.0097 -0.0277 0.483 -0.011
Hilbert_Phase 30m TOD_Late  273088 -0.0027 -0.0235 0.486 -0.004
Hilbert_Phase 1h  TOD_Early 311331 -0.0023 -0.0226 0.487 -0.001
Hilbert_Phase 1h  TOD_Mid   312000 -0.0108 -0.0251 0.485 -0.011
Hilbert_Phase 1h  TOD_Late  273088 -0.0107 -0.0280 0.484 -0.012




--- File Tree Structure ---
|-- config.go
|-- context.py
|-- gnc.go
|-- main.go
|-- math.go
|-- metrics.go
|-- probe.go
|-- stream.go
|-- test.go

// --- File: config.go ---

```go
package main

import (
	"runtime"
	"sort"
)

// This is the shared data root produced by the downloader project.
// It MUST be the directory that directly contains BTCUSDT/, ETHUSDT/, etc.
//
//	Z:\DATA\data\BTCUSDT\2020\01\...
//	Z:\DATA\data\ETHUSDT\2020\01\...
const BaseDir = `Z:\DATA\data`

// SamplingRateSec: How often we "snapshot" the continuous physics.
const SamplingRateSec = 60

// Horizon definitions for the regression targets.
var HorizonLabels = []string{"15m", "30m", "1h"}
var HorizonDelays = []int64{
	15 * 60 * 1000, // 15 min in ms
	30 * 60 * 1000, // 30 min in ms
	60 * 60 * 1000, // 60 min in ms
}

// System tuning for Ryzen 9 7900X (leave 2 cores free for OS/other work).
var CPUThreads = func() int {
	n := runtime.GOMAXPROCS(0)
	if n > 4 {
		return n - 2
	}
	return n
}()

// Symbol selects which symbol to run research/OOS on.
func Symbol() string {
	// Preference order among discovered symbols.
	preferred := []string{
		"BTCUSDT",
		"ETHUSDT",
		"BNBUSDT",
		"SOLUSDT",
		"XRPUSDT",
		"DOGEUSDT",
		"HYPEUSDT",
		"ASTERUSDT",
	}

	var symbols []string
	symbolSet := make(map[string]struct{})

	for sym := range discoverSymbols() {
		symbols = append(symbols, sym)
		symbolSet[sym] = struct{}{}
	}

	if len(symbols) == 0 {
		// Hard fallback if directory scan fails.
		return "BTCUSDT"
	}

	// Try preferred list in order.
	for _, p := range preferred {
		if _, ok := symbolSet[p]; ok {
			return p
		}
	}

	// Fallback: alphabetical first.
	sort.Strings(symbols)
	return symbols[0]
}
```

// --- End File: config.go ---

// --- File: context.py ---

```python
import os
import sys
import gzip
from pathlib import Path

# --- Configuration ---

SCRIPT_FILENAME = os.path.basename(__file__)  # Name of this script
OUTPUT_FILENAME = "context_output.txt"
ROOT_DIR = "."

# Directories to ignore
IGNORE_DIRS = {".venv", ".git", "__pycache__"}
IGNORE_FILE_PATTERNS = {"~", ".swp", ".DS_Store"}

# Relevant Extensions
PY_EXT = ".py"
CSV_EXT = ".csv"
CSV_GZ_EXT = ".csv.gz"
PARQUET_EXT = ".parquet"
YAML_EXT1 = ".yaml"
YAML_EXT2 = ".yml"
ENV_EXT = ".env"
SAFETENSORS_EXT = ".safetensors"
PT_EXT = ".pt"
JSON_EXT = ".json"
CU_EXT = ".cu"
CUH_EXT = ".cuh"
NPZ_EXT = ".npz"
GO_EXT = ".go"

# JSON truncation configuration
LARGE_JSON_SIZE_BYTES = 256 * 1024      # JSON bigger than 256KB is "large"
MAX_JSON_PREVIEW_CHARS = 8 * 1024      # preview at most 8KB of text
MAX_JSON_PREVIEW_LINES = 80            # and at most 80 lines

# Month directory names for collapsing tree output
MONTH_DIRS = {f"{i:02d}" for i in range(1, 13)}


# --- Helpers ---


def is_date_subdir(path_str):
    """
    Checks if path is a date-based subdir under bars_* or btc_parquet_clean.
    Matches logic: base starts with 'd=' and parent is 'bars_*' or 'btc_parquet_clean'.
    """
    path = Path(path_str)
    base = path.name
    if not base.startswith("d="):
        return False
    parent = path.parent.name
    return parent.startswith("bars_") or parent == "btc_parquet_clean"


def get_language_identifier(filename):
    """Returns markdown language tag based on extension."""
    lower = filename.lower()

    # Handle .gz extension stripping
    if lower.endswith(".gz"):
        ext = os.path.splitext(os.path.splitext(lower)[0])[1]
    else:
        ext = os.path.splitext(lower)[1]

    mapping = {
        ".py": "python",
        ".yaml": "yaml", ".yml": "yaml",
        ".env": "bash",
        ".csv": "csv",
        ".go": "go",
        ".js": "javascript",
        ".ts": "typescript",
        ".java": "java",
        ".cpp": "cpp", ".cc": "cpp", ".cxx": "cpp",
        ".c": "c",
        ".cs": "csharp",
        ".rb": "ruby",
        ".php": "php",
        ".swift": "swift",
        ".rs": "rust",
        ".sh": "bash",
        ".sql": "sql",
        ".json": "json",
        ".xml": "xml",
        ".html": "html",
        ".css": "css",
        ".md": "markdown",
        ".cu": "cpp", ".cuh": "cpp",
        ".npz": "text",
    }
    return mapping.get(ext, "")


def get_file_content(path, full=True, n=0):
    """
    Reads file content. Handles .gz transparently.
    Uses 'utf-8' with error replacement to avoid crashing on binary data.

    If full=False, returns up to n lines and adds a small note if truncated or empty.
    """
    try:
        lower = path.lower()
        if lower.endswith(".gz"):
            open_func = gzip.open
        else:
            open_func = open

        with open_func(path, "rt", encoding="utf-8", errors="replace") as f:
            if full:
                return f.read()
            else:
                lines = []
                line_count = 0
                for line in f:
                    if line_count >= n:
                        break
                    lines.append(line)
                    line_count += 1

                content = "".join(lines)
                if line_count == 0:
                    content += "# (File is empty or could not be read)\n"
                elif line_count < n:
                    content += f"# (File has only {line_count} lines)\n"
                return content
    except Exception as e:
        return f"Error reading file: {e}"


def get_large_json_preview(path, max_chars, max_lines):
    """
    Read only the first `max_lines` lines and up to `max_chars` characters
    from a large JSON file, and mark it as truncated if we hit the limit.
    """
    try:
        out = []
        total_chars = 0
        line_count = 0
        truncated = False

        with open(path, "rt", encoding="utf-8", errors="replace") as f:
            for line in f:
                if line_count >= max_lines or total_chars >= max_chars:
                    truncated = True
                    break

                remaining = max_chars - total_chars
                if remaining <= 0:
                    truncated = True
                    break

                # Clip line if it would exceed char budget
                if len(line) > remaining:
                    out.append(line[:remaining])
                    total_chars += remaining
                    truncated = True
                    break
                else:
                    out.append(line)
                    total_chars += len(line)
                    line_count += 1

        preview = "".join(out)
        if truncated:
            preview += (
                "\n// (truncated JSON preview: showing first "
                f"{line_count} lines / {total_chars} chars)\n"
            )
        else:
            preview += "\n// (complete JSON file or small enough)\n"

        return preview
    except Exception as e:
        return f"Error reading JSON file: {e}"


def is_relevant_file(name):
    """Determines if a file is relevant based on extension."""
    lower = name.lower()

    # Skip temp files and output file
    if name == SCRIPT_FILENAME or name == OUTPUT_FILENAME:
        return False

    for pattern in IGNORE_FILE_PATTERNS:
        if pattern in name:  # Simple substring check for temp files
            return False

    if lower.endswith(CSV_GZ_EXT):
        return True

    ext = os.path.splitext(lower)[1]
    valid_exts = {
        PY_EXT,
        GO_EXT,
        CSV_EXT,
        PARQUET_EXT,
        YAML_EXT1,
        YAML_EXT2,
        ENV_EXT,
        SAFETENSORS_EXT,
        PT_EXT,
        JSON_EXT,
        CU_EXT,
        CUH_EXT,
        NPZ_EXT,
    }
    return ext in valid_exts


# --- Main ---


def main():
    print("Starting: building file tree and collecting relevant paths...", file=sys.stderr)

    try:
        outfile = open(OUTPUT_FILENAME, "w", encoding="utf-8")
    except IOError as e:
        print(f"Failed to create {OUTPUT_FILENAME}: {e}", file=sys.stderr)
        return

    relevant_paths = []

    # 1. Build Tree
    outfile.write("--- File Tree Structure ---\n")

    # os.walk yields (dirpath, dirnames, filenames)
    # We sort them to ensure deterministic output similar to file explorers
    for root, dirs, files in os.walk(ROOT_DIR, topdown=True):
        # Filter Directories in-place to stop os.walk from entering them
        dirs[:] = [
            d
            for d in dirs
            if d not in IGNORE_DIRS and not is_date_subdir(os.path.join(root, d))
        ]
        dirs.sort()
        files.sort()

        # Calculate depth for indentation
        rel_path = os.path.relpath(root, ROOT_DIR)
        if rel_path == ".":
            depth = 0
        else:
            depth = rel_path.count(os.sep) + 1

        # --- Directories in tree view ---
        indent = "    " * depth + "|-- "

        if dirs:
            # If ALL child directories look like months (01..12), collapse them
            all_months = all(d in MONTH_DIRS for d in dirs)
            if all_months and len(dirs) > 3:
                first = dirs[0]
                last = dirs[-1]
                outfile.write(f"{indent}[{first}..{last}]/ ({len(dirs)} month dirs)\n")
            else:
                for d in dirs:
                    outfile.write(f"{indent}{d}/\n")

        # --- Files in tree view ---
        for f in files:
            if f == SCRIPT_FILENAME or f == OUTPUT_FILENAME:
                continue

            # Skip temp files
            is_temp = False
            for pat in IGNORE_FILE_PATTERNS:
                if f.endswith(pat):
                    is_temp = True
                    break
            if is_temp:
                continue

            if is_relevant_file(f):
                file_indent = "    " * depth + "|-- "
                outfile.write(f"{file_indent}{f}\n")
                full_path = os.path.join(root, f)
                relevant_paths.append(full_path)

    outfile.write("\n")
    outfile.flush()

    print(f"Tree build complete. Relevant files: {len(relevant_paths)}", file=sys.stderr)
    print("Processing file contents for context...", file=sys.stderr)

    # 2. Dump Content
    for path in relevant_paths:
        lower = path.lower()

        # Skip content dump for large/binary formats
        if (
            lower.endswith(CSV_EXT)
            or lower.endswith(CSV_GZ_EXT)
            or lower.endswith(PARQUET_EXT)
            or lower.endswith(NPZ_EXT)
        ):
            continue

        is_json = lower.endswith(JSON_EXT)

        # Detect large JSON files
        is_large_json = False
        if is_json:
            try:
                size_bytes = os.path.getsize(path)
                if size_bytes > LARGE_JSON_SIZE_BYTES:
                    is_large_json = True
            except OSError:
                is_large_json = False

        # Decide to read full content
        should_read_full = (
            lower.endswith(PY_EXT)
            or lower.endswith(GO_EXT)
            or lower.endswith(YAML_EXT1)
            or lower.endswith(YAML_EXT2)
            or lower.endswith(ENV_EXT)
            or lower.endswith(CU_EXT)
            or lower.endswith(CUH_EXT)
            or lower.endswith(SAFETENSORS_EXT)
            or lower.endswith(PT_EXT)
            or (is_json and not is_large_json)
        )

        # If it's not a large JSON and not in the full-read set, skip
        if not should_read_full and not is_large_json:
            continue

        rel_path = os.path.relpath(path, ROOT_DIR)
        lang_id = get_language_identifier(path)

        print(f"Including file: {rel_path}", file=sys.stderr)

        outfile.write(f"// --- File: {rel_path} ---\n\n")

        # Use preview for large JSON, full content otherwise
        if is_large_json:
            content = get_large_json_preview(
                path, MAX_JSON_PREVIEW_CHARS, MAX_JSON_PREVIEW_LINES
            )
        else:
            content = get_file_content(path)

        outfile.write(f"```{lang_id}\n")
        outfile.write(content)
        if content and not content.endswith("\n"):
            outfile.write("\n")
        outfile.write("```\n\n")

        outfile.write(f"// --- End File: {rel_path} ---\n\n")
        outfile.flush()

    outfile.close()
    print("File content collection complete.", file=sys.stderr)
    print(f"Successfully wrote context to {OUTPUT_FILENAME}", file=sys.stderr)


if __name__ == "__main__":
    main()
```

// --- End File: context.py ---

// --- File: gnc.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"unsafe"
)

// These constants MUST match the downloader project.
const (
	IdxMagic = "QIDX" // index.quantdev magic

	TBMagic   = "TBV1" // trade-block magic
	TBVersion = 1
	TBHdrSize = 64

	// Zen4 cache line size (and alignment unit for columns).
	CacheLine = 64
)

// --- TBV1 header + zero-copy TradeBlock view ---

type tbHeader struct {
	Rows     uint64
	OffAgg   uint32
	OffPrice uint32
	OffQty   uint32
	OffFirst uint32
	OffLast  uint32
	OffTime  uint32
	OffBits  uint32
	BitWords uint64
}

// parseTBHeader validates header + bounds and returns layout info.
func parseTBHeader(hdr []byte, blobLen uint64) (tbHeader, error) {
	var h tbHeader
	if len(hdr) < TBHdrSize {
		return h, fmt.Errorf("header too short")
	}
	if string(hdr[0:4]) != TBMagic {
		return h, fmt.Errorf("magic mismatch")
	}
	v := binary.LittleEndian.Uint32(hdr[4:8])
	if v != TBVersion {
		return h, fmt.Errorf("version mismatch: %d", v)
	}

	rows := binary.LittleEndian.Uint64(hdr[8:16])
	if rows == 0 {
		return h, fmt.Errorf("zero rows")
	}
	h.Rows = rows
	h.OffAgg = binary.LittleEndian.Uint32(hdr[16:20])
	h.OffPrice = binary.LittleEndian.Uint32(hdr[20:24])
	h.OffQty = binary.LittleEndian.Uint32(hdr[24:28])
	h.OffFirst = binary.LittleEndian.Uint32(hdr[28:32])
	h.OffLast = binary.LittleEndian.Uint32(hdr[32:36])
	h.OffTime = binary.LittleEndian.Uint32(hdr[36:40])
	h.OffBits = binary.LittleEndian.Uint32(hdr[40:44])

	if blobLen < uint64(TBHdrSize) {
		return h, fmt.Errorf("blob too small")
	}

	offs := []uint32{
		h.OffAgg, h.OffPrice, h.OffQty,
		h.OffFirst, h.OffLast, h.OffTime, h.OffBits,
	}
	for _, off := range offs {
		if off < TBHdrSize {
			return h, fmt.Errorf("offset %d < header size", off)
		}
		// Enforce the intended 64-byte alignment for all columns.
		if off%CacheLine != 0 {
			return h, fmt.Errorf("offset %d not %d-byte aligned", off, CacheLine)
		}
	}

	validateCol := func(off uint32, elemSize uint64) error {
		end := uint64(off) + rows*elemSize
		if end > blobLen {
			return fmt.Errorf("column out of range (off=%d)", off)
		}
		return nil
	}

	if err := validateCol(h.OffAgg, 8); err != nil {
		return h, err
	}
	if err := validateCol(h.OffPrice, 8); err != nil {
		return h, err
	}
	if err := validateCol(h.OffQty, 8); err != nil {
		return h, err
	}
	if err := validateCol(h.OffFirst, 8); err != nil {
		return h, err
	}
	if err := validateCol(h.OffLast, 8); err != nil {
		return h, err
	}
	if err := validateCol(h.OffTime, 8); err != nil {
		return h, err
	}

	bitWords := (rows + 63) / 64
	if bitWords == 0 {
		return h, fmt.Errorf("invalid bitWords")
	}
	bitsEnd := uint64(h.OffBits) + bitWords*8
	if bitsEnd > blobLen {
		return h, fmt.Errorf("bitset out of range")
	}
	h.BitWords = bitWords
	return h, nil
}

// TradeBlock is a zero-copy view over a TBV1 blob.
type TradeBlock struct {
	Count int

	AggTradeIDs   []uint64
	Prices        []float64
	Quantities    []float64
	FirstTradeIDs []uint64
	LastTradeIDs  []uint64
	Times         []int64

	BuyerBits []uint64
}

// mapTradeBlock creates a view over raw blob without extra allocations.
func mapTradeBlock(raw []byte) (*TradeBlock, error) {
	h, err := parseTBHeader(raw, uint64(len(raw)))
	if err != nil {
		return nil, err
	}
	if len(raw) < TBHdrSize {
		return nil, fmt.Errorf("short blob")
	}

	count := int(h.Rows)
	if count < 0 {
		return nil, fmt.Errorf("negative count")
	}

	tb := &TradeBlock{Count: count}
	base := unsafe.Pointer(&raw[0])

	tb.AggTradeIDs = unsafe.Slice((*uint64)(unsafe.Add(base, uintptr(h.OffAgg))), count)
	tb.Prices = unsafe.Slice((*float64)(unsafe.Add(base, uintptr(h.OffPrice))), count)
	tb.Quantities = unsafe.Slice((*float64)(unsafe.Add(base, uintptr(h.OffQty))), count)
	tb.FirstTradeIDs = unsafe.Slice((*uint64)(unsafe.Add(base, uintptr(h.OffFirst))), count)
	tb.LastTradeIDs = unsafe.Slice((*uint64)(unsafe.Add(base, uintptr(h.OffLast))), count)
	tb.Times = unsafe.Slice((*int64)(unsafe.Add(base, uintptr(h.OffTime))), count)
	tb.BuyerBits = unsafe.Slice((*uint64)(unsafe.Add(base, uintptr(h.OffBits))), int(h.BitWords))

	return tb, nil
}

// IsBuyerMaker checks the boolean bitset efficiently.
func (tb *TradeBlock) IsBuyerMaker(i int) bool {
	if i < 0 || i >= tb.Count {
		return false
	}
	wordIdx := i / 64
	bitIdx := i % 64
	return (tb.BuyerBits[wordIdx] & (1 << bitIdx)) != 0
}

// --- DayColumns (simple SoA view used by RunStream) ---

// DayColumns is the SoA representation of a single day's trades,
// used by the streaming feature engine. Right now we only need
// time/price/quantity for the continuous models.
type DayColumns struct {
	Count  int
	Times  []int64
	Prices []float64
	Qtys   []float64
}

// DayColumnPool reduces allocation pressure (critical for GOGC=200).
var DayColumnPool = sync.Pool{
	New: func() any {
		// Pre-allocate for ~1.5M rows (typical busy day).
		const initCap = 1_500_000
		return &DayColumns{
			Times:  make([]int64, 0, initCap),
			Prices: make([]float64, 0, initCap),
			Qtys:   make([]float64, 0, initCap),
		}
	},
}

// Reset clears the struct for reuse without freeing memory.
func (c *DayColumns) Reset() {
	c.Count = 0
	c.Times = c.Times[:0]
	c.Prices = c.Prices[:0]
	c.Qtys = c.Qtys[:0]
}

// FillFromTradeBlock copies the TBV1 SoA into the DayColumns view.
func (c *DayColumns) FillFromTradeBlock(tb *TradeBlock) {
	c.Reset()
	n := tb.Count
	if n == 0 {
		return
	}

	if cap(c.Times) < n {
		c.Times = make([]int64, n)
		c.Prices = make([]float64, n)
		c.Qtys = make([]float64, n)
	} else {
		c.Times = c.Times[:n]
		c.Prices = c.Prices[:n]
		c.Qtys = c.Qtys[:n]
	}

	copy(c.Times, tb.Times)
	copy(c.Prices, tb.Prices)
	copy(c.Qtys, tb.Quantities)

	c.Count = n
}

// ofiTask identifies a single day (year, month, day) for one symbol.
type ofiTask struct {
	Year, Month, Day int
}

// LoadGNCFile locates and reads a single TBV1 blob for (sym, day) into buf.
// Returns false on any error or if the day is not present in the index.
//
// NOTE: Name kept as LoadGNCFile for API compatibility with existing code;
// it now actually loads a TBV1 trade-block blob.
func LoadGNCFile(baseDir, sym string, t ofiTask, buf *[]byte) bool {
	dir := filepath.Join(baseDir, sym, sprintfYear(t.Year), sprintfMonth(t.Month))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, t.Day)
	if length == 0 {
		return false
	}

	// Safety check: prevent panic if index is corrupted and length is massive.
	// 512MB is a reasonable upper bound for a single day's blob.
	if length > 512*1024*1024 {
		return false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return false
	}
	defer f.Close()

	if cap(*buf) < int(length) {
		*buf = make([]byte, length)
	}
	*buf = (*buf)[:length]

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return false
	}
	if _, err := io.ReadFull(f, *buf); err != nil {
		return false
	}
	return true
}

// InflateGNC decodes a TBV1 blob into DayColumns by mapping the TradeBlock
// and copying just the SoA slices we care about (time, price, qty).
//
// Signature is kept as (int, error) for compatibility with the previous code.
func InflateGNC(rawBlob []byte, cols *DayColumns) (int, error) {
	cols.Reset()

	tb, err := mapTradeBlock(rawBlob)
	if err != nil {
		return 0, err
	}
	if tb.Count == 0 {
		return 0, nil
	}

	cols.FillFromTradeBlock(tb)
	return cols.Count, nil
}

// --- Discovery helpers over the TBV1 index tree ---

// discoverSymbols yields all symbols (top-level dirs) under BaseDir.
func discoverSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		entries, _ := os.ReadDir(BaseDir)
		for _, e := range entries {
			if !e.IsDir() {
				continue
			}
			name := e.Name()
			if len(name) == 0 || name[0] == '.' || name == "features" {
				continue
			}
			if !yield(name) {
				return
			}
		}
	}
}

// discoverTasks yields all (year, month, day) tasks for a symbol.
// Reads 26-byte index rows: Day[2] + Offset[8] + Length[8] + Checksum[8].
func discoverTasks(sym string) iter.Seq[ofiTask] {
	return func(yield func(ofiTask) bool) {
		root := filepath.Join(BaseDir, sym)
		years, err := os.ReadDir(root)
		if err != nil {
			return
		}
		for _, y := range years {
			if !y.IsDir() || len(y.Name()) != 4 {
				continue
			}
			year, err := strconv.Atoi(y.Name())
			if err != nil {
				continue
			}

			months, err := os.ReadDir(filepath.Join(root, y.Name()))
			if err != nil {
				continue
			}
			for _, m := range months {
				if !m.IsDir() || len(m.Name()) != 2 {
					continue
				}
				month, err := strconv.Atoi(m.Name())
				if err != nil {
					continue
				}

				idxPath := filepath.Join(root, y.Name(), m.Name(), "index.quantdev")
				f, err := os.Open(idxPath)
				if err != nil {
					continue
				}

				var hdr [16]byte
				if _, err := io.ReadFull(f, hdr[:]); err == nil && string(hdr[0:4]) == IdxMagic {
					count := binary.LittleEndian.Uint64(hdr[8:16])
					var row [26]byte
					for i := uint64(0); i < count; i++ {
						if _, err := io.ReadFull(f, row[:]); err != nil {
							break
						}
						day := int(binary.LittleEndian.Uint16(row[0:2]))
						if !yield(ofiTask{year, month, day}) {
							f.Close()
							return
						}
					}
				}
				f.Close()
			}
		}
	}
}

// findBlobOffset scans a single index.quantdev for a given day.
func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil || string(hdr[0:4]) != IdxMagic {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:16])

	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			return 0, 0
		}
		if int(binary.LittleEndian.Uint16(row[0:2])) == day {
			offset := binary.LittleEndian.Uint64(row[2:10])
			length := binary.LittleEndian.Uint64(row[10:18])
			return offset, length
		}
	}
	return 0, 0
}

func sprintfYear(y int) string  { return strconv.Itoa(y) }
func sprintfMonth(m int) string { return sprintf2(m) }

func sprintf2(x int) string {
	if x < 10 && x >= 0 {
		return "0" + strconv.Itoa(x)
	}
	return strconv.Itoa(x)
}
```

// --- End File: gnc.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime/debug"
)

func main() {
	// Slightly laxer GC; this is CPU-heavy research code.
	debug.SetGCPercent(200)

	if len(os.Args) < 2 {
		fmt.Println("Usage: go run . [test|probe]")
		return
	}

	switch os.Args[1] {
	case "test":
		// Full OOS research run (writes Continuous_Algo_Report_OOS.txt).
		RunTest()
	case "probe":
		// Structural sanity check of data under BaseDir.
		RunProbe()
	default:
		fmt.Println("Unknown command. Use 'test' or 'probe'")
	}
}
```

// --- End File: main.go ---

// --- File: math.go ---

```go
package main

import (
	"math"
)

// ContinuousModel defines a physics object that updates on dt/price/volume.
type ContinuousModel interface {
	Name() string
	Reset()
	Update(dt float64, p, v float64) float64
}

// ============================================================================
// 1. Baseline Hawkes_Intensity (keep as-is; this is your proven baseline)
// ============================================================================

type ModelHawkesIntensity struct {
	intensity float64
	alpha     float64
	beta      float64
}

func NewHawkesIntensity() *ModelHawkesIntensity {
	// Original parameters that produced your baseline table.
	return &ModelHawkesIntensity{alpha: 5.0, beta: 2.0}
}

func (m *ModelHawkesIntensity) Name() string { return "Hawkes_Intensity" }

func (m *ModelHawkesIntensity) Reset() { m.intensity = 0 }

func (m *ModelHawkesIntensity) Update(dt float64, p, v float64) float64 {
	if dt > 0 {
		m.intensity *= math.Exp(-m.beta * dt)
	}
	m.intensity += m.alpha * math.Log1p(v)
	return m.intensity
}

// ============================================================================
// 2. Hawkes_OFI: your new signed order-flow imbalance variant
// ============================================================================

type ModelHawkesOFI struct {
	buyInt  float64
	sellInt float64
	beta    float64 // decay rate
	lastP   float64
	init    bool
}

func NewHawkesOFI() *ModelHawkesOFI {
	// beta=0.002 -> half-life ~ 350s; longer memory than the baseline Hawkes.
	return &ModelHawkesOFI{beta: 0.002}
}

func (m *ModelHawkesOFI) Name() string { return "Hawkes_OFI" }

func (m *ModelHawkesOFI) Reset() {
	m.buyInt, m.sellInt, m.lastP, m.init = 0, 0, 0, false
}

func (m *ModelHawkesOFI) Update(dt float64, p, v float64) float64 {
	if !m.init {
		m.lastP = p
		m.init = true
		return 0
	}

	if dt > 0 {
		decay := math.Exp(-m.beta * dt)
		m.buyInt *= decay
		m.sellInt *= decay
	}

	impact := math.Log1p(v)

	// Tick rule on trades (no quotes available).
	if p > m.lastP {
		m.buyInt += impact
	} else if p < m.lastP {
		m.sellInt += impact
		// p == lastP is treated as neutral; only decay applies.
	}

	m.lastP = p
	return m.buyInt - m.sellInt
}

// ============================================================================
// 3. Streaming signature: Sig_LevyArea (with sign fix)
// ============================================================================

type ModelSignature struct {
	area      float64
	lastP     float64
	lastV     float64
	cumVol    float64
	decayRate float64
	init      bool
}

func NewSignature() *ModelSignature {
	// decay=0.001 -> tau ~ 1000s (~16 minutes), as you suggested.
	return &ModelSignature{decayRate: 0.001}
}

func (m *ModelSignature) Name() string { return "Sig_LevyArea" }

func (m *ModelSignature) Reset() {
	m.area, m.lastP, m.lastV, m.cumVol, m.init = 0, 0, 0, 0, false
}

func (m *ModelSignature) Update(dt float64, p, v float64) float64 {
	if !m.init {
		m.lastP, m.cumVol, m.lastV, m.init = p, v, v, true
		return 0
	}
	m.cumVol += v

	// Lead-lag area increment (same structure as before).
	inc := (p-m.lastP)*m.cumVol - (m.cumVol-m.lastV)*m.lastP

	if dt > 0 {
		m.area *= math.Exp(-m.decayRate * dt)
	}
	m.area += inc

	m.lastP = p
	m.lastV = m.cumVol

	// IMPORTANT: flip sign to correct the observed systematic anti-signal.
	return -m.area
}

// ============================================================================
// 4. Hilbert_Phase: robust, symplectic oscillator implementation
// ============================================================================

type ModelHilbert struct {
	x1, x2 float64 // x1: smoothed price, x2: velocity
	r, h   float64 // r: natural frequency, h: damping ratio
	init   bool
}

func NewHilbert() *ModelHilbert {
	// r=0.005 -> T ≈ 2π/r ≈ 21 minutes; good coarse-grained cycle.
	// h=1.0   -> critical damping (stable, non-oscillatory).
	return &ModelHilbert{r: 0.005, h: 1.0}
}

// Keep the original name so reports remain on the same row label.
func (m *ModelHilbert) Name() string { return "Hilbert_Phase" }

func (m *ModelHilbert) Reset() { m.x1, m.x2, m.init = 0, 0, false }

func (m *ModelHilbert) Update(dt float64, p, v float64) float64 {
	if !m.init {
		m.x1, m.init = p, true
		return 0
	}
	if dt <= 0 {
		// Return current phase estimate if no time passed.
		// Use the same real/imag decomposition as below.
		re := p - m.x1
		im := m.x2 / m.r
		return math.Atan2(im, re)
	}

	// x'' + 2*h*r*x' + r^2*(x - p) = 0
	err := m.x1 - p
	acc := -m.r*m.r*err - 2.0*m.r*m.h*m.x2

	// Symplectic Euler (semi-implicit) for stability.
	m.x2 += acc * dt
	m.x1 += m.x2 * dt

	re := p - m.x1
	im := m.x2 / m.r

	phase := math.Atan2(im, re)
	if math.IsNaN(phase) || math.IsInf(phase, 0) {
		// Safety clamp: if something goes wrong numerically, don’t poison the series.
		return 0
	}
	return phase
}

// ============================================================================
// 5. Model registry
// ============================================================================

func GetContinuousModels() []ContinuousModel {
	return []ContinuousModel{
		NewHawkesIntensity(), // baseline, proven positive
		NewHawkesOFI(),       // your new OFI-based variant
		NewSignature(),       // sign-corrected signature
		NewHilbert(),         // robust Hilbert_Phase
	}
}
```

// --- End File: math.go ---

// --- File: metrics.go ---

```go
package main

import (
	"math"
	"sort"
)

// Consolidated OOS statistics for a single (model, horizon) pair.
type ReportStats struct {
	TrainCount int
	TestCount  int

	// Correlation / IC (OOS, test-only)
	PearsonIC  float64
	SpearmanIC float64

	// Directional accuracy (OOS)
	HitRate  float64 // fraction of non-zero returns where sign(signal) == sign(return)
	HitRateZ float64 // z-score vs 50% baseline (binomial approximation)

	// Conditional return curve (deciles, OOS)
	DecileMean         []float64 // length 10, in raw return units
	TopDecileRetBps    float64
	BottomDecileRetBps float64
	SpreadBps          float64 // TopDecile - BottomDecile (bps)

	// Information theoretic (OOS)
	MutualInfo   float64 // bits
	NormalizedMI float64 // MI / H(Y)

	// Probabilistic forecast quality (train on train, evaluate on test)
	BaselineLogLoss float64
	SignalLogLoss   float64
	DeltaLogLoss    float64 // Baseline - Signal; >0 is better

	// Economic / risk metrics for sign(signal) strategy (OOS)
	Sharpe       float64
	MaxDrawdown  float64
	AvgTrade     float64
	AvgWin       float64
	AvgLoss      float64
	WinLossRatio float64
}

// OOS rolling-window metrics on the test segment.
type WindowMetrics struct {
	StartTime float64
	EndTime   float64
	Count     int

	PearsonIC  float64
	SpearmanIC float64
	HitRate    float64
	Sharpe     float64
}

// OOS regime metrics (volatility or time-of-day on test segment).
type RegimeMetrics struct {
	Name  string
	Count int

	PearsonIC  float64
	SpearmanIC float64
	HitRate    float64
	Sharpe     float64
}

// internal helper for chronological train/test split
type trainTestSplit struct {
	TrainF []float64
	TrainR []float64

	TestT []float64
	TestF []float64
	TestR []float64
}

// AnalyzeFullSuiteOOS computes all core metrics OOS, with a single chronological
// train/test split for a given (model, horizon) signal.
func AnalyzeFullSuiteOOS(times, feats, returns []float64, trainFrac float64) ReportStats {
	s := splitTrainTest(times, feats, returns, trainFrac)
	trainN := len(s.TrainF)
	testN := len(s.TestF)

	stats := ReportStats{
		TrainCount: trainN,
		TestCount:  testN,
		DecileMean: make([]float64, 10),
	}
	if testN < 30 {
		// Too little test data to say anything meaningful.
		return stats
	}

	// 1. ICs (test-only)
	stats.PearsonIC = Pearson(s.TestF, s.TestR)
	stats.SpearmanIC = Spearman(s.TestF, s.TestR)

	// 2. Hit rate vs 50% baseline (test-only)
	stats.HitRate, stats.HitRateZ = HitRateStats(s.TestF, s.TestR)

	// 3. Conditional return curve (deciles, test-only)
	stats.DecileMean, stats.BottomDecileRetBps, stats.TopDecileRetBps, stats.SpreadBps =
		DecileCurve(s.TestF, s.TestR)

	// 4. Mutual information + NMI (test-only)
	stats.MutualInfo, stats.NormalizedMI = CalcMutualInfo(s.TestF, s.TestR, 10)

	// 5. Δ Log-loss vs baseline:
	//    - baseline LL uses test labels only
	//    - logistic parameters fit on train, LL evaluated on test
	stats.BaselineLogLoss, stats.SignalLogLoss, stats.DeltaLogLoss =
		LogLossImprovementTrainTest(s.TrainF, s.TrainR, s.TestF, s.TestR)

	// 6. Sharpe + basic risk profile (test-only)
	stats.Sharpe, stats.MaxDrawdown, stats.AvgTrade, stats.AvgWin, stats.AvgLoss, stats.WinLossRatio =
		StrategyRiskStats(s.TestF, s.TestR)

	return stats
}

// RollingWindowMetricsOOS computes OOS metrics over multiple contiguous time
// windows on the test segment (after the same train/test split).
func RollingWindowMetricsOOS(times, feats, returns []float64, trainFrac float64, windows int) []WindowMetrics {
	s := splitTrainTest(times, feats, returns, trainFrac)
	n := len(s.TestF)
	if n < 60 || windows <= 0 {
		return nil
	}

	// Require at least ~20 points per window.
	if windows > n/20 {
		windows = n / 20
	}
	if windows < 1 {
		windows = 1
	}

	wSize := n / windows
	var out []WindowMetrics

	for w := 0; w < windows; w++ {
		start := w * wSize
		end := (w + 1) * wSize
		if w == windows-1 {
			end = n
		}
		if end-start < 20 {
			continue
		}

		sig := s.TestF[start:end]
		ret := s.TestR[start:end]
		tStart := s.TestT[start]
		tEnd := s.TestT[end-1]

		hit, _ := HitRateStats(sig, ret)
		sh, _, _, _, _, _ := StrategyRiskStats(sig, ret)

		out = append(out, WindowMetrics{
			StartTime:  tStart,
			EndTime:    tEnd,
			Count:      len(sig),
			PearsonIC:  Pearson(sig, ret),
			SpearmanIC: Spearman(sig, ret),
			HitRate:    hit,
			Sharpe:     sh,
		})
	}
	return out
}

// VolRegimeMetricsOOS computes OOS metrics across volatility regimes
// (low/medium/high), based on |return| within the test segment.
func VolRegimeMetricsOOS(times, feats, returns []float64, trainFrac float64) []RegimeMetrics {
	s := splitTrainTest(times, feats, returns, trainFrac)
	n := len(s.TestR)
	if n < 60 {
		return nil
	}

	vols := make([]float64, n)
	for i := 0; i < n; i++ {
		vols[i] = math.Abs(s.TestR[i])
	}

	sorted := make([]float64, n)
	copy(sorted, vols)
	sort.Float64s(sorted)

	q1 := sorted[n/3]
	q2 := sorted[2*n/3]

	var idxLow, idxMed, idxHigh []int
	for i := 0; i < n; i++ {
		v := vols[i]
		switch {
		case v <= q1:
			idxLow = append(idxLow, i)
		case v <= q2:
			idxMed = append(idxMed, i)
		default:
			idxHigh = append(idxHigh, i)
		}
	}

	makeRegime := func(name string, idxs []int) RegimeMetrics {
		if len(idxs) < 20 {
			return RegimeMetrics{Name: name, Count: len(idxs)}
		}
		sig := make([]float64, len(idxs))
		ret := make([]float64, len(idxs))
		for j, i := range idxs {
			sig[j] = s.TestF[i]
			ret[j] = s.TestR[i]
		}
		hit, _ := HitRateStats(sig, ret)
		sh, _, _, _, _, _ := StrategyRiskStats(sig, ret)
		return RegimeMetrics{
			Name:       name,
			Count:      len(idxs),
			PearsonIC:  Pearson(sig, ret),
			SpearmanIC: Spearman(sig, ret),
			HitRate:    hit,
			Sharpe:     sh,
		}
	}

	return []RegimeMetrics{
		makeRegime("VolLow", idxLow),
		makeRegime("VolMed", idxMed),
		makeRegime("VolHigh", idxHigh),
	}
}

// TimeOfDayRegimeMetricsOOS computes OOS metrics across time-of-day regimes
// (early / mid / late) on the test segment, using ms-of-day from timestamps.
func TimeOfDayRegimeMetricsOOS(times, feats, returns []float64, trainFrac float64) []RegimeMetrics {
	s := splitTrainTest(times, feats, returns, trainFrac)
	n := len(s.TestT)
	if n < 60 {
		return nil
	}

	const dayMillis = 24 * 60 * 60 * 1000.0
	third := dayMillis / 3.0

	var earlyIdx, midIdx, lateIdx []int
	for i := 0; i < n; i++ {
		tod := math.Mod(s.TestT[i], dayMillis)
		switch {
		case tod < third:
			earlyIdx = append(earlyIdx, i)
		case tod < 2*third:
			midIdx = append(midIdx, i)
		default:
			lateIdx = append(lateIdx, i)
		}
	}

	makeRegime := func(name string, idxs []int) RegimeMetrics {
		if len(idxs) < 20 {
			return RegimeMetrics{Name: name, Count: len(idxs)}
		}
		sig := make([]float64, len(idxs))
		ret := make([]float64, len(idxs))
		for j, i := range idxs {
			sig[j] = s.TestF[i]
			ret[j] = s.TestR[i]
		}
		hit, _ := HitRateStats(sig, ret)
		sh, _, _, _, _, _ := StrategyRiskStats(sig, ret)
		return RegimeMetrics{
			Name:       name,
			Count:      len(idxs),
			PearsonIC:  Pearson(sig, ret),
			SpearmanIC: Spearman(sig, ret),
			HitRate:    hit,
			Sharpe:     sh,
		}
	}

	return []RegimeMetrics{
		makeRegime("TOD_Early", earlyIdx),
		makeRegime("TOD_Mid", midIdx),
		makeRegime("TOD_Late", lateIdx),
	}
}

// ---------------------- shared train/test split ----------------------

type parallelSorter struct {
	times, feats, rets []float64
}

func (p parallelSorter) Len() int { return len(p.times) }

func (p parallelSorter) Swap(i, j int) {
	p.times[i], p.times[j] = p.times[j], p.times[i]
	p.feats[i], p.feats[j] = p.feats[j], p.feats[i]
	p.rets[i], p.rets[j] = p.rets[j], p.rets[i]
}

func (p parallelSorter) Less(i, j int) bool {
	return p.times[i] < p.times[j]
}

func splitTrainTest(times, feats, returns []float64, trainFrac float64) trainTestSplit {
	n := len(feats)
	if n == 0 || n != len(returns) || n != len(times) {
		return trainTestSplit{}
	}
	if trainFrac <= 0 || trainFrac >= 1 {
		trainFrac = 0.7
	}

	// Sort all three slices chronologically by time in place.
	sort.Sort(parallelSorter{times: times, feats: feats, rets: returns})

	trainN := int(trainFrac * float64(n))
	if trainN < 20 {
		trainN = 20
	}
	if trainN > n-30 {
		trainN = n - 30
	}
	if trainN <= 0 || trainN >= n {
		trainN = n / 2
	}
	testN := n - trainN
	if testN <= 0 {
		return trainTestSplit{}
	}

	return trainTestSplit{
		TrainF: feats[:trainN],
		TrainR: returns[:trainN],

		TestT: times[trainN:],
		TestF: feats[trainN:],
		TestR: returns[trainN:],
	}
}

// ---------------------- Correlation / IC ----------------------

// Pearson returns the Pearson correlation coefficient between x and y.
func Pearson(x, y []float64) float64 {
	n := len(x)
	if n == 0 || n != len(y) {
		return 0
	}
	var sx, sy, sxx, syy, sxy float64
	for i := 0; i < n; i++ {
		xi := x[i]
		yi := y[i]
		sx += xi
		sy += yi
		sxx += xi * xi
		syy += yi * yi
		sxy += xi * yi
	}
	nf := float64(n)
	num := sxy - sx*sy/nf
	denx := sxx - sx*sx/nf
	deny := syy - sy*sy/nf
	if denx <= 0 || deny <= 0 {
		return 0
	}
	return num / math.Sqrt(denx*deny)
}

// Spearman rank correlation: Pearson over rank-transformed inputs.
func Spearman(x, y []float64) float64 {
	n := len(x)
	if n == 0 || n != len(y) {
		return 0
	}

	rx := rankify(x)
	ry := rankify(y)
	return Pearson(rx, ry)
}

// rankify converts values to average ranks (1..n). Ties get averaged ranks.
func rankify(vals []float64) []float64 {
	n := len(vals)
	type kv struct {
		v float64
		i int
	}
	tmp := make([]kv, n)
	for i, v := range vals {
		tmp[i] = kv{v: v, i: i}
	}
	sort.Slice(tmp, func(i, j int) bool { return tmp[i].v < tmp[j].v })

	ranks := make([]float64, n)
	var i int
	for i < n {
		j := i + 1
		for j < n && tmp[j].v == tmp[i].v {
			j++
		}
		// average rank of [i, j)
		rank := 0.5*float64(i+j-1) + 1.0
		for k := i; k < j; k++ {
			ranks[tmp[k].i] = rank
		}
		i = j
	}
	return ranks
}

// ---------------------- Hit rate / sign accuracy ----------------------

// HitRateStats computes:
//   - hit rate on non-zero returns where sign(signal) == sign(return)
//   - z-score vs 50% null hypothesis (binomial approximation).
func HitRateStats(signal, ret []float64) (hitRate, z float64) {
	n := len(signal)
	if n == 0 || n != len(ret) {
		return 0, 0
	}

	var hits, trials int
	for i := 0; i < n; i++ {
		r := ret[i]
		s := signal[i]
		if r == 0 || s == 0 {
			continue
		}
		trials++
		if (r > 0 && s > 0) || (r < 0 && s < 0) {
			hits++
		}
	}
	if trials == 0 {
		return 0, 0
	}
	hitRate = float64(hits) / float64(trials)

	// Binomial z vs p0 = 0.5
	p0 := 0.5
	variance := p0 * (1 - p0) / float64(trials)
	if variance <= 0 {
		return hitRate, 0
	}
	z = (hitRate - p0) / math.Sqrt(variance)
	return hitRate, z
}

// ---------------------- Decile curve ----------------------

// DecileCurve builds a conditional return curve by signal decile.
// Returns:
//
//	decMeans[10]       - average raw return per decile
//	bottomBps, topBps  - decile 0 and 9 in basis points
//	spreadBps          - top - bottom in basis points
func DecileCurve(signal, ret []float64) (decMeans []float64, bottomBps, topBps, spreadBps float64) {
	n := len(signal)
	if n == 0 || n != len(ret) {
		return make([]float64, 10), 0, 0, 0
	}

	type pair struct {
		s float64
		r float64
	}
	data := make([]pair, 0, n)
	for i := 0; i < n; i++ {
		data = append(data, pair{s: signal[i], r: ret[i]})
	}
	sort.Slice(data, func(i, j int) bool { return data[i].s < data[j].s })

	decMeans = make([]float64, 10)
	counts := make([]int, 10)
	if n < 10 {
		// not enough to split meaningfully
		for i := range data {
			decMeans[0] += data[i].r
			counts[0]++
		}
		if counts[0] > 0 {
			decMeans[0] /= float64(counts[0])
		}
		return decMeans, decMeans[0] * 1e4, decMeans[0] * 1e4, 0
	}

	for i := 0; i < n; i++ {
		dec := int(float64(i) / float64(n) * 10.0)
		if dec == 10 {
			dec = 9
		}
		decMeans[dec] += data[i].r
		counts[dec]++
	}
	for d := 0; d < 10; d++ {
		if counts[d] > 0 {
			decMeans[d] /= float64(counts[d])
		}
	}
	bottom := decMeans[0]
	top := decMeans[9]
	bottomBps = bottom * 1e4
	topBps = top * 1e4
	spreadBps = (top - bottom) * 1e4
	return decMeans, bottomBps, topBps, spreadBps
}

// ---------------------- Mutual information ----------------------

// CalcMutualInfo estimates MI(signal, return) in bits using a simple
// equal-frequency binning scheme. Bins must be >= 2.
func CalcMutualInfo(signal, ret []float64, bins int) (miBits, nmi float64) {
	n := len(signal)
	if n == 0 || n != len(ret) || bins < 2 {
		return 0, 0
	}

	// Quantile-based bins for signal and returns separately.
	sBin := quantileBins(signal, bins)
	rBin := quantileBins(ret, bins)

	// Joint and marginals.
	joint := make([][]float64, bins)
	for i := range joint {
		joint[i] = make([]float64, bins)
	}
	margS := make([]float64, bins)
	margR := make([]float64, bins)

	for i := 0; i < n; i++ {
		sb := sBin[i]
		rb := rBin[i]
		if sb < 0 || sb >= bins || rb < 0 || rb >= bins {
			continue
		}
		joint[sb][rb]++
		margS[sb]++
		margR[rb]++
	}

	nf := float64(n)
	for i := 0; i < bins; i++ {
		margS[i] /= nf
		margR[i] /= nf
		for j := 0; j < bins; j++ {
			joint[i][j] /= nf
		}
	}

	// Mutual information in bits.
	var mi float64
	for i := 0; i < bins; i++ {
		for j := 0; j < bins; j++ {
			p := joint[i][j]
			if p <= 0 {
				continue
			}
			px := margS[i]
			py := margR[j]
			if px <= 0 || py <= 0 {
				continue
			}
			mi += p * math.Log2(p/(px*py))
		}
	}

	// Entropy of Y (returns).
	var hy float64
	for j := 0; j < bins; j++ {
		p := margR[j]
		if p > 0 {
			hy -= p * math.Log2(p)
		}
	}
	if hy > 0 {
		nmi = mi / hy
	}
	return mi, nmi
}

// quantileBins assigns each value to a [0,bins) bin with equal counts as much
// as possible.
func quantileBins(vals []float64, bins int) []int {
	n := len(vals)
	type kv struct {
		v float64
		i int
	}
	tmp := make([]kv, n)
	for i, v := range vals {
		tmp[i] = kv{v: v, i: i}
	}
	sort.Slice(tmp, func(i, j int) bool { return tmp[i].v < tmp[j].v })

	out := make([]int, n)
	if n == 0 {
		return out
	}
	for i := 0; i < n; i++ {
		b := int(float64(i) / float64(n) * float64(bins))
		if b == bins {
			b = bins - 1
		}
		out[tmp[i].i] = b
	}
	return out
}

// ---------------------- Log-loss / logistic improvement ----------------------

// LogLossImprovementTrainTest fits a simple 1D logistic model on train
//
//	p(y>0 | f) = sigmoid(a + b * f)
//
// and compares its log-loss on test vs a constant-probability baseline.
func LogLossImprovementTrainTest(trainF, trainR, testF, testR []float64) (baseLL, signalLL, delta float64) {
	// Convert returns to binary labels: y = 1 if r > 0 else 0.
	toLabels := func(r []float64) []float64 {
		y := make([]float64, len(r))
		for i, v := range r {
			if v > 0 {
				y[i] = 1
			} else {
				y[i] = 0
			}
		}
		return y
	}
	yTrain := toLabels(trainR)
	yTest := toLabels(testR)

	if len(yTrain) == 0 || len(yTest) == 0 {
		return 0, 0, 0
	}

	// Baseline: constant probability = mean of test labels.
	var sumY float64
	for _, v := range yTest {
		sumY += v
	}
	p0 := sumY / float64(len(yTest))
	if p0 <= 0 {
		p0 = 1e-6
	}
	if p0 >= 1 {
		p0 = 1 - 1e-6
	}
	baseLL = avgLogLossConst(yTest, p0)

	// Fit 1D logistic regression on train.
	a, b := fitLogistic1D(trainF, yTrain)
	signalLL = avgLogLossLogistic(testF, yTest, a, b)

	delta = baseLL - signalLL
	return baseLL, signalLL, delta
}

func avgLogLossConst(y []float64, p float64) float64 {
	ll := 0.0
	for _, t := range y {
		if p <= 0 {
			p = 1e-6
		}
		if p >= 1 {
			p = 1 - 1e-6
		}
		if t > 0.5 {
			ll -= math.Log(p)
		} else {
			ll -= math.Log(1 - p)
		}
	}
	return ll / float64(len(y))
}

func avgLogLossLogistic(f, y []float64, a, b float64) float64 {
	n := len(f)
	if n == 0 || n != len(y) {
		return 0
	}
	ll := 0.0
	for i := 0; i < n; i++ {
		z := a + b*f[i]
		p := 1.0 / (1.0 + math.Exp(-z))
		if p <= 0 {
			p = 1e-6
		}
		if p >= 1 {
			p = 1 - 1e-6
		}
		if y[i] > 0.5 {
			ll -= math.Log(p)
		} else {
			ll -= math.Log(1 - p)
		}
	}
	return ll / float64(n)
}

// fitLogistic1D does a crude Newton-Raphson fit for a 1D logistic model.
// It is intentionally simple; we're not trying to be perfect here.
func fitLogistic1D(f, y []float64) (a, b float64) {
	n := len(f)
	if n == 0 || n != len(y) {
		return 0, 0
	}

	// Standardize features to improve conditioning.
	var meanF, varF float64
	for _, v := range f {
		meanF += v
	}
	meanF /= float64(n)
	for _, v := range f {
		d := v - meanF
		varF += d * d
	}
	if varF <= 0 {
		varF = 1
	}
	stdF := math.Sqrt(varF / float64(n))
	if stdF == 0 {
		stdF = 1
	}

	fn := make([]float64, n)
	for i := 0; i < n; i++ {
		fn[i] = (f[i] - meanF) / stdF
	}

	// Initialize params.
	a, b = 0.0, 0.0
	const iters = 25

	for iter := 0; iter < iters; iter++ {
		var g0, g1, h00, h01, h11 float64
		for i := 0; i < n; i++ {
			z := a + b*fn[i]
			p := 1.0 / (1.0 + math.Exp(-z))
			wi := p * (1 - p) // variance
			yi := y[i]

			g0 += (p - yi)
			g1 += (p - yi) * fn[i]

			h00 += wi
			h01 += wi * fn[i]
			h11 += wi * fn[i] * fn[i]
		}
		// Solve 2x2 system H * delta = -g
		det := h00*h11 - h01*h01
		if det == 0 {
			break
		}
		da := (-g0*h11 + g1*h01) / det
		db := (-g1*h00 + g0*h01) / det

		// Dampen updates.
		a += 0.5 * da
		b += 0.5 * db

		if math.Abs(da) < 1e-6 && math.Abs(db) < 1e-6 {
			break
		}
	}

	// Map back to original feature scale:
	//   z = a + b * ((f - meanF)/stdF) = (a - b*meanF/stdF) + (b/stdF)*f
	// so newA = a - b*meanF/stdF, newB = b/stdF.
	newA := a - b*meanF/stdF
	newB := b / stdF
	return newA, newB
}

// ---------------------- Strategy risk / Sharpe ----------------------

// StrategyRiskStats computes returns of a naive sign(signal) strategy:
//
//	r_strat = sign(signal) * return
//
// and then Sharpe, max drawdown, and simple trade stats.
func StrategyRiskStats(signal, ret []float64) (sharpe, maxDD, avgTrade, avgWin, avgLoss, winLoss float64) {
	n := len(signal)
	if n == 0 || n != len(ret) {
		return 0, 0, 0, 0, 0, 0
	}

	var trades []float64
	trades = trades[:0]

	for i := 0; i < n; i++ {
		s := signal[i]
		r := ret[i]
		if s == 0 || r == 0 {
			continue
		}
		var sign float64
		if s > 0 {
			sign = 1
		} else {
			sign = -1
		}
		trades = append(trades, sign*r)
	}

	m := len(trades)
	if m == 0 {
		return 0, 0, 0, 0, 0, 0
	}

	// Basic stats.
	var mean, m2 float64
	for _, x := range trades {
		mean += x
		m2 += x * x
	}
	mean /= float64(m)
	variance := m2/float64(m) - mean*mean
	if variance < 0 {
		variance = 0
	}
	std := math.Sqrt(variance)
	if std > 0 {
		sharpe = mean / std
	}

	avgTrade = mean

	// Win/loss stats + max drawdown
	var winSum, lossSum float64
	var winCount, lossCount int

	equity := 0.0
	peak := 0.0
	maxDrawdown := 0.0

	for _, x := range trades {
		if x > 0 {
			winSum += x
			winCount++
		} else if x < 0 {
			lossSum += x
			lossCount++
		}

		equity += x
		if equity > peak {
			peak = equity
		}
		dd := equity - peak
		if dd < maxDrawdown {
			maxDrawdown = dd
		}
	}

	if winCount > 0 {
		avgWin = winSum / float64(winCount)
	}
	if lossCount > 0 {
		avgLoss = lossSum / float64(lossCount)
	}
	if avgLoss != 0 {
		winLoss = math.Abs(avgWin / avgLoss)
	}

	// maxDrawdown is negative; return positive magnitude.
	return sharpe, -maxDrawdown, avgTrade, avgWin, avgLoss, winLoss
}
```

// --- End File: metrics.go ---

// --- File: probe.go ---

```go
package main

import (
	"fmt"
	"os"
	"sort"
	"text/tabwriter"
	"time"
)

// RunProbe performs a fast diagnostic over all symbols under BaseDir.
// It samples up to 16 days per symbol, runs LoadGNCFile + InflateGNC,
// and reports which symbols have healthy blobs.
func RunProbe() {
	start := time.Now()

	fmt.Println(">>> GNC DATA PROBE <<<")
	fmt.Printf("BaseDir: %s\n\n", BaseDir)

	// Discover symbols from filesystem.
	var symbols []string
	for sym := range discoverSymbols() {
		symbols = append(symbols, sym)
	}
	if len(symbols) == 0 {
		fmt.Println("No symbols discovered under BaseDir.")
		return
	}
	sort.Strings(symbols)

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintln(w, "SYMBOL\tIDX_DAYS\tSAMPLED\tOK\tFAIL\tFIRST_DAY\tLAST_DAY\tMIN_ROWS\tMAX_ROWS\tAVG_ROWS")
	fmt.Fprintln(w, "------\t--------\t-------\t--\t----\t---------\t--------\t--------\t--------\t--------")

	const samplePerSymbol = 16

	for _, sym := range symbols {
		// Collect all tasks (days) for this symbol.
		var tasks []ofiTask
		for t := range discoverTasks(sym) {
			tasks = append(tasks, t)
		}
		if len(tasks) == 0 {
			fmt.Fprintf(w, "%-8s\t0\t0\t0\t0\t-\t-\t0\t0\t0\n", sym)
			continue
		}

		// Sort tasks chronologically to get true FIRST_DAY / LAST_DAY.
		sort.Slice(tasks, func(i, j int) bool {
			if tasks[i].Year != tasks[j].Year {
				return tasks[i].Year < tasks[j].Year
			}
			if tasks[i].Month != tasks[j].Month {
				return tasks[i].Month < tasks[j].Month
			}
			return tasks[i].Day < tasks[j].Day
		})

		idxDays := len(tasks)
		first := tasks[0]
		last := tasks[len(tasks)-1]

		// Determine which indices to sample (spread across the history).
		sampled := samplePerSymbol
		if idxDays < sampled {
			sampled = idxDays
		}
		var sampleIdxs []int
		if sampled > 0 {
			step := idxDays / sampled
			if step < 1 {
				step = 1
			}
			for i, count := 0, 0; i < idxDays && count < sampled; i += step {
				sampleIdxs = append(sampleIdxs, i)
				count++
			}
			if len(sampleIdxs) == 0 {
				sampleIdxs = []int{0}
				sampled = 1
			} else {
				sampled = len(sampleIdxs)
			}
		}

		cols := DayColumnPool.Get().(*DayColumns)
		cols.Reset()
		var buf []byte

		okCount := 0
		failCount := 0
		var minRows, maxRows, totalRows int

		for _, idx := range sampleIdxs {
			t := tasks[idx]

			if !LoadGNCFile(BaseDir, sym, t, &buf) {
				failCount++
				fmt.Printf(
					"  [%s] %04d-%02d-%02d  STATUS=LOAD_FAIL   rows=0 reason=missing_or_unreadable_blob\n",
					sym, t.Year, t.Month, t.Day,
				)
				continue
			}
			rows, err := InflateGNC(buf, cols)
			if err != nil || rows <= 0 {
				failCount++
				fmt.Printf(
					"  [%s] %04d-%02d-%02d  STATUS=DECODE_FAIL rows=%d reason=%v\n",
					sym, t.Year, t.Month, t.Day, rows, err,
				)
				continue
			}

			okCount++
			if okCount == 1 {
				minRows, maxRows = rows, rows
			} else {
				if rows < minRows {
					minRows = rows
				}
				if rows > maxRows {
					maxRows = rows
				}
			}
			totalRows += rows
		}

		DayColumnPool.Put(cols)

		avgRows := 0
		if okCount > 0 {
			avgRows = totalRows / okCount
		}

		firstStr := fmt.Sprintf("%04d-%02d-%02d", first.Year, first.Month, first.Day)
		lastStr := fmt.Sprintf("%04d-%02d-%02d", last.Year, last.Month, last.Day)

		fmt.Fprintf(
			w,
			"%-8s\t%d\t%d\t%d\t%d\t%s\t%s\t%d\t%d\t%d\n",
			sym,
			idxDays,
			sampled,
			okCount,
			failCount,
			firstStr,
			lastStr,
			minRows,
			maxRows,
			avgRows,
		)
	}

	w.Flush()
	fmt.Printf("\n[probe] Finished in %s\n", time.Since(start))
}
```

// --- End File: probe.go ---

// --- File: stream.go ---

```go
package main

import (
	"math"
	"sort"
)

type StreamResult struct {
	Times       []int64   // [sample]
	Prices      []float64 // [sample]
	Features    []float64 // [sample * numModels]
	Targets     []float64 // [sample * numHorizons]
	NumModels   int
	NumHorizons int
}

func RunStream(cols *DayColumns, models []ContinuousModel) StreamResult {
	n := cols.Count
	if n < 100 {
		return StreamResult{}
	}

	numModels := len(models)
	numHorizons := len(HorizonDelays)

	for _, m := range models {
		m.Reset()
	}

	// Rough capacity estimate: one sample per minute.
	estSamples := n / 60
	if estSamples < 1 {
		estSamples = 1
	}

	res := StreamResult{
		Times:       make([]int64, 0, estSamples),
		Prices:      make([]float64, 0, estSamples),
		Features:    make([]float64, 0, estSamples*numModels),
		Targets:     nil, // filled after labeling
		NumModels:   numModels,
		NumHorizons: numHorizons,
	}

	// Scratch slice reused per tick to hold model outputs.
	currFeats := make([]float64, numModels)

	lastT := cols.Times[0]
	nextSampleT := lastT + (SamplingRateSec * 1000)

	for i := 0; i < n; i++ {
		t := cols.Times[i]
		p := cols.Prices[i]
		v := cols.Qtys[i]

		dt := float64(t-lastT) / 1000.0
		if dt < 0 {
			dt = 0
		}
		lastT = t

		for j, m := range models {
			currFeats[j] = m.Update(dt, p, v)
		}

		if t >= nextSampleT {
			// Append one sample row.
			res.Times = append(res.Times, t)
			res.Prices = append(res.Prices, p)
			res.Features = append(res.Features, currFeats...)

			for t >= nextSampleT {
				nextSampleT += (SamplingRateSec * 1000)
			}
		}
	}

	sampleCount := len(res.Times)
	if sampleCount == 0 {
		return StreamResult{}
	}

	// Lookahead labeling on the flat arrays.
	maxTime := cols.Times[n-1]
	res.Targets = make([]float64, sampleCount*numHorizons)

	validCount := 0
	ticksTimes := cols.Times
	ticksPrices := cols.Prices

	for i := 0; i < sampleCount; i++ {
		basePrice := res.Prices[i]
		sampleT := res.Times[i]

		valid := true
		baseTarg := validCount * numHorizons

		for hIdx, delay := range HorizonDelays {
			targetT := sampleT + delay
			if targetT > maxTime {
				valid = false
				break
			}

			// Binary search for first tick with time >= targetT.
			idx := sort.Search(n, func(k int) bool {
				return ticksTimes[k] >= targetT
			})
			if idx == n {
				valid = false
				break
			}
			foundP := ticksPrices[idx]
			if foundP <= 0 {
				valid = false
				break
			}

			res.Targets[baseTarg+hIdx] = math.Log(foundP / basePrice)
		}

		if !valid {
			continue
		}

		// Pack valid rows to the front (Times, Prices, Features).
		if validCount != i {
			res.Times[validCount] = sampleT
			res.Prices[validCount] = basePrice

			srcFeat := i * numModels
			dstFeat := validCount * numModels
			copy(res.Features[dstFeat:dstFeat+numModels], res.Features[srcFeat:srcFeat+numModels])
		}

		validCount++
	}

	if validCount == 0 {
		return StreamResult{}
	}

	res.Times = res.Times[:validCount]
	res.Prices = res.Prices[:validCount]
	res.Features = res.Features[:validCount*numModels]
	res.Targets = res.Targets[:validCount*numHorizons]

	return res
}
```

// --- End File: stream.go ---

// --- File: test.go ---

```go
package main

import (
	"fmt"
	"os"
	"sort"
	"sync"
	"sync/atomic"
	"text/tabwriter"
	"time"
)

type ResultContainer struct {
	Times []float64
	Feats []float64
	Targs []float64
}

// Per-worker storage: [horizon][model] -> ResultContainer
type WorkerResults struct {
	Data [][]*ResultContainer
}

func RunTest() {
	start := time.Now()

	models := GetContinuousModels()
	modelNames := make([]string, len(models))
	for i, m := range models {
		modelNames[i] = m.Name()
	}

	fmt.Printf(">>> CONTINUOUS-TIME ALGO DISCOVERY (OOS REPORT) <<<\n")
	fmt.Printf("   Workers: %d | Models: %d\n", CPUThreads, len(models))

	// Global results[horizon][model].
	results := make([][]*ResultContainer, len(HorizonLabels))
	for h := range results {
		results[h] = make([]*ResultContainer, len(models))
		for m := range results[h] {
			results[h][m] = &ResultContainer{}
		}
	}

	// Collect all (year,month,day) tasks for the chosen symbol.
	tasks := make([]ofiTask, 0)
	sym := Symbol()
	for t := range discoverTasks(sym) {
		tasks = append(tasks, t)
	}

	if len(tasks) == 0 {
		fmt.Println("No tasks discovered; nothing to do.")
		return
	}

	// Sort tasks chronologically so workers process days in a sensible order.
	sort.Slice(tasks, func(i, j int) bool {
		if tasks[i].Year != tasks[j].Year {
			return tasks[i].Year < tasks[j].Year
		}
		if tasks[i].Month != tasks[j].Month {
			return tasks[i].Month < tasks[j].Month
		}
		return tasks[i].Day < tasks[j].Day
	})

	// Per-worker result storage.
	workerResults := make([]*WorkerResults, CPUThreads)
	for i := 0; i < CPUThreads; i++ {
		wr := &WorkerResults{
			Data: make([][]*ResultContainer, len(HorizonLabels)),
		}
		for h := range wr.Data {
			wr.Data[h] = make([]*ResultContainer, len(models))
			for m := range wr.Data[h] {
				wr.Data[h][m] = &ResultContainer{}
			}
		}
		workerResults[i] = wr
	}

	// Task channel and worker pool.
	taskCh := make(chan ofiTask, len(tasks))
	for _, t := range tasks {
		taskCh <- t
	}
	close(taskCh)

	var wg sync.WaitGroup
	var processed atomic.Int64

	for wID := 0; wID < CPUThreads; wID++ {
		wg.Add(1)
		go func(id int) {
			defer wg.Done()

			localStore := workerResults[id]
			localModels := GetContinuousModels()

			cols := DayColumnPool.Get().(*DayColumns)
			defer DayColumnPool.Put(cols)

			var buf []byte

			for task := range taskCh {
				if !LoadGNCFile(BaseDir, sym, task, &buf) {
					continue
				}
				if _, err := InflateGNC(buf, cols); err != nil {
					continue
				}

				streamRes := RunStream(cols, localModels)
				if len(streamRes.Times) == 0 {
					continue
				}

				numSamples := len(streamRes.Times)
				numModels := streamRes.NumModels
				numHorizons := streamRes.NumHorizons

				// Append into thread-local storage.
				for s := 0; s < numSamples; s++ {
					t := float64(streamRes.Times[s])

					featBase := s * numModels
					targBase := s * numHorizons

					for mIdx := 0; mIdx < numModels; mIdx++ {
						featVal := streamRes.Features[featBase+mIdx]
						for hIdx := 0; hIdx < numHorizons; hIdx++ {
							targVal := streamRes.Targets[targBase+hIdx]

							rc := localStore.Data[hIdx][mIdx]
							rc.Times = append(rc.Times, t)
							rc.Feats = append(rc.Feats, featVal)
							rc.Targs = append(rc.Targs, targVal)
						}
					}
				}

				processed.Add(1)
			}
		}(wID)
	}
	wg.Wait()

	// Merge worker-local results into global results.
	for wID := 0; wID < CPUThreads; wID++ {
		wr := workerResults[wID]
		for hIdx := range HorizonLabels {
			for mIdx := range models {
				src := wr.Data[hIdx][mIdx]
				dst := results[hIdx][mIdx]

				if len(src.Times) == 0 {
					continue
				}

				dst.Times = append(dst.Times, src.Times...)
				dst.Feats = append(dst.Feats, src.Feats...)
				dst.Targs = append(dst.Targs, src.Targs...)
			}
		}
	}

	// ---------------------------------------------------------------------
	// Reporting phase
	// ---------------------------------------------------------------------

	f, _ := os.Create("Continuous_Algo_Report_OOS.txt")
	defer f.Close()
	w := tabwriter.NewWriter(f, 0, 0, 1, ' ', 0)

	const trainFrac = 0.7 // 70% earliest samples train, 30% latest samples test

	// 1) Core OOS summary, per model × horizon
	fmt.Fprintf(w, "MODEL\tHORIZON\tTrainN\tTestN\tPearsonIC\tSpearmanIC\tHitRate\tHitZ\tSharpe\tSpread(bps)\tTopDecile(bps)\tBotDecile(bps)\tMI(bits)\tNMI\tΔLogLoss\n")
	fmt.Fprintf(w, "-----\t-------\t------\t-----\t---------\t-----------\t-------\t----\t------\t-----------\t--------------\t---------------\t--------\t---\t--------\n")

	for mIdx, name := range modelNames {
		for hIdx, hName := range HorizonLabels {
			data := results[hIdx][mIdx]
			if len(data.Feats) == 0 {
				continue
			}

			stats := AnalyzeFullSuiteOOS(data.Times, data.Feats, data.Targs, trainFrac)
			if stats.TestCount == 0 {
				continue
			}

			fmt.Fprintf(
				w,
				"%s\t%s\t%d\t%d\t%.4f\t%.4f\t%.3f\t%.2f\t%.3f\t%+.1f\t%+.1f\t%+.1f\t%.3f\t%.3f\t%.4f\n",
				name,
				hName,
				stats.TrainCount,
				stats.TestCount,
				stats.PearsonIC,
				stats.SpearmanIC,
				stats.HitRate,
				stats.HitRateZ,
				stats.Sharpe,
				stats.SpreadBps,
				stats.TopDecileRetBps,
				stats.BottomDecileRetBps,
				stats.MutualInfo,
				stats.NormalizedMI,
				stats.DeltaLogLoss,
			)
		}
		fmt.Fprintf(w, "\n")
	}

	// 2) Rolling OOS metrics on the test segment
	fmt.Fprintf(w, "\n\n# Rolling OOS metrics (test segment only)\n")
	fmt.Fprintf(w, "MODEL\tHORIZON\tWIN\tCount\tPearsonIC\tSpearmanIC\tHitRate\tSharpe\n")
	fmt.Fprintf(w, "-----\t-------\t---\t-----\t---------\t-----------\t-------\t------\n")

	const rollingWindows = 8

	for mIdx, name := range modelNames {
		for hIdx, hName := range HorizonLabels {
			data := results[hIdx][mIdx]
			if len(data.Feats) == 0 {
				continue
			}
			wins := RollingWindowMetricsOOS(data.Times, data.Feats, data.Targs, trainFrac, rollingWindows)
			for winIdx, wm := range wins {
				if wm.Count == 0 {
					continue
				}
				fmt.Fprintf(
					w,
					"%s\t%s\t%d\t%d\t%.4f\t%.4f\t%.3f\t%.3f\n",
					name,
					hName,
					winIdx,
					wm.Count,
					wm.PearsonIC,
					wm.SpearmanIC,
					wm.HitRate,
					wm.Sharpe,
				)
			}
		}
		fmt.Fprintf(w, "\n")
	}

	// 3) Volatility regime OOS metrics
	fmt.Fprintf(w, "\n\n# Volatility regime OOS metrics (test segment only)\n")
	fmt.Fprintf(w, "MODEL\tHORIZON\tREGIME\tCount\tPearsonIC\tSpearmanIC\tHitRate\tSharpe\n")
	fmt.Fprintf(w, "-----\t-------\t------\t-----\t---------\t-----------\t-------\t------\n")

	for mIdx, name := range modelNames {
		for hIdx, hName := range HorizonLabels {
			data := results[hIdx][mIdx]
			if len(data.Feats) == 0 {
				continue
			}
			regs := VolRegimeMetricsOOS(data.Times, data.Feats, data.Targs, trainFrac)
			for _, rm := range regs {
				if rm.Count == 0 {
					continue
				}
				fmt.Fprintf(
					w,
					"%s\t%s\t%s\t%d\t%.4f\t%.4f\t%.3f\t%.3f\n",
					name,
					hName,
					rm.Name,
					rm.Count,
					rm.PearsonIC,
					rm.SpearmanIC,
					rm.HitRate,
					rm.Sharpe,
				)
			}
		}
		fmt.Fprintf(w, "\n")
	}

	// 4) Time-of-day regime OOS metrics
	fmt.Fprintf(w, "\n\n# Time-of-day regime OOS metrics (test segment only)\n")
	fmt.Fprintf(w, "MODEL\tHORIZON\tREGIME\tCount\tPearsonIC\tSpearmanIC\tHitRate\tSharpe\n")
	fmt.Fprintf(w, "-----\t-------\t------\t-----\t---------\t-----------\t-------\t------\n")

	for mIdx, name := range modelNames {
		for hIdx, hName := range HorizonLabels {
			data := results[hIdx][mIdx]
			if len(data.Feats) == 0 {
				continue
			}
			regs := TimeOfDayRegimeMetricsOOS(data.Times, data.Feats, data.Targs, trainFrac)
			for _, rm := range regs {
				if rm.Count == 0 {
					continue
				}
				fmt.Fprintf(
					w,
					"%s\t%s\t%s\t%d\t%.4f\t%.4f\t%.3f\t%.3f\n",
					name,
					hName,
					rm.Name,
					rm.Count,
					rm.PearsonIC,
					rm.SpearmanIC,
					rm.HitRate,
					rm.Sharpe,
				)
			}
		}
		fmt.Fprintf(w, "\n")
	}

	w.Flush()
	fmt.Printf("Done. Processed %d days in %s. OOS report saved to Continuous_Algo_Report_OOS.txt\n", processed.Load(), time.Since(start))
}
```

// --- End File: test.go ---

