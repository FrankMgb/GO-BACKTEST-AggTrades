--- File Tree Structure ---
|-- data/
|-- atoms.go
|-- bench.go
|-- build.go
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- sanity.go
|-- study.go
    |-- BTCUSDT/
    |-- ETHUSDT/
    |-- features/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- BTCUSDT/
        |-- ETHUSDT/
            |-- Atoms_v4/
            |-- Atoms_v4/

// --- File: atoms.go ---

```go
package main

import "math"

// --- Interface ---

type Atom interface {
	Name() string
	Reset() // Clear state for new day
	// Update calculates the feature value for the current trade.
	Update(q, s, p, flow, dt float64) float64
}

// --- Constants ---

const (
	InputFlow = iota // Use q * s
	InputSign        // Use s (Trade Continuation / Direction)
)

// --- Registry ---

func GetActiveAtoms() []Atom {
	return []Atom{
		// --- Baseline ---
		&RawOFI{},
		&RawTCI{},

		// --- Standard EMA (Baseline) ---
		&EMAAtom{NameStr: "OFI_EMA_15s", Tau: 15.0, Input: InputFlow},
		&EMAAtom{NameStr: "TCI_EMA_15s", Tau: 15.0, Input: InputSign}, // Added TCI variant

		// --- Sniper Math (Lag Reduced) ---
		&DEMAAtom{NameStr: "OFI_DEMA_15s", Tau: 15.0, Input: InputFlow},
		&TEMAAtom{NameStr: "OFI_TEMA_15s", Tau: 15.0, Input: InputFlow},
		&DEMAAtom{NameStr: "OFI_DEMA_5s", Tau: 5.0, Input: InputFlow},

		// --- Sign-Based Sniper (New Capability!) ---
		&DEMAAtom{NameStr: "TCI_DEMA_15s", Tau: 15.0, Input: InputSign},

		// --- Advanced / Composite ---
		&RSIAtom{NameStr: "OFI_RSI_15s", Tau: 15.0, Input: InputFlow},
		&CubicAtom{NameStr: "OFI_Cubic_15s", Tau: 15.0, Input: InputFlow},

		// --- Volatility / Force ---
		&ImpulseAtom{NameStr: "Impulse_15s", Tau: 15.0, Input: InputFlow},  // Flow * Speed
		&ForceAtom{NameStr: "Force_DEMA_15s", Tau: 15.0, Input: InputFlow}, // DEMA * Speed
		&InstVelAtom{},

		// --- Experimental ---
		// Composite mixing TEMA and Cubic (both using InputFlow by default)
		&CompositeAtom{
			NameStr: "Sniper_Composite",
			Tau:     15.0,
			Input:   InputFlow,
			WeightA: 0.5,
			WeightB: 0.5,
		},
	}
}

// --- Helper ---

// resolveInput centralizes the Flow vs Sign logic.
// Inlined by compiler for zero overhead.
func resolveInput(mode int, flow, s float64) float64 {
	if mode == InputSign {
		return s
	}
	return flow
}

// --- Implementations ---

// 1. Raw OFI
type RawOFI struct{}

func (r *RawOFI) Name() string                             { return "OFI_Raw" }
func (r *RawOFI) Reset()                                   {}
func (r *RawOFI) Update(q, s, p, flow, dt float64) float64 { return flow }

// 2. Raw TCI
type RawTCI struct{}

func (r *RawTCI) Name() string                             { return "TCI_Raw" }
func (r *RawTCI) Reset()                                   {}
func (r *RawTCI) Update(q, s, p, flow, dt float64) float64 { return s }

// 3. Standard EMA
type EMAAtom struct {
	NameStr string
	Tau     float64
	Input   int
	val     float64
}

func (a *EMAAtom) Name() string { return a.NameStr }
func (a *EMAAtom) Reset()       { a.val = 0 }
func (a *EMAAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)
	a.val += alpha * (x - a.val)
	return a.val
}

// 4. DEMA (Double EMA)
type DEMAAtom struct {
	NameStr string
	Tau     float64
	Input   int
	e1, e2  float64
}

func (a *DEMAAtom) Name() string { return a.NameStr }
func (a *DEMAAtom) Reset()       { a.e1 = 0; a.e2 = 0 }
func (a *DEMAAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)
	a.e1 += alpha * (x - a.e1)
	a.e2 += alpha * (a.e1 - a.e2)
	return 2*a.e1 - a.e2
}

// 5. TEMA (Triple EMA)
type TEMAAtom struct {
	NameStr    string
	Tau        float64
	Input      int
	e1, e2, e3 float64
}

func (a *TEMAAtom) Name() string { return a.NameStr }
func (a *TEMAAtom) Reset()       { a.e1 = 0; a.e2 = 0; a.e3 = 0 }
func (a *TEMAAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)
	a.e1 += alpha * (x - a.e1)
	a.e2 += alpha * (a.e1 - a.e2)
	a.e3 += alpha * (a.e2 - a.e3)
	return 3*a.e1 - 3*a.e2 + a.e3
}

// 6. RSI (Relative Strength)
type RSIAtom struct {
	NameStr  string
	Tau      float64
	Input    int
	up, down float64
}

func (a *RSIAtom) Name() string { return a.NameStr }
func (a *RSIAtom) Reset()       { a.up = 0; a.down = 0 }
func (a *RSIAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)

	u, d := 0.0, 0.0
	if x > 0 {
		u = x
	} else {
		d = -x
	}

	a.up += alpha * (u - a.up)
	a.down += alpha * (d - a.down)
	sum := a.up + a.down

	if sum < 1e-9 {
		return 0.0
	}
	// Scaled -1 to 1
	return 2.0*(a.up/sum) - 1.0
}

// 7. Cubic (Non-Linear)
type CubicAtom struct {
	NameStr string
	Tau     float64
	Input   int
	val     float64
}

func (a *CubicAtom) Name() string { return a.NameStr }
func (a *CubicAtom) Reset()       { a.val = 0 }
func (a *CubicAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)

	// Track EMA of Cube
	x3 := x * x * x
	a.val += alpha * (x3 - a.val)

	// Return Cube Root to linearize scale for downstream tasks
	if a.val > 0 {
		return math.Cbrt(a.val)
	}
	return -math.Cbrt(-a.val)
}

// 8. Impulse (EMA * Speed)
type ImpulseAtom struct {
	NameStr string
	Tau     float64
	Input   int
	val     float64
}

func (a *ImpulseAtom) Name() string { return a.NameStr }
func (a *ImpulseAtom) Reset()       { a.val = 0 }
func (a *ImpulseAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)
	a.val += alpha * (x - a.val)

	// Speed Cap
	speed := 1.0 / dt // dt is guaranteed >= 1e-4 by build.go
	if speed > 100 {
		speed = 100
	}

	return a.val * speed
}

// 9. Force (DEMA * Speed)
type ForceAtom struct {
	NameStr string
	Tau     float64
	Input   int
	e1, e2  float64
}

func (a *ForceAtom) Name() string { return a.NameStr }
func (a *ForceAtom) Reset()       { a.e1 = 0; a.e2 = 0 }
func (a *ForceAtom) Update(q, s, p, flow, dt float64) float64 {
	x := resolveInput(a.Input, flow, s)
	alpha := 1.0 - math.Exp(-dt/a.Tau)

	a.e1 += alpha * (x - a.e1)
	a.e2 += alpha * (a.e1 - a.e2)
	dema := 2*a.e1 - a.e2

	speed := 1.0 / dt
	if speed > 100 {
		speed = 100
	}

	return dema * speed
}

// 10. Instant Velocity
type InstVelAtom struct{}

func (a *InstVelAtom) Name() string { return "Vel_Inst" }
func (a *InstVelAtom) Reset()       {}
func (a *InstVelAtom) Update(q, s, p, flow, dt float64) float64 {
	// Redundant check removed as requested
	speed := 1.0 / dt
	if speed > 100 {
		speed = 100
	}
	return flow * speed
}

// 11. Composite (TEMA + Cubic)
type CompositeAtom struct {
	NameStr          string
	Tau              float64
	Input            int
	WeightA, WeightB float64
	tema             TEMAAtom
	cubic            CubicAtom
}

func (a *CompositeAtom) Name() string { return a.NameStr }
func (a *CompositeAtom) Reset() {
	// Propagate configuration to children
	a.tema.Tau = a.Tau
	a.tema.Input = a.Input
	a.tema.Reset()

	a.cubic.Tau = a.Tau
	a.cubic.Input = a.Input
	a.cubic.Reset()
}
func (a *CompositeAtom) Update(q, s, p, flow, dt float64) float64 {
	v1 := a.tema.Update(q, s, p, flow, dt)
	v2 := a.cubic.Update(q, s, p, flow, dt)
	return a.WeightA*v1 + a.WeightB*v2
}
```

// --- End File: atoms.go ---

// --- File: bench.go ---

```go
package main

import (
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"runtime/pprof"
	"time"
)

// benchStats holds per-benchmark summary for the study path.
type benchStats struct {
	Name            string
	Iters           int
	RowsPerIter     int // number of time steps (rows) per day
	FeatPerIter     int // number of feature series per day (variants * dims)
	BytesPerIter    int // approximate feature bytes read per day
	Total           time.Duration
	AllocBytesPerOp uint64
	MallocsPerOp    uint64
}

// runBench is called from main when you do: go run . bench
func runBench() {
	fmt.Println("=== BENCHMARK: QuantDev STUDY (processStudyDay) ===")
	fmt.Printf("Go: %s | GOOS/GOARCH: %s/%s | Threads: %d\n",
		runtime.Version(),
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
	)

	sym, dayInt, variants, featRoot, ok := findStudySample()
	if !ok {
		fmt.Printf("[bench] no feature sets found under %q\n", filepath.Join(BaseDir, "features"))
		return
	}
	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100

	fmt.Printf("[bench] Sample symbol: %s | day: %04d-%02d-%02d\n", sym, y, m, d)
	fmt.Printf("[bench] Variants: %v\n", variants)

	featureBytes := featureBytesForDay(featRoot, variants, dayInt)
	if featureBytes > 0 {
		fmt.Printf("[bench] Approx feature bytes/day: %d\n", featureBytes)
	}

	// --- Warm-up to decide iteration count ---
	warmStats := benchStudy(sym, dayInt, variants, featRoot, 1)
	warm := warmStats.Total
	if warm <= 0 {
		warm = 2 * time.Millisecond
	}
	target := 500 * time.Millisecond
	iters := int(target / warm)
	if iters < 3 {
		iters = 3
	} else if iters > 2000 {
		iters = 2000
	}

	fmt.Printf("[bench] warm-up: %s per study, selecting %d iterations\n",
		warmStats.Total, iters)

	// --- CPU profile + real benchmark ---
	var cpuFile *os.File
	var err error
	cpuFile, err = os.Create("bench_cpu.pprof")
	if err != nil {
		fmt.Printf("[bench] cannot create CPU profile: %v\n", err)
	} else {
		if err := pprof.StartCPUProfile(cpuFile); err != nil {
			fmt.Printf("[bench] cannot start CPU profile: %v\n", err)
			cpuFile.Close()
			cpuFile = nil
		} else {
			fmt.Println("[bench] CPU profiling: ON")
		}
	}

	stats := benchStudy(sym, dayInt, variants, featRoot, iters)
	stats.BytesPerIter = featureBytes

	if cpuFile != nil {
		pprof.StopCPUProfile()
		cpuFile.Close()
		fmt.Println("[bench] CPU profile written to bench_cpu.pprof")
	}

	printBenchStats(stats)

	// --- Heap profile snapshot ---
	memFile, err := os.Create("bench_mem.pprof")
	if err != nil {
		fmt.Printf("[bench] cannot create heap profile: %v\n", err)
	} else {
		runtime.GC()
		if err := pprof.WriteHeapProfile(memFile); err != nil {
			fmt.Printf("[bench] cannot write heap profile: %v\n", err)
		} else {
			fmt.Println("[bench] Heap profile written to bench_mem.pprof")
		}
		memFile.Close()
	}

	runPprofTop("bench_cpu.pprof", "cpu")
	runPprofTop("bench_mem.pprof", "heap")

	fmt.Println("=== BENCHMARK COMPLETE ===")
}

// benchStudy repeatedly runs processStudyDay for one symbol/day
func benchStudy(sym string, dayInt int, variants []string, featRoot string, iters int) benchStats {
	stats := benchStats{
		Name:  "StudyDay",
		Iters: iters,
	}

	if iters <= 0 {
		return stats
	}

	var sigBuf []float64
	var fileBuf []byte
	var retBuf []float64
	retsPerHBuf := make([][]float64, len(TimeHorizonsMS))
	var gncBuf []byte

	runtime.GC()

	var m0, m1 runtime.MemStats
	runtime.ReadMemStats(&m0)

	start := time.Now()
	for i := 0; i < iters; i++ {
		// FIXED: Removed doQuantiles bool argument
		res := processStudyDay(
			sym, dayInt, variants, featRoot,
			&sigBuf, &fileBuf, &retBuf, &retsPerHBuf, &gncBuf,
		)

		if i == 0 {
			rows := 0
			// V3 DayResult just has Metrics map[string][]Moments
			for _, momsSlice := range res.Metrics {
				if len(momsSlice) > 0 {
					rows = int(momsSlice[0].Count)
					break
				}
			}
			stats.RowsPerIter = rows
			stats.FeatPerIter = len(res.Metrics)
		}
	}
	stats.Total = time.Since(start)

	runtime.ReadMemStats(&m1)
	allocBytes := m1.TotalAlloc - m0.TotalAlloc
	mallocs := m1.Mallocs - m0.Mallocs

	if iters > 0 {
		stats.AllocBytesPerOp = allocBytes / uint64(iters)
		stats.MallocsPerOp = mallocs / uint64(iters)
	}

	return stats
}

func printBenchStats(bs benchStats) {
	if bs.Iters <= 0 || bs.Total <= 0 {
		fmt.Printf("[bench] %s: no data\n", bs.Name)
		return
	}

	nsPerOp := float64(bs.Total.Nanoseconds()) / float64(bs.Iters)
	totalRows := float64(bs.RowsPerIter * bs.Iters)
	totalBytes := float64(bs.BytesPerIter * bs.Iters)
	totalCells := totalRows * float64(bs.FeatPerIter)

	secs := bs.Total.Seconds()
	rowsPerSec := 0.0
	bytesPerSec := 0.0
	cellsPerSec := 0.0
	if secs > 0 {
		rowsPerSec = totalRows / secs
		bytesPerSec = totalBytes / secs
		cellsPerSec = totalCells / secs
	}

	fmt.Printf("\n[bench] %s\n", bs.Name)
	fmt.Printf("  iters:         %d\n", bs.Iters)
	fmt.Printf("  rows/iter:     %d\n", bs.RowsPerIter)
	fmt.Printf("  features/iter: %d\n", bs.FeatPerIter)
	if bs.BytesPerIter > 0 {
		fmt.Printf("  bytes/iter:    %d (feature files)\n", bs.BytesPerIter)
	}
	fmt.Printf("  total time:    %s\n", bs.Total)
	fmt.Printf("  ns/op:         %.0f\n", nsPerOp)

	fmt.Printf("  throughput:    %.3f krows/s", rowsPerSec/1e3)
	if bs.FeatPerIter > 0 {
		fmt.Printf(", %.3f Mcells/s", cellsPerSec/1e6)
	}
	if bs.BytesPerIter > 0 {
		fmt.Printf(", %.3f MB/s (features)\n", bytesPerSec/(1024*1024))
	} else {
		fmt.Println()
	}

	fmt.Printf("  allocs/op:     %d mallocs/op, %d B/op\n",
		bs.MallocsPerOp, bs.AllocBytesPerOp)
}

func runPprofTop(profilePath, kind string) {
	if _, err := os.Stat(profilePath); err != nil {
		return
	}
	cmd := exec.Command("go", "tool", "pprof", "-top", profilePath)
	out, err := cmd.CombinedOutput()
	if err != nil {
		fmt.Printf("[bench] go tool pprof -top (%s) failed: %v\n", kind, err)
		return
	}
	fmt.Printf("\n[pprof-%s] go tool pprof -top %s\n", kind, profilePath)
	fmt.Println(string(out))
}

func findStudySample() (sym string, dayInt int, variants []string, featRoot string, ok bool) {
	// Re-using common.go/study.go logic implicitly via shared package
	// But we need to implement discovery here if not exported.
	// Since we are in main package, we can use discoverFeatureSymbols from study.go if exported?
	// No, discoverFeatureSymbols returns iter.Seq.
	// We'll just do a quick manual scan.

	featDir := filepath.Join(BaseDir, "features")
	entries, err := os.ReadDir(featDir)
	if err != nil {
		return "", 0, nil, "", false
	}

	for _, e := range entries {
		if !e.IsDir() || e.Name()[0] == '.' {
			continue
		}
		s := e.Name()

		featRoot = filepath.Join(featDir, s)
		vEntries, err := os.ReadDir(featRoot)
		if err != nil {
			continue
		}

		var vs []string
		for _, ve := range vEntries {
			if ve.IsDir() && ve.Name()[0] != '.' {
				vs = append(vs, ve.Name())
			}
		}
		if len(vs) == 0 {
			continue
		}

		// Find a day
		vDir := filepath.Join(featRoot, vs[0])
		files, _ := os.ReadDir(vDir)
		for _, f := range files {
			if len(f.Name()) > 4 && filepath.Ext(f.Name()) == ".bin" {
				// 20200101.bin
				base := f.Name()[:len(f.Name())-4]
				if d, err := parseDateInt(base); err == nil {
					return s, d, vs, featRoot, true
				}
			}
		}
	}
	return "", 0, nil, "", false
}

func parseDateInt(s string) (int, error) {
	var n int
	for _, c := range s {
		if c < '0' || c > '9' {
			return 0, fmt.Errorf("bad int")
		}
		n = n*10 + int(c-'0')
	}
	return n, nil
}

// featureBytesForDay sums the sizes of all variant feature files for this day.
func featureBytesForDay(featRoot string, variants []string, dayInt int) int {
	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100
	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	total := 0
	for _, v := range variants {
		path := filepath.Join(featRoot, v, dStr+".bin")
		fi, err := os.Stat(path)
		if err != nil {
			continue
		}
		if fi.Mode().IsRegular() {
			if sz := fi.Size(); sz > 0 {
				total += int(sz)
			}
		}
	}
	return total
}
```

// --- End File: bench.go ---

// --- File: build.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"math"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"time"
)

type ofiTask struct {
	Y, M, D        int
	Offset, Length int64
}

func runBuild() {
	start := time.Now()
	found := false
	for sym := range discoverSymbols() {
		found = true
		buildForSymbol(sym)
	}
	if !found {
		fmt.Printf("[build] no symbols discovered under %q\n", filepath.Join(BaseDir))
	}
	fmt.Printf("[build] Complete in %s\n", time.Since(start))
}

func discoverSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		entries, err := os.ReadDir(BaseDir)
		if err != nil {
			return
		}
		for _, e := range entries {
			if e.IsDir() && e.Name() != "features" && e.Name() != "common" && e.Name()[0] != '.' {
				if !yield(e.Name()) {
					return
				}
			}
		}
	}
}

func buildForSymbol(sym string) {
	fmt.Printf(">>> Building %s (Atoms v4: Modular)\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)
	outDir := filepath.Join(featRoot, "Atoms_v4")
	if err := os.MkdirAll(outDir, 0o755); err != nil {
		fmt.Printf("[build] MkdirAll: %v\n", err)
		return
	}

	tasksCh := make(chan ofiTask, 1024)
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			var binBuf []byte
			var gncBuf []byte
			for t := range tasksCh {
				processDynamicDay(sym, t, outDir, &binBuf, &gncBuf)
			}
		}()
	}

	for t := range discoverTasks(sym) {
		tasksCh <- t
	}
	close(tasksCh)
	wg.Wait()
}

func discoverTasks(sym string) iter.Seq[ofiTask] {
	return func(yield func(ofiTask) bool) {
		root := filepath.Join(BaseDir, sym)
		years, _ := os.ReadDir(root)
		for _, yDir := range years {
			if !yDir.IsDir() {
				continue
			}
			y, err := strconv.Atoi(yDir.Name())
			if err != nil {
				continue
			}
			yearPath := filepath.Join(root, yDir.Name())
			months, _ := os.ReadDir(yearPath)
			for _, mDir := range months {
				if !mDir.IsDir() {
					continue
				}
				m, err := strconv.Atoi(mDir.Name())
				if err != nil {
					continue
				}
				idxPath := filepath.Join(yearPath, mDir.Name(), "index.quantdev")
				f, err := os.Open(idxPath)
				if err != nil {
					continue
				}
				var hdr [16]byte
				if _, err := io.ReadFull(f, hdr[:]); err != nil {
					f.Close()
					continue
				}
				if string(hdr[0:4]) != IdxMagic {
					f.Close()
					continue
				}
				count := binary.LittleEndian.Uint64(hdr[8:])
				var row [26]byte
				for i := uint64(0); i < count; i++ {
					if _, err := io.ReadFull(f, row[:]); err != nil {
						break
					}
					d := int(binary.LittleEndian.Uint16(row[0:]))
					offset := int64(binary.LittleEndian.Uint64(row[2:]))
					length := int64(binary.LittleEndian.Uint64(row[10:]))
					if length > 0 {
						if !yield(ofiTask{Y: y, M: m, D: d, Offset: offset, Length: length}) {
							f.Close()
							return
						}
					}
				}
				f.Close()
			}
		}
	}
}

func processDynamicDay(sym string, t ofiTask, outDir string, binBuf, gncBuf *[]byte) {
	// 1. Get Atoms
	atoms := GetActiveAtoms()
	numAtoms := len(atoms)
	if numAtoms == 0 {
		return
	}

	gncBlob, ok := loadRawGNC(sym, t, gncBuf)
	if !ok {
		return
	}

	colsAny := DayColumnPool.Get()
	cols := colsAny.(*DayColumns)
	cols.Reset()
	defer DayColumnPool.Put(cols)

	rowCount, ok := inflateGNCToColumns(gncBlob, cols)
	if !ok || rowCount < 2 {
		return
	}

	times := cols.Times
	qtys := cols.Qtys
	sides := cols.Sides
	prices := cols.Prices

	// 2. Prepare Header
	// Magic(4) + Count(2) + [Len(1) + Str]...
	var headerBuf []byte
	headerBuf = append(headerBuf, AtomHeaderMagic...)
	var scratch [2]byte
	binary.LittleEndian.PutUint16(scratch[:], uint16(numAtoms))
	headerBuf = append(headerBuf, scratch[:]...)

	for _, a := range atoms {
		name := a.Name()
		if len(name) > 255 {
			name = name[:255]
		}
		headerBuf = append(headerBuf, uint8(len(name)))
		headerBuf = append(headerBuf, []byte(name)...)
		a.Reset()
	}

	// 3. Alloc Buffer
	rowBytes := numAtoms * 4
	reqSize := len(headerBuf) + (rowCount * rowBytes)
	if cap(*binBuf) < reqSize {
		*binBuf = make([]byte, reqSize)
	}
	*binBuf = (*binBuf)[:reqSize]

	// 4. Write Header
	copy((*binBuf)[0:], headerBuf)
	writePtr := len(headerBuf)

	prevTime := times[0]

	for i := 0; i < rowCount; i++ {
		q := qtys[i]
		s := float64(sides[i])
		p := prices[i]
		ts := times[i]
		flow := q * s

		dtSec := 0.0
		if i > 0 {
			dtMs := float64(ts - prevTime)
			if dtMs < 0 {
				dtMs = 0
			}
			dtSec = dtMs / 1000.0
		}
		if dtSec < 1e-4 {
			dtSec = 1e-4
		}

		// Dynamic Update Loop
		for _, atom := range atoms {
			val := atom.Update(q, s, p, flow, dtSec)

			// Clamp
			if val > 50 {
				val = 50
			}
			if val < -50 {
				val = -50
			}

			// Inline Write
			bits := math.Float32bits(float32(val))
			(*binBuf)[writePtr] = byte(bits)
			(*binBuf)[writePtr+1] = byte(bits >> 8)
			(*binBuf)[writePtr+2] = byte(bits >> 16)
			(*binBuf)[writePtr+3] = byte(bits >> 24)
			writePtr += 4
		}
		prevTime = ts
	}

	dateStr := fmt.Sprintf("%04d%02d%02d", t.Y, t.M, t.D)
	outPath := filepath.Join(outDir, dateStr+".bin")
	if err := os.WriteFile(outPath, *binBuf, 0o644); err != nil {
		fmt.Printf("[build] Write error: %v\n", err)
	}
}

func loadRawGNC(sym string, t ofiTask, buf *[]byte) ([]byte, bool) {
	path := filepath.Join(
		BaseDir, sym,
		fmt.Sprintf("%04d", t.Y), fmt.Sprintf("%02d", t.M),
		"data.quantdev",
	)
	f, err := os.Open(path)
	if err != nil {
		return nil, false
	}
	defer f.Close()
	if _, err := f.Seek(t.Offset, io.SeekStart); err != nil {
		return nil, false
	}
	need := int(t.Length)
	if cap(*buf) < need {
		*buf = make([]byte, need)
	}
	b := (*buf)[:need]
	if _, err := io.ReadFull(f, b); err != nil {
		return nil, false
	}
	if string(b[0:4]) != GNCMagic {
		return nil, false
	}
	return b, true
}
```

// --- End File: build.go ---

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
	"sync"
	"unique"
	"unsafe"
)

// --- Shared Configuration ---

const (
	CPUThreads = 24
	BaseDir    = "data"

	// Input Data Layout (GNC-v2)
	PxScale = 100_000_000.0
	QtScale = 100_000_000.0

	GNCChunkSize  = 65536
	GNCMagic      = "GNC2"
	GNCHeaderSize = 32
	IdxMagic      = "QIDX"
	IdxVersion    = 1

	// Output File Magic (V4 Self-Describing)
	AtomHeaderMagic = "ATM4"
)

var SymbolHandle = unique.Make("ETHUSDT")

func Symbol() string { return SymbolHandle.Value() }

// --- OPTIMIZED DATA SCHEMA (SoA) ---

type DayColumns struct {
	Count               int
	Times               []int64
	Prices              []float64
	Qtys                []float64
	Sides               []int8
	Matches             []uint16
	ScratchQtyDict      []float64
	ScratchChunkOffsets []uint32
}

func (c *DayColumns) Reset() {
	c.Count = 0
	c.Times = c.Times[:0]
	c.Prices = c.Prices[:0]
	c.Qtys = c.Qtys[:0]
	c.Sides = c.Sides[:0]
	c.Matches = c.Matches[:0]
	c.ScratchQtyDict = c.ScratchQtyDict[:0]
	c.ScratchChunkOffsets = c.ScratchChunkOffsets[:0]
}

var DayColumnPool = sync.Pool{
	New: func() any {
		return &DayColumns{
			Times:               make([]int64, 0, 65536),
			Prices:              make([]float64, 0, 65536),
			Qtys:                make([]float64, 0, 65536),
			Sides:               make([]int8, 0, 65536),
			Matches:             make([]uint16, 0, 65536),
			ScratchQtyDict:      make([]float64, 0, 1024),
			ScratchChunkOffsets: make([]uint32, 0, 128),
		}
	},
}

// --- Shared GNC Decoder ---

func inflateGNCToColumns(rawBlob []byte, cols *DayColumns) (int, bool) {
	if len(rawBlob) < GNCHeaderSize {
		return 0, false
	}
	if string(rawBlob[0:4]) != GNCMagic {
		return 0, false
	}

	totalRows := int(binary.LittleEndian.Uint32(rawBlob[4:8]))
	if totalRows <= 0 {
		cols.Reset()
		return 0, true
	}

	if cap(cols.Times) < totalRows {
		cols.Times = make([]int64, 0, totalRows)
	}
	if cap(cols.Prices) < totalRows {
		cols.Prices = make([]float64, 0, totalRows)
	}
	if cap(cols.Qtys) < totalRows {
		cols.Qtys = make([]float64, 0, totalRows)
	}
	if cap(cols.Sides) < totalRows {
		cols.Sides = make([]int8, 0, totalRows)
	}
	if cap(cols.Matches) < totalRows {
		cols.Matches = make([]uint16, 0, totalRows)
	}

	footerOffset := binary.LittleEndian.Uint64(rawBlob[24:32])
	if footerOffset >= uint64(len(rawBlob)) {
		return 0, false
	}

	dictBlob := rawBlob[footerOffset:]
	if len(dictBlob) < 4 {
		return 0, false
	}

	dictCount := binary.LittleEndian.Uint32(dictBlob[0:4])
	ptr := 4

	if uint64(ptr)+uint64(dictCount)*8+4 > uint64(len(dictBlob)) {
		return 0, false
	}

	if cap(cols.ScratchQtyDict) < int(dictCount) {
		cols.ScratchQtyDict = make([]float64, 0, int(dictCount))
	}
	qtyDict := cols.ScratchQtyDict[:dictCount]

	for i := 0; i < int(dictCount); i++ {
		qRaw := binary.LittleEndian.Uint64(dictBlob[ptr : ptr+8])
		qtyDict[i] = float64(qRaw) / QtScale
		ptr += 8
	}

	if len(dictBlob) < ptr+4 {
		return 0, false
	}
	chunkCount := binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
	ptr += 4

	if uint64(ptr)+uint64(chunkCount)*4 > uint64(len(dictBlob)) {
		return 0, false
	}

	if cap(cols.ScratchChunkOffsets) < int(chunkCount) {
		cols.ScratchChunkOffsets = make([]uint32, 0, int(chunkCount))
	}
	chunkOffsets := cols.ScratchChunkOffsets[:chunkCount]

	for i := 0; i < int(chunkCount); i++ {
		chunkOffsets[i] = binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
		ptr += 4
	}

	for _, off := range chunkOffsets {
		if uint64(off)+18 > uint64(len(rawBlob)) {
			return 0, false
		}
		chunk := rawBlob[off:]
		n := int(binary.LittleEndian.Uint16(chunk[0:2]))
		baseT := int64(binary.LittleEndian.Uint64(chunk[2:10]))
		baseP := int64(binary.LittleEndian.Uint64(chunk[10:18]))

		pTime := 18
		pPrice := pTime + n*4
		pQty := pPrice + n*8
		pMatches := pQty + n*2
		pSide := pMatches + n*2

		hasMatches := true
		if pSide > len(chunk) {
			pSideLegacy := pQty + n*2
			if pSideLegacy <= len(chunk) {
				hasMatches = false
				pSide = pSideLegacy
			} else {
				return 0, false
			}
		}

		tDeltas := unsafe.Slice((*int32)(unsafe.Pointer(&chunk[pTime])), n)
		pDeltas := unsafe.Slice((*int64)(unsafe.Pointer(&chunk[pPrice])), n)
		qIDs := unsafe.Slice((*uint16)(unsafe.Pointer(&chunk[pQty])), n)

		var ms []uint16
		if hasMatches {
			ms = unsafe.Slice((*uint16)(unsafe.Pointer(&chunk[pMatches])), n)
		}

		sideBits := chunk[pSide:]
		if len(sideBits) < (n+7)/8 {
			return 0, false
		}

		lastT := baseT
		lastP := baseP

		for i := 0; i < n; i++ {
			lastT += int64(tDeltas[i])
			lastP += pDeltas[i]

			cols.Times = append(cols.Times, lastT)
			cols.Prices = append(cols.Prices, float64(lastP)/PxScale)

			qID := int(qIDs[i])
			if qID < len(qtyDict) {
				cols.Qtys = append(cols.Qtys, qtyDict[qID])
			} else {
				cols.Qtys = append(cols.Qtys, 0)
			}

			if hasMatches {
				cols.Matches = append(cols.Matches, ms[i])
			} else {
				cols.Matches = append(cols.Matches, 1)
			}

			bitByte := sideBits[i/8]
			isBuy := (bitByte & (1 << (i % 8))) != 0
			side := int8(-1)
			if isBuy {
				side = 1
			}
			cols.Sides = append(cols.Sides, side)
		}
	}

	cols.Count = len(cols.Prices)
	return cols.Count, true
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bufio"
	"bytes"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"
	"unsafe"
)

const (
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

var (
	httpClient *http.Client
	stopEvent  atomic.Bool
	dirLocks   sync.Map
)

var errNotFound = fmt.Errorf("404")

// --- Ingestion Memory Pool ---

type IngestBuffers struct {
	Ts       []int64
	Ps       []int64
	Qs       []uint64
	Ms       []uint16
	Buys     []bool
	TDeltas  []int32
	PDeltas  []int64
	QIDs     []uint16
	SideBits []byte
}

var ingestBufferPool = sync.Pool{
	New: func() any {
		const cap = 1_000_000
		return &IngestBuffers{
			Ts:       make([]int64, 0, cap),
			Ps:       make([]int64, 0, cap),
			Qs:       make([]uint64, 0, cap),
			Ms:       make([]uint16, 0, cap),
			Buys:     make([]bool, 0, cap),
			TDeltas:  make([]int32, GNCChunkSize),
			PDeltas:  make([]int64, GNCChunkSize),
			QIDs:     make([]uint16, GNCChunkSize),
			SideBits: make([]byte, (GNCChunkSize+7)/8),
		}
	},
}

func (b *IngestBuffers) Reset() {
	b.Ts = b.Ts[:0]
	b.Ps = b.Ps[:0]
	b.Qs = b.Qs[:0]
	b.Ms = b.Ms[:0]
	b.Buys = b.Buys[:0]
}

func init() {
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   30 * time.Second,
	}
}

func runData() {
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt)
	go func() {
		<-sigChan
		stopEvent.Store(true)
		fmt.Println("\n[warn] Stopping gracefully...")
	}()

	fmt.Printf("--- GNC-v2 Ingestion (Streaming) | Symbol: %s ---\n", Symbol())

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt: %v\n", err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				if stopEvent.Load() {
					return
				}
				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	dirPath := filepath.Join(BaseDir, Symbol(), fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	sym := Symbol()
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, sym, sym, DataSet, y, m, day)

	// Download ZIP
	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	// Stream Parse
	bufs := ingestBufferPool.Get().(*IngestBuffers)
	bufs.Reset()
	defer ingestBufferPool.Put(bufs)

	gncBlob, count, err := streamZipToGNCBlob(zipBytes, bufs)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	sum := sha256.Sum256(gncBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	mu.Lock()
	defer mu.Unlock()

	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}

	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(gncBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	fData.Close()

	if err := updateIndex(idxPath, day, offset, len(gncBlob), cSum); err != nil {
		return "error_idx"
	}

	return "ok"
}

// --- Streaming ZIP -> GNC Pipeline ---

func streamZipToGNCBlob(zipData []byte, bufs *IngestBuffers) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}

		rc, err := f.Open()
		if err != nil {
			continue
		}

		// FIX: No defer inside loop. Explicit close after usage.
		count, err := scanCSVToBuffers(rc, bufs)
		rc.Close()

		if err != nil {
			return nil, 0, err
		}

		return encodeGNC(bufs, count)
	}
	return nil, 0, fmt.Errorf("no csv found")
}

func scanCSVToBuffers(r io.Reader, bufs *IngestBuffers) (int, error) {
	scanner := bufio.NewScanner(r)
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, 1024*1024)

	var price int64
	var ts int64
	var qty uint64
	var firstId, lastId int64
	var isBuyerMaker bool

	count := 0
	firstLine := true

	for scanner.Scan() {
		line := scanner.Bytes()
		if len(line) == 0 {
			continue
		}

		if firstLine {
			firstLine = false
			if line[0] < '0' || line[0] > '9' {
				continue
			}
		}

		col := 0
		start := 0

		price = 0
		qty = 0
		ts = 0
		firstId = 0
		lastId = 0
		isBuyerMaker = false

		for i := 0; i <= len(line); i++ {
			var c byte
			if i < len(line) {
				c = line[i]
			} else {
				c = ','
			}

			if c == ',' {
				field := line[start:i]
				switch col {
				case 1:
					price = fastParseFixed(field)
				case 2:
					qty = uint64(fastParseFixed(field))
				case 3:
					firstId = fastParseInt(field)
				case 4:
					lastId = fastParseInt(field)
				case 5:
					ts = fastParseInt(field)
				case 6:
					if len(field) > 0 && (field[0] == 't' || field[0] == 'T') {
						isBuyerMaker = true
					}
				}
				col++
				start = i + 1
			}
		}

		bufs.Ps = append(bufs.Ps, price)
		bufs.Qs = append(bufs.Qs, qty)
		bufs.Ts = append(bufs.Ts, ts)
		bufs.Buys = append(bufs.Buys, !isBuyerMaker)

		matches := int64(1)
		if lastId >= firstId {
			matches = lastId - firstId + 1
		}
		if matches > 65535 {
			matches = 65535
		}
		bufs.Ms = append(bufs.Ms, uint16(matches))

		count++
	}

	return count, scanner.Err()
}

// encodeGNC and encodeChunk ... (Standard GNC logic)
func encodeGNC(bufs *IngestBuffers, count int) ([]byte, uint64, error) {
	if count == 0 {
		return nil, 0, nil
	}

	var buf bytes.Buffer
	buf.Grow(count * 20)

	baseTime := bufs.Ts[0]
	basePrice := bufs.Ps[0]

	buf.WriteString(GNCMagic)

	var scratch [8]byte
	binary.LittleEndian.PutUint32(scratch[:4], uint32(count))
	buf.Write(scratch[:4])

	binary.LittleEndian.PutUint64(scratch[:], uint64(baseTime))
	buf.Write(scratch[:])

	binary.LittleEndian.PutUint64(scratch[:], uint64(basePrice))
	buf.Write(scratch[:])

	footerOffsetPos := buf.Len()
	binary.LittleEndian.PutUint64(scratch[:], 0)
	buf.Write(scratch[:])

	qtyDict := make(map[uint64]uint16, 1000)
	var dictLog []uint64
	chunkOffsets := make([]uint32, 0)

	for i := 0; i < count; i += GNCChunkSize {
		end := i + GNCChunkSize
		if end > count {
			end = count
		}
		chunkOffsets = append(chunkOffsets, uint32(buf.Len()))
		if err := encodeChunk(&buf, bufs, i, end, qtyDict, &dictLog); err != nil {
			return nil, 0, err
		}
	}

	footerStart := buf.Len()

	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(dictLog)))
	buf.Write(scratch[:4])
	for _, q := range dictLog {
		binary.LittleEndian.PutUint64(scratch[:], q)
		buf.Write(scratch[:])
	}

	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(chunkOffsets)))
	buf.Write(scratch[:4])
	for _, off := range chunkOffsets {
		binary.LittleEndian.PutUint32(scratch[:4], off)
		buf.Write(scratch[:4])
	}

	finalBytes := buf.Bytes()
	binary.LittleEndian.PutUint64(finalBytes[footerOffsetPos:], uint64(footerStart))

	return finalBytes, uint64(count), nil
}

func encodeChunk(w *bytes.Buffer, bufs *IngestBuffers, start, end int, dict map[uint64]uint16, log *[]uint64) error {
	count := end - start
	ts := bufs.Ts[start:end]
	ps := bufs.Ps[start:end]
	qs := bufs.Qs[start:end]
	ms := bufs.Ms[start:end]
	buys := bufs.Buys[start:end]

	tDeltas := bufs.TDeltas[:count]
	pDeltas := bufs.PDeltas[:count]
	qIDs := bufs.QIDs[:count]
	sideBits := bufs.SideBits[:(count+7)/8]

	for k := range sideBits {
		sideBits[k] = 0
	}

	chunkBaseT := ts[0]
	chunkBaseP := ps[0]
	var lastT, lastP int64 = chunkBaseT, chunkBaseP

	tDeltas[0] = 0
	pDeltas[0] = 0

	id, ok := dict[qs[0]]
	if !ok {
		if len(*log) >= 65535 {
			return fmt.Errorf("qty dict overflow")
		}
		id = uint16(len(*log))
		dict[qs[0]] = id
		*log = append(*log, qs[0])
	}
	qIDs[0] = id
	if buys[0] {
		sideBits[0] |= 1
	}

	for i := 1; i < count; i++ {
		dt := ts[i] - lastT
		dp := ps[i] - lastP

		if dt > 2147483647 || dt < -2147483648 {
			return fmt.Errorf("time delta overflow")
		}

		tDeltas[i] = int32(dt)
		lastT = ts[i]
		pDeltas[i] = dp
		lastP = ps[i]

		id, ok := dict[qs[i]]
		if !ok {
			if len(*log) >= 65535 {
				return fmt.Errorf("qty dict overflow")
			}
			id = uint16(len(*log))
			dict[qs[i]] = id
			*log = append(*log, qs[i])
		}
		qIDs[i] = id

		if buys[i] {
			sideBits[i/8] |= (1 << (i % 8))
		}
	}

	var head [18]byte
	binary.LittleEndian.PutUint16(head[0:], uint16(count))
	binary.LittleEndian.PutUint64(head[2:], uint64(chunkBaseT))
	binary.LittleEndian.PutUint64(head[10:], uint64(chunkBaseP))
	w.Write(head[:])

	w.Write(unsafeBytes(tDeltas))
	w.Write(unsafeBytes(pDeltas))
	w.Write(unsafeBytes(qIDs))
	w.Write(unsafeBytes(ms))
	w.Write(sideBits)

	return nil
}

func fastParseFixed(b []byte) int64 {
	var num int64
	var seenDot bool
	var dec int
	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		if c < '0' || c > '9' {
			continue
		}
		num = num*10 + int64(c-'0')
		if seenDot {
			dec++
		}
	}
	for dec < 8 {
		num *= 10
		dec++
	}
	return num
}

func fastParseInt(b []byte) int64 {
	var n int64
	for _, c := range b {
		if c < '0' || c > '9' {
			continue
		}
		n = n*10 + int64(c-'0')
	}
	return n
}

func unsafeBytes[T any](s []T) []byte {
	return unsafe.Slice((*byte)(unsafe.Pointer(unsafe.SliceData(s))), len(s)*int(unsafe.Sizeof(s[0])))
}

func download(url string) ([]byte, error) {
	var lastErr error
	for attempt := 0; attempt < 3; attempt++ {
		resp, err := httpClient.Get(url)
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		if resp.StatusCode == 404 {
			resp.Body.Close()
			return nil, errNotFound
		}
		if resp.StatusCode != 200 {
			resp.Body.Close()
			lastErr = fmt.Errorf("status %d", resp.StatusCode)
			time.Sleep(100 * time.Millisecond)
			continue
		}
		data, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		return data, nil
	}
	if lastErr == nil {
		lastErr = fmt.Errorf("unknown dl error")
	}
	return nil, lastErr
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()
	return checkIndex(f, day)
}

func checkIndex(f *os.File, day int) bool {
	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return false
	}
	if string(hdr[0:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			return false
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

func updateIndex(idxPath string, day int, offset int64, length int, csum uint64) error {
	f, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return err
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return err
	}
	if stat.Size() == 0 {
		var hdr [16]byte
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		if _, err := f.Write(hdr[:]); err != nil {
			return err
		}
	}

	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	var count uint64
	if err := binary.Read(f, binary.LittleEndian, &count); err != nil {
		return err
	}

	if _, err := f.Seek(0, io.SeekEnd); err != nil {
		return err
	}
	var row [26]byte
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(length))
	binary.LittleEndian.PutUint64(row[18:], csum)
	if _, err := f.Write(row[:]); err != nil {
		return err
	}

	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	return binary.Write(f, binary.LittleEndian, count+1)
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"runtime/debug"
	"time"
)

func main() {
	// Use all 7900X hardware threads.
	runtime.GOMAXPROCS(CPUThreads)

	// Hard memory limit: 24GB.
	const ramLimit = 24 * 1024 * 1024 * 1024
	debug.SetMemoryLimit(ramLimit)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()

	fmt.Printf("%s | Env: %s/%s | Threads: %d | RAM Limit: 24GB | GOGC: %s | GOAMD64: %s\n",
		runtime.Version(),
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
		os.Getenv("GOGC"),
		os.Getenv("GOAMD64"),
	)

	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData()
	case "build":
		runBuild()
	case "study":
		runStudy()
	case "sanity":
		runSanity()
	case "bench":
		runBench()
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("  data   - Download raw aggTrades")
	fmt.Println("  build  - Run TFI Primitives (RWVI, VAI, etc) -> features")
	fmt.Println("  study  - Run IS/OOS backtest on features")
	fmt.Println("  sanity - Check data integrity")
	fmt.Println("  bench  - Run decode benchmark + inline pprof summaries")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import (
	"math"
	"slices"
)

type MetricStats struct {
	Count        int
	ICPearson    float64
	IC_TStat     float64
	Sharpe       float64
	HitRate      float64
	BreakevenBps float64
	AutoCorr     float64
	AutoCorrAbs  float64
	AvgSegLen    float64
	MaxSegLen    float64
}

type Moments struct {
	Count          float64
	SumSig         float64
	SumRet         float64
	SumProd        float64
	SumSqSig       float64
	SumSqRet       float64
	SumPnL         float64
	SumSqPnL       float64
	Hits           float64
	ValidHits      float64
	SumAbsDeltaSig float64
	SumProdLag     float64
	SumAbsSig      float64
	SumAbsProdLag  float64
	SegCount       float64
	SegLenTotal    float64
	SegLenMax      float64
}

func (m *Moments) Add(m2 Moments) {
	m.Count += m2.Count
	m.SumSig += m2.SumSig
	m.SumRet += m2.SumRet
	m.SumProd += m2.SumProd
	m.SumSqSig += m2.SumSqSig
	m.SumSqRet += m2.SumSqRet
	m.SumPnL += m2.SumPnL
	m.SumSqPnL += m2.SumSqPnL
	m.Hits += m2.Hits
	m.ValidHits += m2.ValidHits
	m.SumAbsDeltaSig += m2.SumAbsDeltaSig
	m.SumProdLag += m2.SumProdLag
	m.SumAbsSig += m2.SumAbsSig
	m.SumAbsProdLag += m2.SumAbsProdLag
	m.SegCount += m2.SegCount
	m.SegLenTotal += m2.SegLenTotal
	if m2.SegLenMax > m.SegLenMax {
		m.SegLenMax = m2.SegLenMax
	}
}

// CalcMomentsVectors is split into two loops to enable AVX-512 vectorization
// for the pure math portion, while isolating the branching logic.
func CalcMomentsVectors(sigs, rets []float64) Moments {
	var m Moments
	n := len(sigs)
	if n == 0 {
		return m
	}

	// --- LOOP 1: Pure Math (Vectorization Candidate) ---
	// Local accumulators to encourage register usage
	var sumSig, sumRet, sumProd, sumSqSig, sumSqRet, sumPnL, sumSqPnL, sumAbsSig float64

	// Bounds check elimination
	_ = rets[n-1]
	_ = sigs[n-1]

	for i := 0; i < n; i++ {
		s := sigs[i]
		r := rets[i]

		sumSig += s
		sumRet += r
		sumProd += s * r
		sumSqSig += s * s
		sumSqRet += r * r

		pnl := s * r
		sumPnL += pnl
		sumSqPnL += pnl * pnl

		// Branchless Abs for vectorization
		// Go 1.25 compiler recognizes this pattern for float abs
		absS := s
		if s < 0 {
			absS = -s
		}
		sumAbsSig += absS
	}

	m.Count = float64(n)
	m.SumSig = sumSig
	m.SumRet = sumRet
	m.SumProd = sumProd
	m.SumSqSig = sumSqSig
	m.SumSqRet = sumSqRet
	m.SumPnL = sumPnL
	m.SumSqPnL = sumSqPnL
	m.SumAbsSig = sumAbsSig

	// --- LOOP 2: Logic & Dependencies (Scalar/Branchy) ---
	var prevSig float64
	var prevSign float64
	var curSegLen float64

	var hits, validHits, sumAbsDelta, sumProdLag, sumAbsProdLag float64
	var segCount, segLenTotal, segLenMax float64

	for i := 0; i < n; i++ {
		s := sigs[i]
		r := rets[i]

		// Hit Logic
		if s != 0 && r != 0 {
			validHits++
			if (s > 0 && r > 0) || (s < 0 && r < 0) {
				hits++
			}
		}

		// Lag Logic
		if i > 0 {
			d := s - prevSig
			if d < 0 {
				d = -d
			}
			sumAbsDelta += d
			sumProdLag += s * prevSig

			absPrev := prevSig
			if absPrev < 0 {
				absPrev = -absPrev
			}
			absS := s
			if absS < 0 {
				absS = -absS
			}
			sumAbsProdLag += absS * absPrev
		}

		// Segmentation Logic
		sign := 0.0
		if s > 0 {
			sign = 1.0
		} else if s < 0 {
			sign = -1.0
		}

		if sign != 0 {
			if prevSign == sign {
				curSegLen++
			} else {
				if curSegLen > 0 {
					segCount++
					segLenTotal += curSegLen
					if curSegLen > segLenMax {
						segLenMax = curSegLen
					}
				}
				curSegLen = 1
			}
		} else {
			if curSegLen > 0 {
				segCount++
				segLenTotal += curSegLen
				if curSegLen > segLenMax {
					segLenMax = curSegLen
				}
				curSegLen = 0
			}
		}
		prevSig = s
		prevSign = sign
	}

	// Final segment close
	if curSegLen > 0 {
		segCount++
		segLenTotal += curSegLen
		if curSegLen > segLenMax {
			segLenMax = curSegLen
		}
	}

	m.Hits = hits
	m.ValidHits = validHits
	m.SumAbsDeltaSig = sumAbsDelta
	m.SumProdLag = sumProdLag
	m.SumAbsProdLag = sumAbsProdLag
	m.SegCount = segCount
	m.SegLenTotal = segLenTotal
	m.SegLenMax = segLenMax

	return m
}

func FinalizeMetrics(m Moments, dailyICs []float64) MetricStats {
	if m.Count <= 1 {
		return MetricStats{Count: int(m.Count)}
	}
	ms := MetricStats{Count: int(m.Count)}

	num := m.Count*m.SumProd - m.SumSig*m.SumRet
	denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
	denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
	if denX > 0 && denY > 0 {
		ms.ICPearson = num / math.Sqrt(denX*denY)
	}

	meanPnL := m.SumPnL / m.Count
	varPnL := (m.SumSqPnL / m.Count) - meanPnL*meanPnL
	if varPnL > 1e-18 {
		ms.Sharpe = meanPnL / math.Sqrt(varPnL)
	}

	if m.ValidHits > 0 {
		ms.HitRate = m.Hits / m.ValidHits
	}
	if m.SumAbsDeltaSig > 1e-18 {
		ms.BreakevenBps = (m.SumPnL / m.SumAbsDeltaSig) * 10000.0
	}

	meanSig := m.SumSig / m.Count
	covLag := (m.SumProdLag / m.Count) - meanSig*meanSig
	varSig := (m.SumSqSig / m.Count) - meanSig*meanSig
	if varSig > 1e-18 {
		ms.AutoCorr = covLag / varSig
	}

	if m.Count > 0 {
		meanAbs := m.SumAbsSig / m.Count
		covAbs := (m.SumAbsProdLag / m.Count) - meanAbs*meanAbs
		varAbs := (m.SumSqSig / m.Count) - meanAbs*meanAbs
		if varAbs > 1e-18 {
			ms.AutoCorrAbs = covAbs / varAbs
		}
	}

	if m.SegCount > 0 {
		ms.AvgSegLen = m.SegLenTotal / m.SegCount
	}
	ms.MaxSegLen = m.SegLenMax

	if len(dailyICs) > 1 {
		var sum, sumSq float64
		n := float64(len(dailyICs))
		for _, v := range dailyICs {
			sum += v
			sumSq += v * v
		}
		mean := sum / n
		variance := (sumSq / n) - mean*mean
		if variance > 1e-18 {
			stdDev := math.Sqrt(variance)
			ms.IC_TStat = mean / (stdDev / math.Sqrt(n))
		}
	}
	return ms
}

type BucketResult struct {
	ID        int
	AvgSig    float64
	AvgRetBps float64
	Count     int
}

func ComputeQuantilesStrided(sigs, rets []float64, numBuckets, stride int) []BucketResult {
	n := len(sigs)
	if n == 0 || numBuckets <= 0 {
		return nil
	}

	estSize := n / stride
	type pair struct{ s, r float64 }
	pairs := make([]pair, 0, estSize)

	for i := 0; i < n; i += stride {
		pairs = append(pairs, pair{s: sigs[i], r: rets[i]})
	}

	if len(pairs) == 0 {
		return nil
	}

	slices.SortFunc(pairs, func(a, b pair) int {
		if a.s < b.s {
			return -1
		}
		if a.s > b.s {
			return 1
		}
		return 0
	})

	subN := len(pairs)
	results := make([]BucketResult, numBuckets)
	bucketSize := subN / numBuckets
	if bucketSize == 0 {
		bucketSize = 1
	}

	for b := 0; b < numBuckets; b++ {
		start := b * bucketSize
		end := start + bucketSize
		if b == numBuckets-1 || end > subN {
			end = subN
		}

		var sumS, sumR float64
		count := 0
		for i := start; i < end; i++ {
			sumS += pairs[i].s
			sumR += pairs[i].r
			count++
		}
		if count > 0 {
			results[b] = BucketResult{
				ID:        b + 1,
				AvgSig:    sumS / float64(count),
				AvgRetBps: (sumR / float64(count)) * 10000.0,
				Count:     count * stride,
			}
		}
	}
	return results
}

type BucketAgg struct {
	Count     int
	SumSig    float64
	SumRetBps float64
}

func (ba *BucketAgg) Add(br BucketResult) {
	if br.Count <= 0 {
		return
	}
	ba.Count += br.Count
	ba.SumSig += br.AvgSig * float64(br.Count)
	ba.SumRetBps += br.AvgRetBps * float64(br.Count)
}

func (ba BucketAgg) Finalize(id int) BucketResult {
	if ba.Count == 0 {
		return BucketResult{ID: id}
	}
	den := float64(ba.Count)
	return BucketResult{
		ID:        id,
		AvgSig:    ba.SumSig / den,
		AvgRetBps: ba.SumRetBps / den,
		Count:     ba.Count,
	}
}
```

// --- End File: metrics.go ---

// --- File: sanity.go ---

```go
package main

import (
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sort"
	"sync"
	"text/tabwriter"
	"time"
)

// Global Report Accumulator
type SanityReport struct {
	Mu           sync.Mutex
	TotalMonths  int
	TotalDays    int
	TotalTrades  int64
	TotalBytes   int64
	CorruptFiles int
	MissingDays  []string // List of "YYYY-MM-DD" gaps
	Errors       []string
}

var report SanityReport

func runSanity() {
	start := time.Now()
	root := filepath.Join(BaseDir, Symbol())
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("[sanity] ReadDir(%s): %v\n", root, err)
		return
	}

	var tasks []string
	// Discover all Month directories
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		yearPath := filepath.Join(root, y.Name())
		months, err := os.ReadDir(yearPath)
		if err != nil {
			fmt.Printf("[sanity] ReadDir(%s): %v\n", yearPath, err)
			continue
		}
		for _, m := range months {
			if m.IsDir() {
				tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
			}
		}
	}

	fmt.Printf("--- SANITY CHECK: %s | %d Months Found ---\n", Symbol(), len(tasks))

	// Reset Report
	report = SanityReport{MissingDays: make([]string, 0), Errors: make([]string, 0)}
	report.TotalMonths = len(tasks)

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	// Workers
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()

	printSummary(time.Since(start))
}

func validateMonth(dirPath string) {
	// Parse Year/Month from path for gap detection
	// Path ends in .../YYYY/MM
	_, mStr := filepath.Split(dirPath)
	yStr := filepath.Base(filepath.Dir(dirPath))
	year := fastAtoi(yStr)
	month := fastAtoi(mStr)

	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	// Local accumulators to minimize locking
	var lTrades int64
	var lBytes int64
	var lDays int
	var lCorrupt int
	lErrors := make([]string, 0)
	presentDays := make(map[int]bool)

	// 1. Check Files Exist
	fIdx, err := os.Open(idxPath)
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("MISSING IDX: %s", dirPath))
		report.Mu.Unlock()
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("MISSING DATA: %s", dirPath))
		report.Mu.Unlock()
		return
	}
	defer fData.Close()

	dstat, err := fData.Stat()
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("STAT FAIL: %s (%v)", dataPath, err))
		report.Mu.Unlock()
		return
	}

	// 2. Validate Index Header
	var hdr [16]byte
	if _, err := io.ReadFull(fIdx, hdr[:]); err != nil {
		lErrors = append(lErrors, fmt.Sprintf("BAD IDX HDR: %s", dirPath))
		mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, nil)
		return
	}
	if string(hdr[:4]) != IdxMagic {
		lErrors = append(lErrors, fmt.Sprintf("BAD MAGIC: %s", dirPath))
		mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, nil)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])

	// 3. Iterate Days
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row[:]); err != nil {
			lErrors = append(lErrors, fmt.Sprintf("IDX TRUNCATED: %s", dirPath))
			break
		}

		day := int(binary.LittleEndian.Uint16(row[0:]))
		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int64(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		presentDays[day] = true
		lDays++
		lBytes += length

		// Validate Data Blob
		if length < 32 {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Corrupt Blob (Len<32): %s Day %d", dirPath, day))
			continue
		}

		if offset < 0 || length < 0 || offset+length > dstat.Size() {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Blob exceeds file size: %s Day %d", dirPath, day))
			continue
		}

		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			lCorrupt++
			continue
		}

		// Read Header only first to check magic/count
		var blobHeader [32]byte
		if _, err := io.ReadFull(fData, blobHeader[:]); err != nil {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Read Fail: %s Day %d", dirPath, day))
			continue
		}

		if string(blobHeader[0:4]) != GNCMagic {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Bad GNC Magic: %s Day %d", dirPath, day))
			continue
		}

		tradeCount := binary.LittleEndian.Uint32(blobHeader[4:8])
		lTrades += int64(tradeCount)

		// Full Checksum (Expensive but necessary for 'Sanity')
		// Rewind to read full blob
		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			lCorrupt++
			continue
		}

		// Safety check on length alloc
		if length > 256*1024*1024 { // Cap at 256MB per day chunk for sanity
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Huge Blob (%d MB): %s Day %d", length/1024/1024, dirPath, day))
			continue
		}

		blob := make([]byte, int(length))
		if _, err := io.ReadFull(fData, blob); err != nil {
			lCorrupt++
			continue
		}

		sum := sha256.Sum256(blob)
		if binary.LittleEndian.Uint64(sum[:8]) != expSum {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Checksum Mismatch: %s Day %d", dirPath, day))
		}
	}

	// 4. Gap Detection
	// Calculate valid days for this specific month/year
	expectedDays := daysInMonth(year, month)
	var missing []string

	// Don't check gaps for the current (incomplete) month if it matches today's month
	now := time.Now()
	isCurrentMonth := (now.Year() == year && int(now.Month()) == month)

	limit := expectedDays
	if isCurrentMonth {
		limit = now.Day() - 1 // Expect up to yesterday
	}

	for d := 1; d <= limit; d++ {
		if !presentDays[d] {
			missing = append(missing, fmt.Sprintf("%04d-%02d-%02d", year, month, d))
		}
	}

	mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, missing)
}

func mergeReport(trades, bytes int64, days, corrupt int, errs []string, missing []string) {
	report.Mu.Lock()
	defer report.Mu.Unlock()

	report.TotalTrades += trades
	report.TotalBytes += bytes
	report.TotalDays += days
	report.CorruptFiles += corrupt
	if len(errs) > 0 {
		report.Errors = append(report.Errors, errs...)
	}
	if len(missing) > 0 {
		report.MissingDays = append(report.MissingDays, missing...)
	}
}

// --- Helpers ---

func daysInMonth(year, month int) int {
	// Days in month lookup
	if month == 2 {
		if isLeap(year) {
			return 29
		}
		return 28
	}
	if month == 4 || month == 6 || month == 9 || month == 11 {
		return 30
	}
	return 31
}

func isLeap(year int) bool {
	return year%4 == 0 && (year%100 != 0 || year%400 == 0)
}

func printSummary(duration time.Duration) {
	fmt.Println("\n=======================================================")
	fmt.Printf("   DATA INTEGRITY REPORT (%s)   \n", Symbol())
	fmt.Println("=======================================================")

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintf(w, "Scan Duration:\t%s\n", duration)
	fmt.Fprintf(w, "Months Scanned:\t%d\n", report.TotalMonths)
	fmt.Fprintf(w, "Total Days:\t%d\n", report.TotalDays)
	fmt.Fprintf(w, "Total Trades:\t%s\n", fmtHumanInt(report.TotalTrades))
	fmt.Fprintf(w, "Total Size:\t%s\n", fmtHumanBytes(report.TotalBytes))
	fmt.Fprintf(w, "Corrupt files:\t%d\n", report.CorruptFiles)

	gapCount := len(report.MissingDays)
	fmt.Fprintf(w, "Missing Days:\t%d\n", gapCount)

	w.Flush()
	fmt.Println("-------------------------------------------------------")

	if gapCount > 0 {
		fmt.Println("GAPS FOUND (First 10):")
		sort.Strings(report.MissingDays)
		for i, gap := range report.MissingDays {
			if i >= 10 {
				fmt.Printf("... and %d more\n", gapCount-10)
				break
			}
			fmt.Printf(" - [MISSING] %s\n", gap)
		}
		fmt.Println("-------------------------------------------------------")
	}

	if len(report.Errors) > 0 {
		fmt.Println("CRITICAL ERRORS (First 10):")
		for i, err := range report.Errors {
			if i >= 10 {
				fmt.Printf("... and %d more\n", len(report.Errors)-10)
				break
			}
			fmt.Printf(" - [FAIL] %s\n", err)
		}
	} else if report.CorruptFiles == 0 && gapCount == 0 {
		fmt.Println(">> STATUS: GREEN (100% Integrity) <<")
	} else {
		fmt.Println(">> STATUS: AMBER (Gaps or Corruption Detected) <<")
	}
	fmt.Println("=======================================================")
}

func fmtHumanInt(n int64) string {
	s := fmt.Sprintf("%d", n)
	if n < 1000 {
		return s
	}
	// Simple comma insertion
	var res []byte
	for i, c := range s {
		if i > 0 && (len(s)-i)%3 == 0 {
			res = append(res, ',')
		}
		res = append(res, byte(c))
	}
	return string(res)
}

func fmtHumanBytes(b int64) string {
	const unit = 1024
	if b < unit {
		return fmt.Sprintf("%d B", b)
	}
	div, exp := int64(unit), 0
	for n := b / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.2f %cB", float64(b)/float64(div), "KMGTPE"[exp])
}
```

// --- End File: sanity.go ---

// --- File: study.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"math"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"text/tabwriter"
	"time"
	"unsafe"
)

const (
	OOSDateStr = "2024-01-01"
)

var TimeHorizonsMS = []int{
	500,   // 0.5s - Sniper
	1000,  // 1s
	2000,  // 2s
	5000,  // 5s
	10000, // 10s
}

var oosBoundaryYMD int

func init() {
	oosBoundaryYMD = parseOOSBoundary(OOSDateStr)
}

type DayResult struct {
	YMD     int
	Metrics map[string][]Moments
}

func runStudy() {
	startT := time.Now()
	found := false
	for sym := range discoverFeatureSymbols() {
		found = true
		studySymbol(sym)
	}
	if !found {
		fmt.Printf("[study] No features found in %s/features\n", BaseDir)
	} else {
		fmt.Printf("[study] ALL COMPLETE in %s\n", time.Since(startT))
	}
}

func discoverFeatureSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		featDir := filepath.Join(BaseDir, "features")
		entries, err := os.ReadDir(featDir)
		if err != nil {
			return
		}
		for _, e := range entries {
			if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
				if !yield(e.Name()) {
					return
				}
			}
		}
	}
}

func studySymbol(sym string) {
	fmt.Printf("\n>>> STUDY: %s <<<\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)
	entries, _ := os.ReadDir(featRoot)
	var variants []string
	for _, e := range entries {
		if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
			variants = append(variants, e.Name())
		}
	}
	if len(variants) == 0 {
		return
	}

	// Discovery
	var tasks []int
	for d := range discoverStudyDays(filepath.Join(featRoot, variants[0])) {
		tasks = append(tasks, d)
	}
	totalTasks := len(tasks)
	fmt.Printf("Variants: %d | Days: %d\n", len(variants), totalTasks)

	isAcc := make(map[string][]Moments)
	oosAcc := make(map[string][]Moments)
	var accMu sync.Mutex

	resultsChan := make(chan DayResult, 64)
	jobsChan := make(chan int, len(tasks))
	var wg sync.WaitGroup
	var completed atomic.Int64
	doneChan := make(chan bool)

	go func() {
		ticker := time.NewTicker(1000 * time.Millisecond)
		defer ticker.Stop()
		for {
			select {
			case <-doneChan:
				return
			case <-ticker.C:
				printProgress(int(completed.Load()), totalTasks)
			}
		}
	}()

	for i := 0; i < 24; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			var sigBuf []float64
			var fileBuf []byte
			var retBuf []float64
			retsPerHBuf := make([][]float64, len(TimeHorizonsMS))
			var gncBuf []byte
			for idx := range jobsChan {
				dayInt := tasks[idx]
				res := processStudyDay(sym, dayInt, variants, featRoot, &sigBuf, &fileBuf, &retBuf, &retsPerHBuf, &gncBuf)
				resultsChan <- res
				completed.Add(1)
			}
		}()
	}

	for i := range tasks {
		jobsChan <- i
	}
	close(jobsChan)

	go func() {
		wg.Wait()
		close(resultsChan)
		close(doneChan)
	}()

	for res := range resultsChan {
		isOOS := res.YMD >= oosBoundaryYMD
		accMu.Lock()
		for vName, moms := range res.Metrics {
			if _, ok := isAcc[vName]; !ok {
				isAcc[vName] = make([]Moments, len(TimeHorizonsMS))
				oosAcc[vName] = make([]Moments, len(TimeHorizonsMS))
			}
			target := isAcc[vName]
			if isOOS {
				target = oosAcc[vName]
			}
			for hIdx := range TimeHorizonsMS {
				target[hIdx].Add(moms[hIdx])
			}
		}
		accMu.Unlock()
	}

	var finalKeys []string
	for k := range isAcc {
		finalKeys = append(finalKeys, k)
	}
	sort.Strings(finalKeys)

	for hIdx, ms := range TimeHorizonsMS {
		printHorizonTable(ms, finalKeys, isAcc, oosAcc, hIdx)
	}
}

func processStudyDay(
	sym string, dayInt int, variants []string, featRoot string,
	sigBuf *[]float64, fileBuf *[]byte, retBuf *[]float64,
	retsPerH *[][]float64, gncBuf *[]byte,
) DayResult {
	y, m, d := dayInt/10000, (dayInt%10000)/100, dayInt%100
	res := DayResult{
		YMD:     dayInt,
		Metrics: make(map[string][]Moments),
	}

	colsAny := DayColumnPool.Get()
	cols := colsAny.(*DayColumns)
	cols.Reset()
	defer DayColumnPool.Put(cols)

	rowCount, ok := loadDayColumns(sym, y, m, d, cols, gncBuf)
	if !ok || rowCount == 0 {
		return res
	}
	n := rowCount

	p, tm := cols.Prices, cols.Times
	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	for hIdx, ms := range TimeHorizonsMS {
		computeReturns(p, tm, n, ms, retBuf)
		target := (*retsPerH)[hIdx]
		if cap(target) < n {
			target = make([]float64, n+n/4)
			(*retsPerH)[hIdx] = target
		}
		target = target[:n]
		copy(target, (*retBuf)[:n])
	}

	for _, v := range variants {
		sigPath := filepath.Join(featRoot, v, dStr+".bin")
		rawSigs, byteSize, ok := fastLoadBytes(sigPath, fileBuf)
		if !ok || byteSize == 0 {
			continue
		}

		// --- HEADER PARSING ---
		if len(rawSigs) < 6 || string(rawSigs[0:4]) != AtomHeaderMagic {
			continue // wrong magic
		}

		ptr := 4
		numFeats := int(binary.LittleEndian.Uint16(rawSigs[ptr:]))
		ptr += 2

		featureNames := make([]string, numFeats)
		for i := 0; i < numFeats; i++ {
			if ptr >= len(rawSigs) {
				break
			}
			nameLen := int(rawSigs[ptr])
			ptr++
			if ptr+nameLen > len(rawSigs) {
				break
			}
			featureNames[i] = string(rawSigs[ptr : ptr+nameLen])
			ptr += nameLen
		}

		// Data starts at ptr
		dataBlob := rawSigs[ptr:]
		dims := numFeats

		if n > cap(*sigBuf) {
			*sigBuf = make([]float64, n+n/4)
		}

		for dim := 0; dim < dims; dim++ {
			target := (*sigBuf)[:n]
			decodeFeatureDim(dataBlob, n, dims, dim, target)

			key := v
			if dim < len(featureNames) {
				key = featureNames[dim]
			}
			moms := make([]Moments, len(TimeHorizonsMS))
			for hIdx := range TimeHorizonsMS {
				moms[hIdx] = CalcMomentsVectors(target, (*retsPerH)[hIdx][:n])
			}
			res.Metrics[key] = moms
		}
	}
	return res
}

func printHorizonTable(hMS int, keys []string, isAcc, oosAcc map[string][]Moments, hIdx int) {
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	sec := float64(hMS) / 1000.0
	fmt.Fprintf(w, "\n== Horizon %.3fs ==\n", sec)
	fmt.Fprintln(w, "FEATURE\tIS_IC\tOOS_IC\tIS_BPS\tOOS_BPS")
	for _, k := range keys {
		mIS := isAcc[k][hIdx]
		mOOS := oosAcc[k][hIdx]

		icIS, icOOS := 0.0, 0.0
		if mIS.Count > 0 {
			num := mIS.Count*mIS.SumProd - mIS.SumSig*mIS.SumRet
			den := math.Sqrt((mIS.Count*mIS.SumSqSig - mIS.SumSig*mIS.SumSig) * (mIS.Count*mIS.SumSqRet - mIS.SumRet*mIS.SumRet))
			if den > 0 {
				icIS = num / den
			}
		}
		if mOOS.Count > 0 {
			num := mOOS.Count*mOOS.SumProd - mOOS.SumSig*mOOS.SumRet
			den := math.Sqrt((mOOS.Count*mOOS.SumSqSig - mOOS.SumSig*mOOS.SumSig) * (mOOS.Count*mOOS.SumSqRet - mOOS.SumRet*mOOS.SumRet))
			if den > 0 {
				icOOS = num / den
			}
		}

		bpsIS := 0.0
		if mIS.SumAbsDeltaSig > 0 {
			bpsIS = (mIS.SumPnL / mIS.SumAbsDeltaSig) * 10000.0
		}
		bpsOOS := 0.0
		if mOOS.SumAbsDeltaSig > 0 {
			bpsOOS = (mOOS.SumPnL / mOOS.SumAbsDeltaSig) * 10000.0
		}

		fmt.Fprintf(w, "%s\t%.4f\t%.4f\t%.2f\t%.2f\n", k, icIS, icOOS, bpsIS, bpsOOS)
	}
	w.Flush()
}

// --- HELPERS ---

func discoverStudyDays(vDir string) iter.Seq[int] {
	return func(yield func(int) bool) {
		files, err := os.ReadDir(vDir)
		if err != nil {
			return
		}
		var days []int
		for _, f := range files {
			if strings.HasSuffix(f.Name(), ".bin") {
				if val := fastAtoi(strings.TrimSuffix(f.Name(), ".bin")); val > 0 {
					days = append(days, val)
				}
			}
		}
		sort.Ints(days)
		for _, d := range days {
			if !yield(d) {
				return
			}
		}
	}
}

func loadDayColumns(sym string, y, m, d int, cols *DayColumns, gncBuf *[]byte) (int, bool) {
	dir := filepath.Join(BaseDir, sym, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")
	offset, length := findBlobOffset(idxPath, d)
	if length == 0 {
		return 0, false
	}
	f, err := os.Open(dataPath)
	if err != nil {
		return 0, false
	}
	defer f.Close()
	stat, err := f.Stat()
	if err != nil {
		return 0, false
	}
	off64 := int64(offset)
	len64 := int64(length)
	if off64 < 0 || len64 <= 0 || off64+len64 > stat.Size() {
		return 0, false
	}
	if len64 > 512*1024*1024 {
		return 0, false
	}
	need := int(len64)
	if cap(*gncBuf) < need {
		*gncBuf = make([]byte, need)
	}
	raw := (*gncBuf)[:need]
	if _, err := f.Seek(off64, io.SeekStart); err != nil {
		return 0, false
	}
	if _, err := io.ReadFull(f, raw); err != nil {
		return 0, false
	}
	return inflateGNCToColumns(raw, cols)
}

func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()
	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return 0, 0
	}
	if string(hdr[0:4]) != IdxMagic {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}

func fastLoadBytes(path string, fileBuf *[]byte) ([]byte, int, bool) {
	f, err := os.Open(path)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()
	fi, err := f.Stat()
	if err != nil {
		return nil, 0, false
	}
	size := int(fi.Size())
	if size == 0 {
		return nil, 0, false
	}
	if cap(*fileBuf) < size {
		*fileBuf = make([]byte, size)
	}
	buf := (*fileBuf)[:size]
	if _, err := io.ReadFull(f, buf); err != nil {
		return nil, 0, false
	}
	return buf, size, true
}

func decodeFeatureDim(raw []byte, n, dims, dim int, out []float64) {
	// Re-using same logic, simplified for self-contained file
	minBytes := n * dims * 4
	if len(raw) < minBytes || dim < 0 || dim >= dims {
		for i := 0; i < n; i++ {
			out[i] = 0
		}
		return
	}
	f32s := unsafe.Slice((*float32)(unsafe.Pointer(&raw[0])), len(raw)/4)
	for i := 0; i < n; i++ {
		out[i] = float64(f32s[i*dims+dim])
	}
}

func computeReturns(p []float64, tm []int64, n int, horizonMS int, outBuf *[]float64) {
	if n > cap(*outBuf) {
		*outBuf = make([]float64, n+n/4)
	}
	outSlice := (*outBuf)[:n]
	hVal := int64(horizonMS)
	right := 0
	for left := 0; left < n; left++ {
		targetTime := tm[left] + hVal
		if right < left {
			right = left
		}
		for right < n && tm[right] < targetTime {
			right++
		}
		if right >= n {
			for k := left; k < n; k++ {
				outSlice[k] = 0
			}
			return
		}
		pStart := p[left]
		pEnd := p[right]
		if pStart > 0 {
			outSlice[left] = (pEnd - pStart) / pStart
		} else {
			outSlice[left] = 0
		}
	}
}

func parseOOSBoundary(d string) int {
	return fastAtoi(d[0:4])*10000 + fastAtoi(d[5:7])*100 + fastAtoi(d[8:10])
}

func fastAtoi(s string) int {
	n := 0
	for i := 0; i < len(s); i++ {
		c := s[i]
		if c >= '0' && c <= '9' {
			n = n*10 + int(c-'0')
		}
	}
	return n
}

func printProgress(curr, total int) {
	if total == 0 {
		return
	}
	percent := float64(curr) / float64(total)
	fmt.Printf("\rProgress: %.1f%% (%d/%d)", percent*100, curr, total)
}
```

// --- End File: study.go ---

