--- File Tree Structure ---
|-- data/
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- ofibuild.go
|-- ofistudy.go
|-- sanity.go
    |-- BTCUSDT/
    |-- features/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..11]/ (11 month dirs)
        |-- BTCUSDT/
            |-- B_Hawkes_Adaptive/

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
)

// --- Shared Configuration ---
const (
	CPUThreads = 24
	Symbol     = "BTCUSDT"
	BaseDir    = "data"

	// Binary Layout
	PxScale    = 100_000_000.0
	QtScale    = 100_000_000.0
	HeaderSize = 48
	RowSize    = 48

	// Magic Headers
	AggMagic   = "AGG3"
	IdxMagic   = "QIDX"
	IdxVersion = 1 // Index file format version
)

// --- Zero-Alloc Trade Parsing ---

type AggRow struct {
	TsMs       int64
	PriceFixed uint64
	QtyFixed   uint64
	Flags      uint16
}

// ParseAggRow interprets a 48-byte row from data.quantdev without allocation.
func ParseAggRow(row []byte) AggRow {
	return AggRow{
		TsMs:       int64(binary.LittleEndian.Uint64(row[38:])),
		PriceFixed: binary.LittleEndian.Uint64(row[8:]),
		QtyFixed:   binary.LittleEndian.Uint64(row[16:]),
		Flags:      binary.LittleEndian.Uint16(row[36:]),
	}
}

func TradePrice(row AggRow) float64 {
	return float64(row.PriceFixed) / PxScale
}

func TradeQty(row AggRow) float64 {
	return float64(row.QtyFixed) / QtScale
}

func TradeDollar(row AggRow) float64 {
	return TradePrice(row) * TradeQty(row)
}

// TradeSign returns +1 for taker buy, -1 for taker sell.
func TradeSign(row AggRow) float64 {
	if row.Flags&1 != 0 {
		// buyer is maker -> taker is seller
		return -1.0
	}
	return 1.0
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"syscall"
	"time"
)

// --- Local Constants (Specific to Data Downloader) ---
const (
	// Data Source
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

// --- Globals ---
var (
	httpClient *http.Client
	stopEvent  bool
	stopMu     sync.Mutex

	// Directory Locks to fix race conditions on monthly files
	dirLocks sync.Map
)

func init() {
	// High-throughput Transport (no TLS hacks)
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   20 * time.Second,
	}
}

// runData is called by main()
func runData() {
	// Graceful Shutdown
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
	go func() {
		<-sigChan
		stopMu.Lock()
		stopEvent = true
		stopMu.Unlock()
		fmt.Println("\n[warn] Stopping gracefully (finish current jobs)...")
	}()

	fmt.Printf("--- data.go (Ryzen 7900X Optimized) | Symbol: %s ---\n", Symbol)

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt %q: %v\n", FallbackDt, err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	if end.Before(start) {
		fmt.Println("Nothing to do.")
		return
	}

	// Generate Job Queue
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	// Workers
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				// Check Stop Signal
				stopMu.Lock()
				if stopEvent {
					stopMu.Unlock()
					return
				}
				stopMu.Unlock()

				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	// Stats
	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	// Paths
	dirPath := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	// 1. Get Directory Lock (serialize Index/Data per month)
	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	// 2. Check Index (Fast Read)
	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	// 3. Download (Concurrent / Slow IO)
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, Symbol, Symbol, DataSet, y, m, day)

	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	// 4. Fast Parse (Concurrent / High CPU)
	aggBlob, count, err := fastZipToAgg3(d, zipBytes)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	// 5. Compress (Concurrent / High CPU)
	var b bytes.Buffer
	const zLevel = zlib.BestSpeed
	w, err := zlib.NewWriterLevel(&b, zLevel)
	if err != nil {
		return "error_zlib"
	}
	if _, err := w.Write(aggBlob); err != nil {
		w.Close()
		return "error_zlib_write"
	}
	if err := w.Close(); err != nil {
		return "error_zlib_close"
	}
	compBlob := b.Bytes()

	// 6. Checksum (Concurrent)
	sum := sha256.Sum256(aggBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	// 7. Write Data & Update Index (Serialized / Fast IO)
	mu.Lock()
	defer mu.Unlock()

	// Double check index in case another thread finished this day while we were processing
	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	// Append Data
	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}
	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(compBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	if err := fData.Close(); err != nil {
		return "error_close_data"
	}

	// Append / Init Index
	fIdx, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return "error_idx"
	}
	defer fIdx.Close()

	idxStat, err := fIdx.Stat()
	if err != nil {
		return "error_idx_stat"
	}
	if idxStat.Size() == 0 {
		// Init Index Header
		hdr := make([]byte, 16)
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		binary.LittleEndian.PutUint64(hdr[8:], 0) // Count = 0
		if _, err := fIdx.Write(hdr); err != nil {
			return "error_idx_hdr"
		}
	}

	// Index Row: Day(2), Offset(8), Length(8), Checksum(8) = 26 bytes
	row := make([]byte, 26)
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(len(compBlob)))
	binary.LittleEndian.PutUint64(row[18:], cSum)

	if _, err := fIdx.Seek(0, io.SeekEnd); err != nil {
		return "error_idx_seek"
	}
	if _, err := fIdx.Write(row); err != nil {
		return "error_idx_write"
	}

	// Increment Index Count (Atomic update under lock)
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	var currentCount uint64
	if err := binary.Read(fIdx, binary.LittleEndian, &currentCount); err != nil {
		return "error_idx_read"
	}
	if _, err := fIdx.Seek(8, io.SeekStart); err != nil {
		return "error_idx_seek"
	}
	if err := binary.Write(fIdx, binary.LittleEndian, currentCount+1); err != nil {
		return "error_idx_write"
	}

	return "ok"
}

// --- Helpers ---

var errNotFound = fmt.Errorf("404")

func download(url string) ([]byte, error) {
	// Retry logic optimized for throughput
	for i := 0; i < 3; i++ {
		resp, err := httpClient.Get(url)
		if err == nil {
			if resp != nil {
				if resp.StatusCode == http.StatusOK {
					data, readErr := io.ReadAll(resp.Body)
					resp.Body.Close()
					return data, readErr
				}
				if resp.StatusCode == http.StatusNotFound {
					resp.Body.Close()
					return nil, errNotFound
				}
				resp.Body.Close()
			}
		}
		time.Sleep(time.Duration(i+1) * 100 * time.Millisecond)
	}
	return nil, fmt.Errorf("timeout")
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()

	// Read Header
	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return false
	}
	if string(hdr[:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])

	// Scan Rows (max 31)
	row := make([]byte, 26)
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

// fastZipToAgg3: Zero-alloc column scanning + Binary Packing
func fastZipToAgg3(t time.Time, zipData []byte) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	const zLevel = zlib.BestSpeed // informational only, used in header

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}
		rc, err := f.Open()
		if err != nil {
			continue
		}

		// 32GB RAM allows reading full CSV.
		data, err := io.ReadAll(rc)
		rc.Close()
		if err != nil {
			return nil, 0, err
		}

		// Estimation: ~70 bytes CSV -> 48 bytes Bin
		estRows := len(data) / 50
		if estRows < 1 {
			estRows = 1
		}
		blob := make([]byte, 0, estRows*RowSize)
		rowBuf := make([]byte, RowSize)

		var (
			minTs int64 = math.MaxInt64
			maxTs int64 = math.MinInt64
			count uint64

			// Column Tracking
			// 0:id, 1:px, 2:qty, 3:fid, 4:lid, 5:ts, 6:m
			colIdx int
			start  int
			i      int
			n      = len(data)
		)

		// Skip Header Line
		for i < n {
			if data[i] == '\n' {
				i++
				start = i
				break
			}
			i++
		}

		// State Machine Loop
		for i < n {
			b := data[i]

			switch b {
			case ',':
				colSlice := data[start:i]

				switch colIdx {
				case 0: // AggID
					binary.LittleEndian.PutUint64(rowBuf[0:], fastParseUint(colSlice))
				case 1: // Price
					binary.LittleEndian.PutUint64(rowBuf[8:], fastParseFloatFixed(colSlice))
				case 2: // Qty
					binary.LittleEndian.PutUint64(rowBuf[16:], fastParseFloatFixed(colSlice))
				case 3: // FirstID
					binary.LittleEndian.PutUint64(rowBuf[24:], fastParseUint(colSlice))
				case 4: // LastID -> Count
					fid := binary.LittleEndian.Uint64(rowBuf[24:])
					lid := fastParseUint(colSlice)
					binary.LittleEndian.PutUint32(rowBuf[32:], uint32(lid-fid+1))
				case 5: // Time
					ts := fastParseUint(colSlice)
					binary.LittleEndian.PutUint64(rowBuf[38:], ts)
					if int64(ts) < minTs {
						minTs = int64(ts)
					}
					if int64(ts) > maxTs {
						maxTs = int64(ts)
					}
				}

				colIdx++
				start = i + 1

			case '\n':
				// End of row (Maker Flag)
				colSlice := data[start:i]

				// Case 6: is_buyer_maker
				flags := uint16(0)
				if len(colSlice) > 0 {
					c := colSlice[0]
					if c == 't' || c == 'T' {
						flags = 1
					}
				}
				binary.LittleEndian.PutUint16(rowBuf[36:], flags)

				// Append Row
				blob = append(blob, rowBuf...)
				count++

				colIdx = 0
				start = i + 1
			}
			i++
		}

		// Handle case where file doesn't end with \n
		if colIdx == 6 && start < n {
			colSlice := data[start:n]
			flags := uint16(0)
			if len(colSlice) > 0 && (colSlice[0] == 't' || colSlice[0] == 'T') {
				flags = 1
			}
			binary.LittleEndian.PutUint16(rowBuf[36:], flags)
			blob = append(blob, rowBuf...)
			count++
		}

		if count == 0 {
			return nil, 0, nil
		}

		// Header construction (Matches AggHeader: 48 bytes)
		hdr := make([]byte, HeaderSize)
		copy(hdr[0:], AggMagic)
		hdr[4] = 1
		hdr[5] = uint8(t.Day())
		binary.LittleEndian.PutUint16(hdr[6:], uint16(zLevel)) // zlib level (informational)
		binary.LittleEndian.PutUint64(hdr[8:], count)
		binary.LittleEndian.PutUint64(hdr[16:], uint64(minTs))
		binary.LittleEndian.PutUint64(hdr[24:], uint64(maxTs))
		// Bytes 32-47 are padding (zero initialized by make)

		return append(hdr, blob...), count, nil
	}
	return nil, 0, fmt.Errorf("no csv")
}

// --- High Performance Parsers (No Alloc) ---

func fastParseUint(b []byte) uint64 {
	var n uint64
	for _, c := range b {
		n = n*10 + uint64(c-'0')
	}
	return n
}

// Converts "123.45" -> 12345000000 (scaled 1e8)
func fastParseFloatFixed(b []byte) uint64 {
	var n uint64
	seenDot := false
	decimals := 0

	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		n = n*10 + uint64(c-'0')
		if seenDot {
			decimals++
		}
	}

	// Adjust Scale 1e8
	const target = 8
	if decimals < target {
		for i := 0; i < (target - decimals); i++ {
			n *= 10
		}
	} else if decimals > target {
		for i := 0; i < (decimals - target); i++ {
			n /= 10
		}
	}
	return n
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"time"
)

func main() {
	// Hardware Optimization: Ryzen 9 7900X
	runtime.GOMAXPROCS(CPUThreads)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()
	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData() // Download
	case "build":
		runBuild() // Run Models -> .bin
	case "study":
		runStudy() // IS/OOS Stats
	case "sanity":
		runSanity() // Integrity Check
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("  data   - Download raw aggTrades")
	fmt.Println("  build  - Run Hawkes/Adaptive/EMA models -> features")
	fmt.Println("  study  - Run IS/OOS backtest on features")
	fmt.Println("  sanity - Check data integrity")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import "math"

// MetricStats holds the final calculated metrics for human consumption.
type MetricStats struct {
	Count        int
	ICPearson    float64 // Linear IC
	Sharpe       float64 // Per-trade Sharpe
	SharpeAnnual float64 // t-statistic (Sharpe * sqrt(N))
	HitRate      float64 // Directional accuracy
	BreakevenBps float64 // Cost per unit turnover to zero PnL
}

// Moments holds raw sums for global aggregation (IC, Sharpe, Turnover).
// Optimized for memory alignment on amd64.
type Moments struct {
	Count     float64
	SumSig    float64
	SumRet    float64
	SumProd   float64 // Sum(Sig * Ret)
	SumSqSig  float64
	SumSqRet  float64
	SumPnL    float64
	SumSqPnL  float64
	Hits      float64
	ValidHits float64
	Turnover  float64
}

// Add accumulates m2 into m (In-place).
func (m *Moments) Add(m2 Moments) {
	m.Count += m2.Count
	m.SumSig += m2.SumSig
	m.SumRet += m2.SumRet
	m.SumProd += m2.SumProd
	m.SumSqSig += m2.SumSqSig
	m.SumSqRet += m2.SumSqRet
	m.SumPnL += m2.SumPnL
	m.SumSqPnL += m2.SumSqPnL
	m.Hits += m2.Hits
	m.ValidHits += m2.ValidHits
	m.Turnover += m2.Turnover
}

// CalcMoments computes raw moments for a single chunk/day.
// Optimized for AVX2 pipeline (simple float operations, branch-light).
func CalcMoments(sig, ret []float64) Moments {
	n := len(sig)
	if n < 2 {
		return Moments{}
	}

	var m Moments
	m.Count = float64(n)

	// Local registers for hot loop
	var (
		sSum, rSum, sSq, rSq, prodSum float64
		pnlSum, pnlSq                 float64
		hits, valid                   float64
		turnover, prevSig             float64
	)

	prevSig = sig[0] // approximation

	for i := 0; i < n; i++ {
		s := sig[i]
		r := ret[i]

		// Pearson components
		sSum += s
		rSum += r
		sSq += s * s
		rSq += r * r
		prodSum += s * r

		// PnL components
		pnl := s * r
		pnlSum += pnl
		pnlSq += pnl * pnl

		// Hit Rate
		if s != 0 && r != 0 {
			valid++
			if (s > 0 && r > 0) || (s < 0 && r < 0) {
				hits++
			}
		}

		// Turnover
		if i > 0 {
			d := s - prevSig
			if d < 0 {
				d = -d
			}
			turnover += d
		}
		prevSig = s
	}

	m.SumSig = sSum
	m.SumRet = rSum
	m.SumSqSig = sSq
	m.SumSqRet = rSq
	m.SumProd = prodSum
	m.SumPnL = pnlSum
	m.SumSqPnL = pnlSq
	m.Hits = hits
	m.ValidHits = valid
	m.Turnover = turnover

	return m
}

// FinalizeMetrics computes the ratios from aggregated moments.
func FinalizeMetrics(m Moments) MetricStats {
	if m.Count <= 1 {
		return MetricStats{}
	}

	ms := MetricStats{Count: int(m.Count)}

	// 1. Global Pearson IC
	num := m.Count*m.SumProd - m.SumSig*m.SumRet
	denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
	denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
	if denX > 0 && denY > 0 {
		ms.ICPearson = num / math.Sqrt(denX*denY)
	}

	// 2. Per-Trade Sharpe
	meanPnL := m.SumPnL / m.Count
	varPnL := (m.SumSqPnL / m.Count) - (meanPnL * meanPnL)

	if varPnL > 0 {
		ms.Sharpe = meanPnL / math.Sqrt(varPnL)
		// Annualized via t-stat logic
		ms.SharpeAnnual = ms.Sharpe * math.Sqrt(m.Count)
	}

	// 3. Hit Rate
	if m.ValidHits > 0 {
		ms.HitRate = m.Hits / m.ValidHits
	}

	// 4. Breakeven
	if m.Turnover > 0 {
		ms.BreakevenBps = (m.SumPnL / m.Turnover) * 10000.0
	}

	return ms
}
```

// --- End File: metrics.go ---

// --- File: ofibuild.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"time"
)

// --- Build Configuration ---

// Hint for per-thread output buffer reservation; real days can exceed this and
// the buffer is grown as needed.
const BuildMaxRows = 10_000_000

// --- Main Builder (Single Champion Variant) ---

func runBuild() {
	start := time.Now()

	featRoot := filepath.Join(BaseDir, "features", Symbol)
	variantID := "B_Hawkes_Adaptive"

	fmt.Printf("--- FEATURE BUILDER | %s | 1 Variants ---\n", Symbol)
	fmt.Printf("Output: %s\n", featRoot)

	// Ensure variant directory exists.
	if err := os.MkdirAll(filepath.Join(featRoot, variantID), 0755); err != nil {
		fmt.Printf("[err] mkdir %s: %v\n", featRoot, err)
		return
	}

	// Discover all (Y,M,D) with data.
	tasks := discoverTasks()
	fmt.Printf("[build] Processing %d days.\n", len(tasks))

	workerCount := CPUThreads
	if workerCount < 1 {
		workerCount = 1
	}
	fmt.Printf("[build] Using %d threads.\n", workerCount)

	// Champion config (same as before).
	cfg := HawkesAdaptiveConfig{
		HawkesCfg: Hawkes2ScaleConfig{
			TauFast: 2, TauSlow: 300,
			MuBuy:  0.1,
			MuSell: 0.1,

			A_pp_fast: 1.2, A_pm_fast: 0.0,
			A_mp_fast: 0.0, A_mm_fast: 1.2,

			A_pp_slow: 0.3, A_pm_slow: 0.1,
			A_mp_slow: 0.1, A_mm_slow: 0.3,

			D0:           50_000,
			VolLambda:    0.999,
			ZScoreLambda: 0.9999,
			SquashScale:  2.0,
		},
		ActivityLambda: 0.99,
		ActMid:         15.0,
		ActSlope:       2.0,
	}

	jobs := make(chan ofiTask, len(tasks))
	var wg sync.WaitGroup

	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			// Per-thread reusable output buffer.
			binBuf := make([]byte, 0, BuildMaxRows*8)

			for t := range jobs {
				processBuildDay(t, featRoot, variantID, cfg, &binBuf)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()

	fmt.Printf("[build] Complete in %s\n", time.Since(start))
}

// processBuildDay: load raw AGG3 once, run champion model over all rows.
func processBuildDay(
	t ofiTask,
	root string,
	variantID string,
	cfg HawkesAdaptiveConfig,
	binBuf *[]byte,
) {
	rawBytes, rowCount, ok := loadRawDay(t.Y, t.M, t.D)
	if !ok || rowCount == 0 {
		return
	}

	n := int(rowCount)
	dateStr := fmt.Sprintf("%04d%02d%02d", t.Y, t.M, t.D)
	outPath := filepath.Join(root, variantID, dateStr+".bin")

	// Skip if already built.
	if _, err := os.Stat(outPath); err == nil {
		return
	}

	// Ensure buffer capacity.
	reqSize := n * 8
	if cap(*binBuf) < reqSize {
		*binBuf = make([]byte, reqSize)
	}
	*binBuf = (*binBuf)[:reqSize]

	// Concrete model instance (no interface dispatch).
	st := NewHawkesAdaptiveState(cfg)

	// Hot loop: parse row, update model, write float64.
	for i := 0; i < n; i++ {
		off := i * RowSize
		row := ParseAggRow(rawBytes[off : off+RowSize])

		sig := st.Update(row)

		binary.LittleEndian.PutUint64((*binBuf)[i*8:], math.Float64bits(sig))
	}

	if err := os.WriteFile(outPath, *binBuf, 0644); err != nil {
		fmt.Printf("[err] write %s: %v\n", outPath, err)
	}
}

// --- Data Loader (Same Binary Layout as data.go) ---

// loadRawDay decompresses one day's AGG3 blob and returns the raw rows buffer
// (header stripped) and rowCount.
func loadRawDay(y, m, d int) ([]byte, uint64, bool) {
	dir := filepath.Join(BaseDir, Symbol, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, d)
	if length == 0 {
		return nil, 0, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()

	if _, err := f.Seek(int64(offset), io.SeekStart); err != nil {
		return nil, 0, false
	}

	compData := make([]byte, length)
	if _, err := io.ReadFull(f, compData); err != nil {
		return nil, 0, false
	}

	r, err := zlib.NewReader(bytes.NewReader(compData))
	if err != nil {
		return nil, 0, false
	}
	raw, err := io.ReadAll(r)
	r.Close()
	if err != nil {
		return nil, 0, false
	}

	if len(raw) < HeaderSize {
		return nil, 0, false
	}

	rowCount := binary.LittleEndian.Uint64(raw[8:])
	return raw[HeaderSize:], rowCount, true
}

// findBlobOffset reads the index file and returns (offset,length) for day d.
func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(f, hdr); err != nil {
		return 0, 0
	}
	if string(hdr[:4]) != IdxMagic {
		return 0, 0
	}

	count := binary.LittleEndian.Uint64(hdr[8:])
	row := make([]byte, 26)

	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}

// discoverTasks enumerates all (Y,M,D) from existing index files.
func discoverTasks() []ofiTask {
	root := filepath.Join(BaseDir, Symbol)
	var tasks []ofiTask

	years, err := os.ReadDir(root)
	if err != nil {
		return tasks
	}

	for _, yDir := range years {
		if !yDir.IsDir() {
			continue
		}
		y, err := strconv.Atoi(yDir.Name())
		if err != nil || y <= 0 {
			continue
		}

		months, err := os.ReadDir(filepath.Join(root, yDir.Name()))
		if err != nil {
			continue
		}
		for _, mDir := range months {
			if !mDir.IsDir() {
				continue
			}
			m, err := strconv.Atoi(mDir.Name())
			if err != nil || m < 1 || m > 12 {
				continue
			}

			idxPath := filepath.Join(root, yDir.Name(), mDir.Name(), "index.quantdev")
			f, err := os.Open(idxPath)
			if err != nil {
				continue
			}

			hdr := make([]byte, 16)
			if _, err := io.ReadFull(f, hdr); err != nil {
				f.Close()
				continue
			}
			if string(hdr[:4]) != IdxMagic {
				f.Close()
				continue
			}

			count := binary.LittleEndian.Uint64(hdr[8:])
			row := make([]byte, 26)
			for i := uint64(0); i < count; i++ {
				if _, err := io.ReadFull(f, row); err != nil {
					break
				}
				d := int(binary.LittleEndian.Uint16(row[0:]))
				if d >= 1 && d <= 31 {
					tasks = append(tasks, ofiTask{Y: y, M: m, D: d})
				}
			}
			f.Close()
		}
	}

	return tasks
}

type ofiTask struct{ Y, M, D int }

// --- Champion Model: Hawkes Adaptive ---------------------------------------

// Hawkes2ScaleConfig: shared kernel parameters.
type Hawkes2ScaleConfig struct {
	TauFast, TauSlow, MuBuy, MuSell            float64
	A_pp_fast, A_pm_fast, A_mp_fast, A_mm_fast float64
	A_pp_slow, A_pm_slow, A_mp_slow, A_mm_slow float64
	D0, VolLambda, ZScoreLambda, SquashScale   float64
}

type HawkesAdaptiveConfig struct {
	HawkesCfg                        Hawkes2ScaleConfig
	ActivityLambda, ActMid, ActSlope float64
}

type HawkesAdaptiveState struct {
	base                                         Hawkes2ScaleConfig
	lastTsMs                                     int64
	eBuyFast, eSellFast                          float64
	eBuySlow, eSellSlow                          float64
	actLambda, actEWMA, actMid, actSlope, squash float64
	vol                                          VolEWMA
	z                                            ZScoreEWMA
}

func NewHawkesAdaptiveState(cfg HawkesAdaptiveConfig) *HawkesAdaptiveState {
	return &HawkesAdaptiveState{
		base:      cfg.HawkesCfg,
		actLambda: cfg.ActivityLambda,
		actMid:    cfg.ActMid,
		actSlope:  cfg.ActSlope,
		squash:    cfg.HawkesCfg.SquashScale,
		vol:       VolEWMA{Lambda: cfg.HawkesCfg.VolLambda},
		z:         ZScoreEWMA{Lambda: cfg.HawkesCfg.ZScoreLambda},
	}
}

func (st *HawkesAdaptiveState) Update(row AggRow) float64 {
	ts := row.TsMs
	d := TradeDollar(row)
	s := TradeSign(row)
	px := TradePrice(row)

	// dt in seconds
	var dtSec float64
	if st.lastTsMs == 0 {
		st.lastTsMs = ts
		dtSec = 0
	} else {
		dtSec = float64(ts-st.lastTsMs) / 1000.0
		if dtSec < 0 {
			dtSec = 0
		}
		st.lastTsMs = ts
	}

	// Activity EWMA + Hawkes decay
	if dtSec > 0 {
		st.actEWMA = st.actLambda*st.actEWMA + (1-st.actLambda)*(1.0/dtSec)

		df := math.Exp((-1.0 / st.base.TauFast) * dtSec)
		ds := math.Exp((-1.0 / st.base.TauSlow) * dtSec)

		st.eBuyFast *= df
		st.eSellFast *= df
		st.eBuySlow *= ds
		st.eSellSlow *= ds
	}

	// Log-saturated mark on dollar flow (uses Log1p for precision).
	mark := 0.0
	if d > 0 && st.base.D0 > 0 {
		mark = math.Log1p(d / st.base.D0)
	}

	// Signed excitation update.
	if s > 0 {
		st.eBuyFast += mark
		st.eBuySlow += mark
	} else {
		st.eSellFast += mark
		st.eSellSlow += mark
	}

	// Fast kernel intensities.
	bf := st.base.MuBuy + st.base.A_pp_fast*st.eBuyFast + st.base.A_pm_fast*st.eSellFast
	sf := st.base.MuSell + st.base.A_mp_fast*st.eBuyFast + st.base.A_mm_fast*st.eSellFast

	// Slow kernel intensities.
	bs := st.base.MuBuy + st.base.A_pp_slow*st.eBuySlow + st.base.A_pm_slow*st.eSellSlow
	ss := st.base.MuSell + st.base.A_mp_slow*st.eBuySlow + st.base.A_mm_slow*st.eSellSlow

	// Activity-based slow weight (higher activity -> lower slow weight).
	wSlow := 0.5
	if st.actEWMA > 0 {
		x := (math.Log(st.actEWMA+1e-9) - math.Log(st.actMid+1e-9)) * st.actSlope
		wSlow = 1.0 / (1.0 + math.Exp(x))
	}
	if wSlow < 0 {
		wSlow = 0
	} else if wSlow > 1 {
		wSlow = 1
	}
	wFast := 1.0 - wSlow

	buy := wFast*bf + wSlow*bs
	sell := wFast*sf + wSlow*ss
	if buy < 0 {
		buy = 0
	}
	if sell < 0 {
		sell = 0
	}

	// Directional imbalance.
	imb := 0.0
	if den := buy + sell; den > 1e-12 {
		imb = (buy - sell) / den
	}

	// Vol normalization and z-scoring.
	st.vol.Update(px)
	sigma := st.vol.Sigma()
	if sigma <= 0 {
		sigma = 1
	}

	zVal := st.z.Update(imb / sigma)
	return Squash(zVal, st.squash)
}

// --- Shared EWMA Helpers ----------------------------------------------------

type VolEWMA struct {
	Lambda  float64
	VarEwma float64
	LastPx  float64
	HasLast bool
}

func (v *VolEWMA) Update(price float64) {
	if !v.HasLast {
		v.LastPx = price
		v.HasLast = true
		return
	}
	if price <= 0 {
		return
	}
	r := math.Log(price / v.LastPx)
	v.LastPx = price
	v.VarEwma = v.Lambda*v.VarEwma + (1-v.Lambda)*r*r
}

func (v *VolEWMA) Sigma() float64 {
	if v.VarEwma <= 0 {
		return 0
	}
	return math.Sqrt(v.VarEwma)
}

type ZScoreEWMA struct {
	Lambda float64
	Mean   float64
	Var    float64
	Warmed bool
}

func (z *ZScoreEWMA) Update(x float64) float64 {
	if !z.Warmed {
		z.Mean = x
		z.Var = 0
		z.Warmed = true
		return 0
	}
	mPrev := z.Mean
	z.Mean = z.Lambda*z.Mean + (1-z.Lambda)*x
	dx := x - mPrev
	z.Var = z.Lambda*z.Var + (1-z.Lambda)*dx*dx
	if z.Var <= 0 {
		return 0
	}
	return (x - z.Mean) / math.Sqrt(z.Var)
}

func Squash(x, scale float64) float64 {
	return math.Tanh(scale * x)
}
```

// --- End File: ofibuild.go ---

// --- File: ofistudy.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"math"
	"os"
	"path/filepath"
	"slices"
	"sort"
	"strings"
	"sync"
	"text/tabwriter"
	"time"
)

// --- Study Config ---
const (
	OOSDateStr   = "2024-01-01"
	StudyThreads = 24
	StudyMaxRows = 10_000_000
)

var TimeHorizonsSec = []int{10, 30, 60, 180, 300}
var oosBoundaryYMD int

func init() {
	oosBoundaryYMD = parseOOSBoundary(OOSDateStr)
}

// DayResult carries raw Moments instead of final Stats
type DayResult struct {
	YMD     int
	Moments [][]Moments // [Variant][Horizon]
}

func runStudy() {
	startT := time.Now()
	featRoot := filepath.Join(BaseDir, "features", Symbol)

	// 1. Discover Variants
	entries, err := os.ReadDir(featRoot)
	if err != nil {
		fmt.Printf("[err] reading feature dir: %v\n", err)
		return
	}
	var variants []string
	for _, e := range entries {
		if e.IsDir() {
			variants = append(variants, e.Name())
		}
	}
	slices.Sort(variants)
	if len(variants) == 0 {
		fmt.Println("[warn] No variants found.")
		return
	}

	fmt.Printf("--- OFI STUDY | %s | %d Variants | Split: %s ---\n", Symbol, len(variants), OOSDateStr)

	// 2. Discover Common Days
	tasks := discoverStudyDays(filepath.Join(featRoot, variants[0]))
	fmt.Printf("[job] Processing %d days using %d threads.\n", len(tasks), StudyThreads)

	// 3. Global Accumulators: [Variant][Horizon]
	isAcc := make([][]Moments, len(variants))
	oosAcc := make([][]Moments, len(variants))
	for i := range variants {
		isAcc[i] = make([]Moments, len(TimeHorizonsSec))
		oosAcc[i] = make([]Moments, len(TimeHorizonsSec))
	}

	// Track Day Counts for display
	isDays := 0
	oosDays := 0

	// 4. Parallel Pipeline
	resultsChan := make(chan DayResult, 64)
	jobsChan := make(chan int, len(tasks))
	var wg sync.WaitGroup

	for i := 0; i < StudyThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()

			// Thread-local allocation
			maxRows := StudyMaxRows
			prices := make([]float64, maxRows)
			times := make([]int64, maxRows)
			sigBuf := make([]float64, maxRows)
			scratchSig := make([]float64, 0, maxRows)
			scratchRet := make([]float64, 0, maxRows)
			fileBuf := make([]byte, maxRows*8)

			for idx := range jobsChan {
				dayInt := tasks[idx]
				res := processStudyDay(
					dayInt, variants, featRoot,
					prices, times, sigBuf, fileBuf,
					&scratchSig, &scratchRet,
				)
				resultsChan <- res
			}
		}()
	}

	for i := range tasks {
		jobsChan <- i
	}
	close(jobsChan)

	go func() {
		wg.Wait()
		close(resultsChan)
	}()

	// 5. Aggregation (Main Thread)
	for res := range resultsChan {
		if len(res.Moments) == 0 {
			continue
		}
		isOOS := res.YMD >= oosBoundaryYMD

		if isOOS {
			oosDays++
		} else {
			isDays++
		}

		for vIdx := range variants {
			for hIdx := range TimeHorizonsSec {
				m := res.Moments[vIdx][hIdx]
				if m.Count > 0 {
					if isOOS {
						oosAcc[vIdx][hIdx].Add(m)
					} else {
						isAcc[vIdx][hIdx].Add(m)
					}
				}
			}
		}
	}

	// 6. Reporting
	fmt.Println()
	approxIS := isDays
	approxOOS := oosDays

	for hIdx, sec := range TimeHorizonsSec {
		printHorizonTable(sec, variants, isAcc, oosAcc, hIdx, approxIS, approxOOS)
		fmt.Println()
	}

	fmt.Printf("[study] Complete in %s\n", time.Since(startT))
}

func processStudyDay(
	dayInt int,
	variants []string,
	featRoot string,
	prices []float64,
	times []int64,
	sigBuf []float64,
	fileBuf []byte,
	scratchSig, scratchRet *[]float64,
) DayResult {
	y, m, d := dayInt/10000, (dayInt%10000)/100, dayInt%100

	res := DayResult{
		YMD:     dayInt,
		Moments: make([][]Moments, len(variants)),
	}

	rawBytes, rowCount, ok := loadRawDay(y, m, d)
	if !ok || rowCount == 0 {
		return res
	}
	n := int(rowCount)

	// Buffer management
	if n > cap(prices) {
		newCap := n + n/4
		prices = make([]float64, newCap)
		times = make([]int64, newCap)
		sigBuf = make([]float64, newCap)
	}
	if n*8 > cap(fileBuf) {
		fileBuf = make([]byte, n*8+1024)
	}
	if n > cap(*scratchSig) {
		*scratchSig = make([]float64, 0, n)
		*scratchRet = make([]float64, 0, n)
	}

	prices = prices[:n]
	times = times[:n]
	sigBuf = sigBuf[:n]

	// Parse Raw (Vectorized)
	for i := 0; i < n; i++ {
		off := i * RowSize
		prices[i] = float64(binary.LittleEndian.Uint64(rawBytes[off+8:]))
		times[i] = int64(binary.LittleEndian.Uint64(rawBytes[off+38:]))
	}

	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	for vIdx, v := range variants {
		sigPath := filepath.Join(featRoot, v, dStr+".bin")
		loadedSigs, ok := fastLoadFloats(sigPath, fileBuf, sigBuf)

		res.Moments[vIdx] = make([]Moments, len(TimeHorizonsSec))

		if !ok || len(loadedSigs) != n {
			continue
		}

		for hIdx, sec := range TimeHorizonsSec {
			prepSig, prepRet := prepareVectors(loadedSigs, prices, times, sec*1000, scratchSig, scratchRet)
			res.Moments[vIdx][hIdx] = CalcMoments(prepSig, prepRet)
		}
	}
	return res
}

func prepareVectors(
	sig, prices []float64,
	times []int64,
	horizonMs int,
	scratchSig, scratchRet *[]float64,
) ([]float64, []float64) {
	n := len(sig)
	vSig := (*scratchSig)[:0]
	vRet := (*scratchRet)[:0]

	j := 0
	hVal := int64(horizonMs)

	for i := 0; i < n; i++ {
		pStart := prices[i]
		s := sig[i]
		if pStart <= 0 || s == 0 {
			continue
		}

		tTarget := times[i] + hVal

		if j < i+1 {
			j = i + 1
		}
		for j < n && times[j] < tTarget {
			j++
		}
		if j >= n {
			break
		}

		pEnd := prices[j]
		if pEnd > 0 {
			vSig = append(vSig, s)
			vRet = append(vRet, (pEnd-pStart)/pStart)
		}
	}

	return vSig, vRet
}

func fastLoadFloats(path string, fileBuf []byte, outBuf []float64) ([]float64, bool) {
	f, err := os.Open(path)
	if err != nil {
		return nil, false
	}
	defer f.Close()

	fi, err := f.Stat()
	if err != nil {
		return nil, false
	}
	size := int(fi.Size())
	if size <= 0 || size%8 != 0 {
		return nil, false
	}
	count := size / 8

	if count > cap(outBuf) {
		outBuf = make([]float64, count)
	} else {
		outBuf = outBuf[:count]
	}

	if cap(fileBuf) < size {
		fileBuf = make([]byte, size)
	}
	fileBuf = fileBuf[:size]

	if _, err := io.ReadFull(f, fileBuf); err != nil {
		return nil, false
	}

	for i := 0; i < count; i++ {
		outBuf[i] = math.Float64frombits(binary.LittleEndian.Uint64(fileBuf[i*8:]))
	}

	return outBuf, true
}

func printHorizonTable(
	sec int,
	variants []string,
	isAcc, oosAcc [][]Moments,
	hIdx, isDays, oosDays int,
) {
	type row struct {
		Name string
		IS   MetricStats
		OOS  MetricStats
	}
	var rows []row

	for vIdx, v := range variants {
		rows = append(rows, row{
			Name: v,
			IS:   FinalizeMetrics(isAcc[vIdx][hIdx]),
			OOS:  FinalizeMetrics(oosAcc[vIdx][hIdx]),
		})
	}

	slices.SortFunc(rows, func(a, b row) int {
		if a.OOS.Sharpe > b.OOS.Sharpe {
			return -1
		}
		if a.OOS.Sharpe < b.OOS.Sharpe {
			return 1
		}
		return 0
	})

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	// --- FIX: Use isDays and oosDays in the header ---
	fmt.Fprintf(w, "== Horizon %d sec [IS: %d days | OOS: %d days] ==\n", sec, isDays, oosDays)
	fmt.Fprintln(w, "VARIANT\tIS_IC\tOOS_IC\tIS_SR(Tr)\tOOS_SR(Tr)\tIS_HIT\tOOS_HIT\tIS_BE\tOOS_BE")
	fmt.Fprintln(w, "-------\t-----\t------\t---------\t----------\t------\t-------\t-----\t------")

	for _, r := range rows {
		fmt.Fprintf(w, "%s\t%.4f\t%.4f\t%.4f\t%.4f\t%.1f%%\t%.1f%%\t%.1f\t%.1f\n",
			r.Name,
			r.IS.ICPearson, r.OOS.ICPearson,
			r.IS.Sharpe, r.OOS.Sharpe,
			r.IS.HitRate*100, r.OOS.HitRate*100,
			r.IS.BreakevenBps, r.OOS.BreakevenBps,
		)
	}
	w.Flush()
}

func discoverStudyDays(vDir string) []int {
	var days []int
	files, _ := os.ReadDir(vDir)
	for _, f := range files {
		if strings.HasSuffix(f.Name(), ".bin") {
			name := strings.TrimSuffix(f.Name(), ".bin")
			if len(name) == 8 {
				if val := fastAtoi(name); val > 0 {
					days = append(days, val)
				}
			}
		}
	}
	sort.Ints(days)
	return days
}
func parseOOSBoundary(d string) int {
	return fastAtoi(d[0:4])*10000 + fastAtoi(d[5:7])*100 + fastAtoi(d[8:10])
}
func fastAtoi(s string) int {
	n := 0
	for i := 0; i < len(s); i++ {
		n = n*10 + int(s[i]-'0')
	}
	return n
}
```

// --- End File: ofistudy.go ---

// --- File: sanity.go ---

```go
package main

import (
	"bytes"
	"compress/zlib"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
)

// runSanity verifies that all month-level data/index files are structurally sound
// and that checksums and basic header/length invariants hold.
func runSanity() {
	root := filepath.Join(BaseDir, Symbol)
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("SANITY: cannot read root %s: %v\n", root, err)
		return
	}

	var tasks []string
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		months, err := os.ReadDir(filepath.Join(root, y.Name()))
		if err != nil {
			fmt.Printf("SANITY: cannot read year %s: %v\n", y.Name(), err)
			continue
		}
		for _, m := range months {
			if m.IsDir() {
				tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
			}
		}
	}

	fmt.Printf("SANITY CHECK: %s (%d months)\n", Symbol, len(tasks))

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()
}

func validateMonth(dir string) {
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	fIdx, err := os.Open(idxPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Index: %v)\n", dir, err)
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		fmt.Printf("FAIL: %s (No Data: %v)\n", dir, err)
		return
	}
	defer fData.Close()

	hdr := make([]byte, 16)
	if _, err := io.ReadFull(fIdx, hdr); err != nil {
		fmt.Printf("FAIL: %s (Index header read error: %v)\n", dir, err)
		return
	}

	if string(hdr[:4]) != IdxMagic {
		fmt.Printf("FAIL: %s (Bad index magic)\n", dir)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])
	row := make([]byte, 26)
	issues := 0

	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row); err != nil {
			fmt.Printf("FAIL: %s (Index row read error at %d: %v)\n", dir, i, err)
			issues++
			break
		}

		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		if length <= 0 {
			issues++
			continue
		}

		// Read compressed blob
		compData := make([]byte, length)
		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			issues++
			continue
		}
		if _, err := io.ReadFull(fData, compData); err != nil {
			issues++
			continue
		}

		r, err := zlib.NewReader(bytes.NewReader(compData))
		if err != nil {
			issues++
			continue
		}
		aggBlob, err := io.ReadAll(r)
		r.Close()
		if err != nil {
			issues++
			continue
		}

		// Check checksum
		s := sha256.Sum256(aggBlob)
		if binary.LittleEndian.Uint64(s[:8]) != expSum {
			issues++
			continue
		}

		// Basic structural checks
		if len(aggBlob) < HeaderSize {
			issues++
			continue
		}

		// Validate header magic
		if string(aggBlob[:4]) != AggMagic {
			issues++
			continue
		}

		rowCount := binary.LittleEndian.Uint64(aggBlob[8:])
		expectedLen := HeaderSize + int(rowCount)*RowSize
		if expectedLen != len(aggBlob) {
			issues++
			continue
		}
	}

	if issues > 0 {
		fmt.Printf("ISSUES: %s (%d errors)\n", dir, issues)
	}
}
```

// --- End File: sanity.go ---

