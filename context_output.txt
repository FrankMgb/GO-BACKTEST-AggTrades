PS Z:\Benchmarks> go run . build
go1.25.5 X:jsonv2,greenteagc | Env: windows/amd64 | Threads: 24 | RAM Limit: 24GB | GOGC: 200 | GOAMD64: v4
>>> Building BTCUSDT (Canonical 2025 Atoms)
[build] Complete in 20.0234607s

[sys] Execution Time: 20.0239951s | Mem: 9958 MB
PS Z:\Benchmarks> go run . study 
go1.25.5 X:jsonv2,greenteagc | Env: windows/amd64 | Threads: 24 | RAM Limit: 24GB | GOGC: 200 | GOAMD64: v4

>>> STUDY: BTCUSDT <<<
Variants: 1 | Days: 2163
[========================================] 100.0% (2163/2163) | 112.8 days/s  
== Horizon 0.500s [IS: 1461 | OOS: 702] ==
FEATURE                 IS_IC    IS_T    OOS_IC   OOS_T   AC1     |AC1|  AVG_SEG   MAX_SEG  IS_BPS/TR  OOS_BPS/TR
Atoms_v1_f01_OFI        0.0653   88.05   0.0795   90.58   0.098   0.084  3.11      1103.0   0.30       0.36
Atoms_v1_f02_TCI        0.1406   76.40   0.3204   113.30  0.357   0.000  3.11      1103.0   0.38       0.77
Atoms_v1_f03_Whale      0.0222   59.43   0.0207   44.57   0.041   0.042  1.03      6.0      0.52       0.68
Atoms_v1_f04_Lumpiness  -0.0110  -56.47  -0.0066  -45.08  0.018   0.018  3.11      1103.0   -0.60      -0.85
Atoms_v1_f05_Sweep      0.0974   89.27   0.1531   93.49   0.110   0.071  3.11      1103.0   0.30       0.37
Atoms_v1_f06_Fragility  0.0816   48.84   0.2445   83.92   0.190   0.225  3.11      1103.0   0.19       0.41
Atoms_v1_f07_Magnet     -0.0006  -2.81   -0.0001  0.52    0.386   0.386  33155.20  65467.0  0.07       0.06
Atoms_v1_f08_Velocity   0.0204   68.77   0.0201   53.40   0.008   0.013  1.64      128.0    0.38       0.40
Atoms_v1_f09_Accel      0.0000   -1.71   -0.0001  -1.21   -0.507  0.514  1.43      10.0     0.00       -0.00
Atoms_v1_f10_Gap        0.0168   53.20   0.0173   50.47   0.047   0.337  1.64      128.0    0.06       0.03
Atoms_v1_f11_DGT        0.0582   78.73   0.0746   81.47   0.073   0.064  2.33      1101.0   0.29       0.38
Atoms_v1_f12_Absorb     0.0295   91.29   0.0296   67.97   0.059   0.054  1.63      103.0    0.27       0.25
Atoms_v1_f13_Fractal    -0.0005  2.54    0.0043   2.66    0.103   0.103  9.00      6431.0   -0.00      0.03

-- Monotonicity Check (IS) Horizon 0.500s --
FEATURE                MONO   B1(Sell) B2   B3   B4   B5(Buy)
Atoms_v1_f01_OFI       0.995  -0.3     -0.2 -0.0 0.2  0.3
Atoms_v1_f02_TCI       0.953  -0.2     -0.2 0.0  0.2  0.3
Atoms_v1_f03_Whale     0.435  0.0      0.0  0.0  -0.0 0.0
Atoms_v1_f04_Lumpiness -0.995 0.3      0.2  -0.0 -0.2 -0.3
Atoms_v1_f05_Sweep     0.997  -0.3     -0.2 0.0  0.2  0.3
Atoms_v1_f06_Fragility 0.951  -0.2     -0.2 0.0  0.2  0.2
Atoms_v1_f07_Magnet    -0.961 0.0      0.0  0.0  0.0  0.0
Atoms_v1_f08_Velocity  0.927  -0.2     -0.0 0.0  0.0  0.2
Atoms_v1_f09_Accel     -0.531 0.0      0.0  0.0  -0.0 0.0
Atoms_v1_f10_Gap       0.922  -0.1     -0.1 0.0  0.1  0.1
Atoms_v1_f11_DGT       0.986  -0.3     -0.2 -0.0 0.2  0.3
Atoms_v1_f12_Absorb    0.869  -0.0     0.0  0.0  -0.0 0.0
Atoms_v1_f13_Fractal   0.475  0.0      0.0  0.0  0.0  0.0

== Horizon 1.000s [IS: 1461 | OOS: 702] ==
FEATURE                 IS_IC    IS_T    OOS_IC   OOS_T   AC1     |AC1|  AVG_SEG   MAX_SEG  IS_BPS/TR  OOS_BPS/TR
Atoms_v1_f01_OFI        0.0604   84.21   0.0707   79.60   0.098   0.084  3.11      1103.0   0.35       0.38
Atoms_v1_f02_TCI        0.1233   73.58   0.2942   101.70  0.357   0.000  3.11      1103.0   0.41       0.83
Atoms_v1_f03_Whale      0.0212   58.70   0.0189   42.30   0.041   0.042  1.03      6.0      0.62       0.73
Atoms_v1_f04_Lumpiness  -0.0106  -54.02  -0.0061  -39.49  0.018   0.018  3.11      1103.0   -0.72      -0.92
Atoms_v1_f05_Sweep      0.0860   86.12   0.1417   88.66   0.110   0.071  3.11      1103.0   0.33       0.41
Atoms_v1_f06_Fragility  0.0717   46.54   0.2295   80.69   0.190   0.225  3.11      1103.0   0.20       0.45
Atoms_v1_f07_Magnet     -0.0008  -3.17   -0.0003  0.71    0.386   0.386  33155.20  65467.0  0.13       0.04
Atoms_v1_f08_Velocity   0.0186   66.87   0.0181   50.05   0.008   0.013  1.64      128.0    0.44       0.42
Atoms_v1_f09_Accel      -0.0000  -0.48   -0.0000  -0.86   -0.507  0.514  1.43      10.0     -0.00      -0.00
Atoms_v1_f10_Gap        0.0157   58.96   0.0175   52.64   0.047   0.337  1.64      128.0    0.07       0.04
Atoms_v1_f11_DGT        0.0542   77.09   0.0659   73.39   0.073   0.064  2.33      1101.0   0.34       0.40
Atoms_v1_f12_Absorb     0.0267   80.61   0.0270   60.86   0.059   0.054  1.63      103.0    0.30       0.27
Atoms_v1_f13_Fractal    0.0018   3.38    0.0006   2.11    0.103   0.103  9.00      6431.0   0.02       0.01

-- Monotonicity Check (IS) Horizon 1.000s --
FEATURE                MONO   B1(Sell) B2   B3   B4   B5(Buy)
Atoms_v1_f01_OFI       0.994  -0.3     -0.2 -0.0 0.2  0.3
Atoms_v1_f02_TCI       0.950  -0.3     -0.3 0.0  0.3  0.3
Atoms_v1_f03_Whale     0.740  0.0      0.0  0.0  0.0  0.0
Atoms_v1_f04_Lumpiness -0.994 0.3      0.2  0.0  -0.2 -0.3
Atoms_v1_f05_Sweep     0.998  -0.4     -0.2 0.0  0.2  0.4
Atoms_v1_f06_Fragility 0.956  -0.3     -0.2 0.0  0.2  0.3
Atoms_v1_f07_Magnet    -0.300 0.0      0.0  0.0  0.0  0.0
Atoms_v1_f08_Velocity  0.931  -0.3     -0.0 0.0  0.0  0.3
Atoms_v1_f09_Accel     -0.284 0.0      0.0  0.0  -0.0 0.0
Atoms_v1_f10_Gap       0.913  -0.1     -0.1 0.0  0.2  0.1
Atoms_v1_f11_DGT       0.983  -0.3     -0.2 -0.0 0.3  0.3
Atoms_v1_f12_Absorb    0.865  -0.0     0.0  0.0  0.0  0.1
Atoms_v1_f13_Fractal   0.899  0.0      0.0  0.0  0.0  0.0

== Horizon 2.000s [IS: 1461 | OOS: 702] ==
FEATURE                 IS_IC    IS_T    OOS_IC   OOS_T   AC1     |AC1|  AVG_SEG   MAX_SEG  IS_BPS/TR  OOS_BPS/TR
Atoms_v1_f01_OFI        0.0517   74.59   0.0632   74.60   0.098   0.084  3.11      1103.0   0.40       0.40
Atoms_v1_f02_TCI        0.1039   68.22   0.2679   93.60   0.357   0.000  3.11      1103.0   0.47       0.90
Atoms_v1_f03_Whale      0.0182   53.29   0.0168   40.70   0.041   0.042  1.03      6.0      0.72       0.78
Atoms_v1_f04_Lumpiness  -0.0091  -48.70  -0.0053  -38.76  0.018   0.018  3.11      1103.0   -0.82      -0.96
Atoms_v1_f05_Sweep      0.0717   77.85   0.1300   83.87   0.110   0.071  3.11      1103.0   0.36       0.45
Atoms_v1_f06_Fragility  0.0605   44.06   0.2112   77.08   0.190   0.225  3.11      1103.0   0.23       0.50
Atoms_v1_f07_Magnet     -0.0009  -2.09   -0.0005  0.48    0.386   0.386  33155.20  65467.0  0.24       -0.06
Atoms_v1_f08_Velocity   0.0155   59.90   0.0165   47.77   0.008   0.013  1.64      128.0    0.49       0.45
Atoms_v1_f09_Accel      -0.0000  -1.03   -0.0000  -0.60   -0.507  0.514  1.43      10.0     -0.00      -0.00
Atoms_v1_f10_Gap        0.0135   62.53   0.0174   54.94   0.047   0.337  1.64      128.0    0.08       0.05
Atoms_v1_f11_DGT        0.0467   69.31   0.0588   68.75   0.073   0.064  2.33      1101.0   0.39       0.43
Atoms_v1_f12_Absorb     0.0223   70.38   0.0244   56.43   0.059   0.054  1.63      103.0    0.34       0.29
Atoms_v1_f13_Fractal    0.0048   3.67    -0.0045  1.36    0.103   0.103  9.00      6431.0   0.07       -0.04

-- Monotonicity Check (IS) Horizon 2.000s --
FEATURE                MONO   B1(Sell) B2   B3  B4   B5(Buy)
Atoms_v1_f01_OFI       0.992  -0.3     -0.2 0.0 0.2  0.4
Atoms_v1_f02_TCI       0.948  -0.3     -0.3 0.0 0.3  0.3
Atoms_v1_f03_Whale     0.455  -0.0     0.0  0.0 0.0  0.0
Atoms_v1_f04_Lumpiness -0.992 0.4      0.3  0.0 -0.2 -0.3
Atoms_v1_f05_Sweep     0.998  -0.4     -0.2 0.0 0.3  0.4
Atoms_v1_f06_Fragility 0.961  -0.3     -0.3 0.0 0.3  0.3
Atoms_v1_f07_Magnet    -0.637 0.0      0.0  0.0 0.0  0.0
Atoms_v1_f08_Velocity  0.928  -0.3     -0.0 0.0 0.0  0.3
Atoms_v1_f09_Accel     -0.266 0.0      0.0  0.0 -0.0 0.0
Atoms_v1_f10_Gap       0.915  -0.1     -0.2 0.0 0.2  0.1
Atoms_v1_f11_DGT       0.980  -0.4     -0.3 0.0 0.3  0.4
Atoms_v1_f12_Absorb    0.793  -0.1     0.0  0.0 0.0  0.1
Atoms_v1_f13_Fractal   0.930  0.0      0.0  0.0 0.0  0.0

== Horizon 5.000s [IS: 1461 | OOS: 702] ==
FEATURE                 IS_IC    IS_T    OOS_IC   OOS_T   AC1     |AC1|  AVG_SEG   MAX_SEG  IS_BPS/TR  OOS_BPS/TR
Atoms_v1_f01_OFI        0.0347   58.82   0.0481   60.62   0.098   0.084  3.11      1103.0   0.41       0.41
Atoms_v1_f02_TCI        0.0687   55.22   0.2110   74.21   0.357   0.000  3.11      1103.0   0.47       0.96
Atoms_v1_f03_Whale      0.0123   43.07   0.0129   35.94   0.041   0.042  1.03      6.0      0.73       0.81
Atoms_v1_f04_Lumpiness  -0.0059  -39.85  -0.0043  -33.73  0.018   0.018  3.11      1103.0   -0.81      -1.04
Atoms_v1_f05_Sweep      0.0481   61.75   0.1029   69.75   0.110   0.071  3.11      1103.0   0.37       0.48
Atoms_v1_f06_Fragility  0.0407   38.39   0.1700   66.78   0.190   0.225  3.11      1103.0   0.23       0.54
Atoms_v1_f07_Magnet     -0.0007  -1.96   -0.0005  0.16    0.386   0.386  33155.20  65467.0  0.36       -0.06
Atoms_v1_f08_Velocity   0.0101   47.71   0.0132   41.94   0.008   0.013  1.64      128.0    0.48       0.49
Atoms_v1_f09_Accel      0.0000   -0.93   -0.0000  -1.10   -0.507  0.514  1.43      10.0     0.00       -0.00
Atoms_v1_f10_Gap        0.0105   57.56   0.0160   48.22   0.047   0.337  1.64      128.0    0.09       0.06
Atoms_v1_f11_DGT        0.0306   54.52   0.0442   56.49   0.073   0.064  2.33      1101.0   0.39       0.43
Atoms_v1_f12_Absorb     0.0164   57.31   0.0196   47.55   0.059   0.054  1.63      103.0    0.38       0.31
Atoms_v1_f13_Fractal    0.0060   2.27    -0.0006  1.09    0.103   0.103  9.00      6431.0   0.12       -0.01

-- Monotonicity Check (IS) Horizon 5.000s --
FEATURE                MONO   B1(Sell) B2   B3  B4   B5(Buy)
Atoms_v1_f01_OFI       0.989  -0.3     -0.2 0.0 0.3  0.4
Atoms_v1_f02_TCI       0.952  -0.3     -0.3 0.0 0.3  0.3
Atoms_v1_f03_Whale     0.468  -0.0     0.0  0.0 0.0  0.0
Atoms_v1_f04_Lumpiness -0.989 0.4      0.3  0.0 -0.2 -0.3
Atoms_v1_f05_Sweep     0.998  -0.4     -0.2 0.0 0.3  0.4
Atoms_v1_f06_Fragility 0.958  -0.3     -0.3 0.0 0.3  0.3
Atoms_v1_f07_Magnet    -0.749 0.0      0.0  0.0 0.0  0.0
Atoms_v1_f08_Velocity  0.935  -0.3     -0.0 0.1 0.0  0.3
Atoms_v1_f09_Accel     -0.043 0.0      0.0  0.0 0.0  0.0
Atoms_v1_f10_Gap       0.914  -0.1     -0.2 0.1 0.2  0.2
Atoms_v1_f11_DGT       0.978  -0.3     -0.3 0.0 0.3  0.4
Atoms_v1_f12_Absorb    0.784  -0.1     0.0  0.0 0.0  0.1
Atoms_v1_f13_Fractal   0.786  0.0      0.0  0.0 0.0  0.0

== Horizon 10.000s [IS: 1461 | OOS: 702] ==
FEATURE                 IS_IC    IS_T    OOS_IC   OOS_T   AC1     |AC1|  AVG_SEG   MAX_SEG  IS_BPS/TR  OOS_BPS/TR
Atoms_v1_f01_OFI        0.0250   48.14   0.0375   52.54   0.098   0.084  3.11      1103.0   0.40       0.41
Atoms_v1_f02_TCI        0.0488   45.17   0.1696   63.07   0.357   0.000  3.11      1103.0   0.45       0.98
Atoms_v1_f03_Whale      0.0089   35.91   0.0103   30.78   0.041   0.042  1.03      6.0      0.72       0.81
Atoms_v1_f04_Lumpiness  -0.0043  -34.58  -0.0032  -30.67  0.018   0.018  3.11      1103.0   -0.80      -0.98
Atoms_v1_f05_Sweep      0.0340   50.68   0.0830   61.45   0.110   0.071  3.11      1103.0   0.35       0.49
Atoms_v1_f06_Fragility  0.0289   33.92   0.1394   59.92   0.190   0.225  3.11      1103.0   0.23       0.56
Atoms_v1_f07_Magnet     -0.0001  0.12    -0.0005  -0.63   0.386   0.386  33155.20  65467.0  0.74       -0.10
Atoms_v1_f08_Velocity   0.0071   39.59   0.0106   37.55   0.008   0.013  1.64      128.0    0.46       0.50
Atoms_v1_f09_Accel      -0.0000  -0.42   -0.0000  -0.65   -0.507  0.514  1.43      10.0     -0.00      -0.00
Atoms_v1_f10_Gap        0.0087   43.21   0.0141   37.48   0.047   0.337  1.64      128.0    0.11       0.07
Atoms_v1_f11_DGT        0.0214   43.57   0.0339   48.20   0.073   0.064  2.33      1101.0   0.37       0.42
Atoms_v1_f12_Absorb     0.0130   49.60   0.0161   42.93   0.059   0.054  1.63      103.0    0.40       0.32
Atoms_v1_f13_Fractal    0.0085   1.84    -0.0010  0.10    0.103   0.103  9.00      6431.0   0.24       -0.02

-- Monotonicity Check (IS) Horizon 10.000s --
FEATURE                MONO   B1(Sell) B2   B3  B4   B5(Buy)
Atoms_v1_f01_OFI       0.989  -0.3     -0.2 0.0 0.3  0.4
Atoms_v1_f02_TCI       0.946  -0.3     -0.2 0.1 0.3  0.3
Atoms_v1_f03_Whale     0.390  -0.0     0.1  0.1 0.0  0.0
Atoms_v1_f04_Lumpiness -0.989 0.4      0.3  0.0 -0.2 -0.3
Atoms_v1_f05_Sweep     0.998  -0.4     -0.1 0.0 0.3  0.4
Atoms_v1_f06_Fragility 0.956  -0.2     -0.2 0.1 0.3  0.3
Atoms_v1_f07_Magnet    -0.321 0.1      0.0  0.0 0.0  0.0
Atoms_v1_f08_Velocity  0.932  -0.3     -0.0 0.1 0.1  0.3
Atoms_v1_f09_Accel     0.099  0.0      0.0  0.1 0.0  0.0
Atoms_v1_f10_Gap       0.896  -0.1     -0.2 0.1 0.2  0.2
Atoms_v1_f11_DGT       0.979  -0.3     -0.2 0.0 0.3  0.4
Atoms_v1_f12_Absorb    0.697  -0.1     0.1  0.1 0.0  0.1
Atoms_v1_f13_Fractal   0.725  0.0      0.1  0.0 0.0  0.1

[study] ALL COMPLETE in 19.2215265s

[sys] Execution Time: 19.2215265s | Mem: 6722 MB
PS Z:\Benchmarks> 
PS Z:\Benchmarks> go run . bench 
go1.25.5 X:jsonv2,greenteagc | Env: windows/amd64 | Threads: 24 | RAM Limit: 24GB | GOGC: 200 | GOAMD64: v4
=== BENCHMARK: QuantDev STUDY (processStudyDay) ===
Go: go1.25.5 X:jsonv2,greenteagc | GOOS/GOARCH: windows/amd64 | Threads: 24
[bench] Sample symbol: BTCUSDT | day: 2022-12-17
[bench] Variants: [Atoms_v1]
[bench] Approx feature bytes/day: 883792
[bench] warm-up: 17.3649ms per study, selecting 28 iterations (fallback=false)
[bench] CPU profiling: ON
[bench] CPU profile written to bench_cpu.pprof

[bench] StudyDay
  iters:         28
  rows/iter:     16996
  features/iter: 13
  bytes/iter:    883792 (feature files)
  total time:    380.452ms
  ns/op:         13587571
  throughput:    1250.849 krows/s, 16.261 Mcells/s, 62.031 MB/s (features)
  allocs/op:     301 mallocs/op, 4907686 B/op
[bench] Heap profile written to bench_mem.pprof

[pprof-cpu] go tool pprof -top bench_cpu.pprof
File: benchmarks.exe
Build ID: C:\Users\dylan\AppData\Local\go-build\4b\4b7b85d33d33538529aedc6f870da8e2182b7e694899694979aacb8dcf226f3e-d\benchmarks.exe2025-12-03 17:45:54.1946538 -0600 CST
Type: cpu
Time: 2025-12-03 17:56:45 CST
Duration: 503.38ms, Total samples = 360ms (71.52%)
Showing nodes accounting for 360ms, 100% of 360ms total
      flat  flat%   sum%        cum   cum%
     270ms 75.00% 75.00%      270ms 75.00%  main.CalcMomentsVectors
      20ms  5.56% 80.56%       20ms  5.56%  main.ComputeQuantilesStrided.func1
      10ms  2.78% 83.33%       10ms  2.78%  main.computeReturns
      10ms  2.78% 86.11%       10ms  2.78%  main.inflateGNCToColumns
      10ms  2.78% 88.89%       10ms  2.78%  runtime.cgocall
      10ms  2.78% 91.67%       10ms  2.78%  runtime.memmove
      10ms  2.78% 94.44%       10ms  2.78%  runtime.stdcall1
      10ms  2.78% 97.22%       10ms  2.78%  slices.insertionSortCmpFunc[go.shape.struct { main.s float64; main.r float64 }] (inline)
      10ms  2.78%   100%       30ms  8.33%  slices.partitionCmpFunc[go.shape.struct { main.s float64; main.r float64 }]
         0     0%   100%       50ms 13.89%  main.ComputeQuantilesStrided
         0     0%   100%      350ms 97.22%  main.benchStudy
         0     0%   100%       10ms  2.78%  main.fastLoadBytes
         0     0%   100%       10ms  2.78%  main.loadDayColumns
         0     0%   100%      350ms 97.22%  main.main
         0     0%   100%      350ms 97.22%  main.processStudyDay
         0     0%   100%      350ms 97.22%  main.runBench
         0     0%   100%       10ms  2.78%  os.Open (inline)
         0     0%   100%       10ms  2.78%  os.OpenFile
         0     0%   100%       10ms  2.78%  os.openFileNolog
         0     0%   100%       10ms  2.78%  runtime.gopreempt_m (inline)
         0     0%   100%       10ms  2.78%  runtime.goschedImpl
         0     0%   100%       10ms  2.78%  runtime.growslice
         0     0%   100%      350ms 97.22%  runtime.main
         0     0%   100%       10ms  2.78%  runtime.morestack
         0     0%   100%       10ms  2.78%  runtime.newstack
         0     0%   100%       10ms  2.78%  runtime.notewakeup
         0     0%   100%       10ms  2.78%  runtime.semawakeup
         0     0%   100%       10ms  2.78%  runtime.startm
         0     0%   100%       10ms  2.78%  runtime.syscall_syscalln
         0     0%   100%       10ms  2.78%  runtime.wakep
         0     0%   100%       40ms 11.11%  slices.SortFunc[go.shape.[]main.pair·1,go.shape.struct { main.s float64; main.r float64 }] (inline)
         0     0%   100%       40ms 11.11%  slices.pdqsortCmpFunc[go.shape.struct { main.s float64; main.r float64 }]
         0     0%   100%       10ms  2.78%  syscall.Open
         0     0%   100%       10ms  2.78%  syscall.Syscall9
         0     0%   100%       10ms  2.78%  syscall.createFile


[pprof-heap] go tool pprof -top bench_mem.pprof
File: benchmarks.exe
Build ID: C:\Users\dylan\AppData\Local\go-build\4b\4b7b85d33d33538529aedc6f870da8e2182b7e694899694979aacb8dcf226f3e-d\benchmarks.exe2025-12-03 17:45:54.1946538 -0600 CST
Type: inuse_space
Time: 2025-12-03 17:56:46 CST
Showing nodes accounting for 15631.80kB, 100% of 15631.80kB total
      flat  flat%   sum%        cum   cum%
13066.80kB 83.59% 83.59% 13066.80kB 83.59%  main.inflateGNCToColumns
    2565kB 16.41%   100%     2565kB 16.41%  runtime.allocm
         0     0%   100% 13066.80kB 83.59%  main.benchStudy
         0     0%   100% 13066.80kB 83.59%  main.loadDayColumns
         0     0%   100% 13066.80kB 83.59%  main.main
         0     0%   100% 13066.80kB 83.59%  main.processStudyDay
         0     0%   100% 13066.80kB 83.59%  main.runBench
         0     0%   100% 13066.80kB 83.59%  runtime.main
         0     0%   100%     2052kB 13.13%  runtime.mcall
         0     0%   100%      513kB  3.28%  runtime.mstart
         0     0%   100%      513kB  3.28%  runtime.mstart0
         0     0%   100%      513kB  3.28%  runtime.mstart1
         0     0%   100%     2565kB 16.41%  runtime.newm
         0     0%   100%     2052kB 13.13%  runtime.park_m
         0     0%   100%     2565kB 16.41%  runtime.resetspinning
         0     0%   100%     2565kB 16.41%  runtime.schedule
         0     0%   100%     2565kB 16.41%  runtime.startm
         0     0%   100%     2565kB 16.41%  runtime.wakep

=== BENCHMARK COMPLETE ===

[sys] Execution Time: 997.5415ms | Mem: 14 MB
PS Z:\Benchmarks> 

--- File Tree Structure ---
|-- data/
|-- bench.go
|-- build.go
|-- common.go
|-- data.go
|-- main.go
|-- metrics.go
|-- sanity.go
|-- study.go
    |-- BTCUSDT/
    |-- features/
        |-- 2020/
        |-- 2021/
        |-- 2022/
        |-- 2023/
        |-- 2024/
        |-- 2025/
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
            |-- [01..12]/ (12 month dirs)
        |-- BTCUSDT/
            |-- Atoms_v1/

// --- File: bench.go ---

```go
package main

import (
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"runtime"
	"runtime/pprof"
	"time"
)

// benchStats holds per-benchmark summary for the study path.
type benchStats struct {
	Name            string
	Iters           int
	RowsPerIter     int // number of time steps (rows) per day
	FeatPerIter     int // number of feature series per day (variants * dims)
	BytesPerIter    int // approximate feature bytes read per day
	Total           time.Duration
	AllocBytesPerOp uint64
	MallocsPerOp    uint64
}

// runBench is called from main when you do: go run . bench
// It benchmarks the STUDY pipeline on a single day:
//
//	loadDayColumns + feature decode + returns + moments + quantiles.
func runBench() {
	fmt.Println("=== BENCHMARK: QuantDev STUDY (processStudyDay) ===")
	fmt.Printf("Go: %s | GOOS/GOARCH: %s/%s | Threads: %d\n",
		runtime.Version(),
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
	)

	sym, dayInt, variants, featRoot, ok := findStudySample()
	if !ok {
		fmt.Printf("[bench] no feature sets found under %q\n", filepath.Join(BaseDir, "features"))
		return
	}
	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100

	fmt.Printf("[bench] Sample symbol: %s | day: %04d-%02d-%02d\n", sym, y, m, d)
	fmt.Printf("[bench] Variants: %v\n", variants)

	featureBytes := featureBytesForDay(featRoot, variants, dayInt)
	if featureBytes > 0 {
		fmt.Printf("[bench] Approx feature bytes/day: %d\n", featureBytes)
	}

	// Quantiles are the expensive part; mimic real logic but keep worst-case feel.
	doQuantiles := dayInt < oosBoundaryYMD

	// --- Warm-up to decide iteration count ---
	warmStats := benchStudy(sym, dayInt, variants, featRoot, 1, doQuantiles)
	warm := warmStats.Total
	if warm <= 0 {
		// Clock weirdness / too fast — assume a tiny but non-zero duration.
		warm = 2 * time.Millisecond
	}
	target := 500 * time.Millisecond
	iters := int(target / warm)
	if iters < 3 {
		iters = 3
	} else if iters > 2000 {
		iters = 2000
	}

	fmt.Printf("[bench] warm-up: %s per study, selecting %d iterations (fallback=%v)\n",
		warmStats.Total, iters, warmStats.Total <= 0)

	// --- CPU profile + real benchmark ---
	var cpuFile *os.File
	var err error
	cpuFile, err = os.Create("bench_cpu.pprof")
	if err != nil {
		fmt.Printf("[bench] cannot create CPU profile: %v\n", err)
	} else {
		if err := pprof.StartCPUProfile(cpuFile); err != nil {
			fmt.Printf("[bench] cannot start CPU profile: %v\n", err)
			cpuFile.Close()
			cpuFile = nil
		} else {
			fmt.Println("[bench] CPU profiling: ON")
		}
	}

	stats := benchStudy(sym, dayInt, variants, featRoot, iters, doQuantiles)
	stats.BytesPerIter = featureBytes

	if cpuFile != nil {
		pprof.StopCPUProfile()
		cpuFile.Close()
		fmt.Println("[bench] CPU profile written to bench_cpu.pprof")
	}

	printBenchStats(stats)

	// --- Heap profile snapshot ---
	memFile, err := os.Create("bench_mem.pprof")
	if err != nil {
		fmt.Printf("[bench] cannot create heap profile: %v\n", err)
	} else {
		runtime.GC()
		if err := pprof.WriteHeapProfile(memFile); err != nil {
			fmt.Printf("[bench] cannot write heap profile: %v\n", err)
		} else {
			fmt.Println("[bench] Heap profile written to bench_mem.pprof")
		}
		memFile.Close()
	}

	// --- Inline pprof -top summaries (best-effort) ---
	runPprofTop("bench_cpu.pprof", "cpu")
	runPprofTop("bench_mem.pprof", "heap")

	fmt.Println("=== BENCHMARK COMPLETE ===")
}

// benchStudy repeatedly runs processStudyDay for one symbol/day
// and measures time + allocations. This hits:
//
//   - loadDayColumns (GNC decompress)
//   - computeReturns for each horizon
//   - feature decode for each variant/dim
//   - CalcMomentsVectors
//   - ComputeQuantilesStrided (if doQuantiles)
//
// i.e. the "mega compute" path.
func benchStudy(sym string, dayInt int, variants []string, featRoot string, iters int, doQuantiles bool) benchStats {
	stats := benchStats{
		Name:  "StudyDay",
		Iters: iters,
	}

	if iters <= 0 {
		return stats
	}

	// Prepare worker-like buffers (same pattern as runStudy workers).
	var sigBuf []float64
	var fileBuf []byte
	var retBuf []float64
	retsPerHBuf := make([][]float64, len(TimeHorizonsMS))
	var gncBuf []byte

	runtime.GC()

	var m0, m1 runtime.MemStats
	runtime.ReadMemStats(&m0)

	start := time.Now()
	for i := 0; i < iters; i++ {
		res := processStudyDay(
			sym, dayInt, variants, featRoot,
			&sigBuf, &fileBuf, &retBuf, &retsPerHBuf, &gncBuf,
			doQuantiles,
		)

		// On first iter, infer rows and feature count from Moments.
		if i == 0 {
			rows := 0
			for _, momsSlice := range res.Metrics {
				if len(momsSlice) > 0 {
					rows = int(momsSlice[0].Count)
					break
				}
			}
			stats.RowsPerIter = rows
			stats.FeatPerIter = len(res.Metrics)
		}
	}
	stats.Total = time.Since(start)

	runtime.ReadMemStats(&m1)
	allocBytes := m1.TotalAlloc - m0.TotalAlloc
	mallocs := m1.Mallocs - m0.Mallocs

	if iters > 0 {
		stats.AllocBytesPerOp = allocBytes / uint64(iters)
		stats.MallocsPerOp = mallocs / uint64(iters)
	}

	return stats
}

// printBenchStats pretty-prints the stats in a human-friendly way.
func printBenchStats(bs benchStats) {
	if bs.Iters <= 0 || bs.Total <= 0 {
		fmt.Printf("[bench] %s: no data\n", bs.Name)
		return
	}

	nsPerOp := float64(bs.Total.Nanoseconds()) / float64(bs.Iters)
	totalRows := float64(bs.RowsPerIter * bs.Iters)
	totalBytes := float64(bs.BytesPerIter * bs.Iters)
	totalCells := totalRows * float64(bs.FeatPerIter) // rows × features

	secs := bs.Total.Seconds()
	rowsPerSec := 0.0
	bytesPerSec := 0.0
	cellsPerSec := 0.0
	if secs > 0 {
		rowsPerSec = totalRows / secs
		bytesPerSec = totalBytes / secs
		cellsPerSec = totalCells / secs
	}

	fmt.Printf("\n[bench] %s\n", bs.Name)
	fmt.Printf("  iters:         %d\n", bs.Iters)
	fmt.Printf("  rows/iter:     %d\n", bs.RowsPerIter)
	fmt.Printf("  features/iter: %d\n", bs.FeatPerIter)
	if bs.BytesPerIter > 0 {
		fmt.Printf("  bytes/iter:    %d (feature files)\n", bs.BytesPerIter)
	}
	fmt.Printf("  total time:    %s\n", bs.Total)
	fmt.Printf("  ns/op:         %.0f\n", nsPerOp)

	fmt.Printf("  throughput:    %.3f krows/s", rowsPerSec/1e3)
	if bs.FeatPerIter > 0 {
		fmt.Printf(", %.3f Mcells/s", cellsPerSec/1e6)
	}
	if bs.BytesPerIter > 0 {
		fmt.Printf(", %.3f MB/s (features)\n", bytesPerSec/(1024*1024))
	} else {
		fmt.Println()
	}

	fmt.Printf("  allocs/op:     %d mallocs/op, %d B/op\n",
		bs.MallocsPerOp, bs.AllocBytesPerOp)
}

// runPprofTop runs "go tool pprof -top <profile>" and prints its output.
// Best-effort: if 'go' isn't on PATH or anything fails, it just logs and returns.
func runPprofTop(profilePath, kind string) {
	if _, err := os.Stat(profilePath); err != nil {
		// No profile file; nothing to do.
		return
	}

	cmd := exec.Command("go", "tool", "pprof", "-top", profilePath)
	out, err := cmd.CombinedOutput()
	if err != nil {
		fmt.Printf("[bench] go tool pprof -top (%s) failed: %v\n", kind, err)
		if len(out) > 0 {
			fmt.Printf("[pprof-%s]\n%s\n", kind, string(out))
		}
		return
	}

	fmt.Printf("\n[pprof-%s] go tool pprof -top %s\n", kind, profilePath)
	fmt.Println(string(out))
}

// findStudySample locates the first symbol/day that has feature files
// so we can benchmark a realistic study workload.
func findStudySample() (sym string, dayInt int, variants []string, featRoot string, ok bool) {
	syms := discoverFeatureSymbols()
	if syms == nil {
		return "", 0, nil, "", false
	}

	for s := range syms {
		featRoot = filepath.Join(BaseDir, "features", s)
		entries, err := os.ReadDir(featRoot)
		if err != nil {
			continue
		}

		var vs []string
		for _, e := range entries {
			if e.IsDir() && !isDotDir(e.Name()) {
				vs = append(vs, e.Name())
			}
		}
		if len(vs) == 0 {
			continue
		}

		// Use the first variant (same as runStudy) to discover days.
		baseVariantDir := filepath.Join(featRoot, vs[0])

		var days []int
		for d := range discoverStudyDays(baseVariantDir) {
			days = append(days, d)
		}
		if len(days) == 0 {
			continue
		}

		// Pick a mid-day (roughly typical load).
		dayInt = days[len(days)/2]
		return s, dayInt, vs, featRoot, true
	}
	return "", 0, nil, "", false
}

func isDotDir(name string) bool {
	return len(name) > 0 && name[0] == '.'
}

// featureBytesForDay sums the sizes of all variant feature files for this day.
// It's an approximation for "bytes/iter" to give a sense of memory bandwidth.
func featureBytesForDay(featRoot string, variants []string, dayInt int) int {
	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100
	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	total := 0
	for _, v := range variants {
		path := filepath.Join(featRoot, v, dStr+".bin")
		fi, err := os.Stat(path)
		if err != nil {
			continue
		}
		// Only count regular files.
		if fi.Mode().IsRegular() {
			if sz := fi.Size(); sz > 0 {
				total += int(sz)
			}
		}
	}
	return total
}
```

// --- End File: bench.go ---

// --- File: build.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"math"
	"os"
	"path/filepath"
	"strconv"
	"sync"
	"time"
)

// --- Atomic Configuration ---

type AtomConfig struct {
	WhaleThreshold float64 // K_95
}

var DefaultAtoms = AtomConfig{
	WhaleThreshold: 10.0,
}

const (
	EPS = 1e-9
)

type ofiTask struct {
	Y, M, D        int
	Offset, Length int64
}

// --- Execution Logic ---

func runBuild() {
	start := time.Now()

	// Pipeline: Symbols -> Build
	found := false
	for sym := range discoverSymbols() {
		found = true
		buildForSymbol(sym)
	}

	if !found {
		fmt.Printf("[build] no symbols discovered under %q\n", BaseDir)
	}
	fmt.Printf("[build] Complete in %s\n", time.Since(start))
}

func discoverSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		entries, err := os.ReadDir(BaseDir)
		if err != nil {
			fmt.Printf("[build] ReadDir(%s): %v\n", BaseDir, err)
			return
		}
		for _, e := range entries {
			if !e.IsDir() {
				continue
			}
			name := e.Name()
			if name == "features" || name == "common" || len(name) == 0 || name[0] == '.' {
				continue
			}
			if !yield(name) {
				return
			}
		}
	}
}

func buildForSymbol(sym string) {
	fmt.Printf(">>> Building %s (Canonical 2025 Atoms)\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)

	tasksCh := make(chan ofiTask, 1024)

	outDir := filepath.Join(featRoot, "Atoms_v1")
	if err := os.MkdirAll(outDir, 0755); err != nil {
		fmt.Printf("[build] MkdirAll(%s): %v\n", outDir, err)
		return
	}

	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			var binBuf []byte
			var gncBuf []byte
			for t := range tasksCh {
				processAtomDay(sym, t, outDir, DefaultAtoms, &binBuf, &gncBuf)
			}
		}()
	}

	count := 0
	for t := range discoverTasks(sym) {
		tasksCh <- t
		count++
	}
	close(tasksCh)

	if count == 0 {
		fmt.Printf("[build] no tasks for symbol %s\n", sym)
	}

	wg.Wait()
}

func discoverTasks(sym string) iter.Seq[ofiTask] {
	return func(yield func(ofiTask) bool) {
		root := filepath.Join(BaseDir, sym)
		years, err := os.ReadDir(root)
		if err != nil {
			return
		}

		for _, yDir := range years {
			if !yDir.IsDir() {
				continue
			}
			y, err := strconv.Atoi(yDir.Name())
			if err != nil || y <= 0 {
				continue
			}
			yearPath := filepath.Join(root, yDir.Name())
			months, err := os.ReadDir(yearPath)
			if err != nil {
				continue
			}
			for _, mDir := range months {
				if !mDir.IsDir() {
					continue
				}
				m, err := strconv.Atoi(mDir.Name())
				if err != nil || m < 1 || m > 12 {
					continue
				}

				idxPath := filepath.Join(yearPath, mDir.Name(), "index.quantdev")
				f, err := os.Open(idxPath)
				if err != nil {
					continue
				}

				var hdr [16]byte
				if _, err := io.ReadFull(f, hdr[:]); err != nil {
					f.Close()
					continue
				}
				if string(hdr[0:4]) != IdxMagic {
					f.Close()
					continue
				}
				count := binary.LittleEndian.Uint64(hdr[8:])
				var row [26]byte
				for i := uint64(0); i < count; i++ {
					if _, err := io.ReadFull(f, row[:]); err != nil {
						break
					}
					d := int(binary.LittleEndian.Uint16(row[0:]))
					offset := int64(binary.LittleEndian.Uint64(row[2:]))
					length := int64(binary.LittleEndian.Uint64(row[10:]))
					if length > 0 {
						task := ofiTask{
							Y: y, M: m, D: d,
							Offset: offset, Length: length,
						}
						if !yield(task) {
							f.Close()
							return
						}
					}
				}
				f.Close()
			}
		}
	}
}

func processAtomDay(sym string, t ofiTask, outDir string, cfg AtomConfig, binBuf, gncBuf *[]byte) {
	dateStr := fmt.Sprintf("%04d%02d%02d", t.Y, t.M, t.D)
	outPath := filepath.Join(outDir, dateStr+".bin")

	gncBlob, ok := loadRawGNC(sym, t, gncBuf)
	if !ok {
		return
	}

	colsAny := DayColumnPool.Get()
	cols := colsAny.(*DayColumns)
	cols.Reset()
	defer DayColumnPool.Put(cols)

	rowCount, ok := inflateGNCToColumns(gncBlob, cols)
	if !ok || rowCount < 2 {
		return
	}

	reqSize := rowCount * FeatRowBytes
	if cap(*binBuf) < reqSize {
		*binBuf = make([]byte, reqSize)
	}
	*binBuf = (*binBuf)[:reqSize]

	times := cols.Times
	qtys := cols.Qtys
	prices := cols.Prices
	sides := cols.Sides
	matches := cols.Matches

	writeVal := func(rowIdx, atomIdx int, val float64) {
		off := rowIdx*FeatRowBytes + atomIdx*4
		binary.LittleEndian.PutUint32((*binBuf)[off:], math.Float32bits(float32(val)))
	}

	prevP := prices[0]
	prevVel := 0.0

	for i := 0; i < rowCount; i++ {
		q := qtys[i]
		s := float64(sides[i])
		p := prices[i]

		m := 1.0
		if len(matches) > i {
			m = float64(matches[i])
		}

		dt := 0.0
		if i > 0 {
			dt = float64(times[i] - times[i-1])
		}
		dp := 0.0
		if i > 0 {
			dp = p - prevP
		}

		// 1. OFI
		writeVal(i, 0, q*s)
		// 2. TCI
		writeVal(i, 1, s)
		// 3. Whale
		val3 := 0.0
		if q > cfg.WhaleThreshold {
			val3 = q * s
		}
		writeVal(i, 2, val3)
		// 4. Lumpiness
		writeVal(i, 3, -(q*q)*s)
		// 5. Sweep
		writeVal(i, 4, m*s)
		// 6. Fragility
		val6 := 0.0
		if q > EPS {
			val6 = (m / q) * s
		}
		writeVal(i, 5, val6)
		// 7. Magnet
		rem := math.Mod(p, 0.5)
		if rem < 0 {
			rem += 0.5
		}
		dist := math.Min(rem, 0.5-rem)
		writeVal(i, 6, 1.0/(1.0+dist))
		// 8. Velocity
		vel := 0.0
		if dt > EPS {
			vel = q / dt
		}
		writeVal(i, 7, vel*s)
		// 9. Accel
		accel := vel - prevVel
		writeVal(i, 8, accel)
		// 10. Gap
		writeVal(i, 9, dt*s)
		// 11. DGT
		signDp := 0.0
		if dp > 0 {
			signDp = 1.0
		} else if dp < 0 {
			signDp = -1.0
		}
		val11 := 0.0
		if s == signDp {
			val11 = q * s
		}
		writeVal(i, 10, val11)
		// 12. Absorb
		val12 := 0.0
		if s != signDp {
			val12 = q * s
		}
		writeVal(i, 11, val12)
		// 13. Fractal
		val13 := 0.0
		if q > EPS {
			val13 = math.Abs(dp) / q
		}
		writeVal(i, 12, val13)

		prevP = p
		prevVel = vel
	}

	if err := os.WriteFile(outPath, *binBuf, 0644); err != nil {
		fmt.Printf("[build] WriteFile(%s): %v\n", outPath, err)
	}
}

func loadRawGNC(sym string, t ofiTask, buf *[]byte) ([]byte, bool) {
	path := filepath.Join(BaseDir,
		sym,
		fmt.Sprintf("%04d", t.Y),
		fmt.Sprintf("%02d", t.M),
		"data.quantdev",
	)
	f, err := os.Open(path)
	if err != nil {
		return nil, false
	}
	defer f.Close()

	if _, err := f.Seek(t.Offset, io.SeekStart); err != nil {
		return nil, false
	}

	if t.Length <= 0 || t.Length > 1<<31-1 {
		return nil, false
	}
	need := int(t.Length)
	if cap(*buf) < need {
		*buf = make([]byte, need)
	}
	b := (*buf)[:need]

	if _, err := io.ReadFull(f, b); err != nil {
		return nil, false
	}
	if len(b) < 4 || string(b[0:4]) != GNCMagic {
		return nil, false
	}
	return b, true
}
```

// --- End File: build.go ---

// --- File: common.go ---

```go
package main

import (
	"encoding/binary"
	"sync"
	"unique"
	"unsafe"
)

// --- Shared Configuration ---

const (
	// Ryzen 9 7900X: 12 Cores / 24 Threads.
	CPUThreads = 24
	BaseDir    = "data"

	// Binary Layout Constants (GNC-v2)
	PxScale = 100_000_000.0
	QtScale = 100_000_000.0

	// GNC Chunking
	GNCChunkSize = 65536

	// Magic Headers
	GNCMagic      = "GNC2"
	GNCHeaderSize = 32
	IdxMagic      = "QIDX"
	IdxVersion    = 1

	// Feature layout on disk (13 Canonical Atoms)
	FeatDims     = 13
	FeatBytes    = 4
	FeatRowBytes = FeatDims * FeatBytes
)

// Intern the symbol to keep it in L3 cache.
var SymbolHandle = unique.Make("BTCUSDT")

func Symbol() string { return SymbolHandle.Value() }

// --- OPTIMIZED DATA SCHEMA (SoA) ---

type DayColumns struct {
	Count   int
	Times   []int64   // epoch ms
	Prices  []float64 // scaled
	Qtys    []float64 // scaled
	Sides   []int8    // 1, -1
	Matches []uint16  // M_t
}

func (c *DayColumns) Reset() {
	c.Count = 0
	c.Times = c.Times[:0]
	c.Prices = c.Prices[:0]
	c.Qtys = c.Qtys[:0]
	c.Sides = c.Sides[:0]
	c.Matches = c.Matches[:0]
}

var DayColumnPool = sync.Pool{
	New: func() any {
		return &DayColumns{
			Times:   make([]int64, 0),
			Prices:  make([]float64, 0),
			Qtys:    make([]float64, 0),
			Sides:   make([]int8, 0),
			Matches: make([]uint16, 0),
		}
	},
}

// --- Shared GNC Decoder ---

func inflateGNCToColumns(rawBlob []byte, cols *DayColumns) (int, bool) {
	if len(rawBlob) < GNCHeaderSize {
		return 0, false
	}
	if string(rawBlob[0:4]) != GNCMagic {
		return 0, false
	}

	totalRows := int(binary.LittleEndian.Uint32(rawBlob[4:8]))
	if totalRows <= 0 {
		cols.Reset()
		return 0, true
	}

	// Grow vectors
	if cap(cols.Times) < totalRows {
		cols.Times = make([]int64, 0, totalRows)
	} else {
		cols.Times = cols.Times[:0]
	}
	if cap(cols.Prices) < totalRows {
		cols.Prices = make([]float64, 0, totalRows)
	} else {
		cols.Prices = cols.Prices[:0]
	}
	if cap(cols.Qtys) < totalRows {
		cols.Qtys = make([]float64, 0, totalRows)
	} else {
		cols.Qtys = cols.Qtys[:0]
	}
	if cap(cols.Sides) < totalRows {
		cols.Sides = make([]int8, 0, totalRows)
	} else {
		cols.Sides = cols.Sides[:0]
	}
	if cap(cols.Matches) < totalRows {
		cols.Matches = make([]uint16, 0, totalRows)
	} else {
		cols.Matches = cols.Matches[:0]
	}

	footerOffset := binary.LittleEndian.Uint64(rawBlob[24:32])
	if footerOffset >= uint64(len(rawBlob)) {
		return 0, false
	}

	dictBlob := rawBlob[footerOffset:]
	if len(dictBlob) < 4 {
		return 0, false
	}

	dictCount := binary.LittleEndian.Uint32(dictBlob[0:4])
	ptr := 4

	if uint64(ptr)+uint64(dictCount)*8+4 > uint64(len(dictBlob)) {
		return 0, false
	}

	qtyDict := make([]float64, dictCount)
	for i := 0; i < int(dictCount); i++ {
		qRaw := binary.LittleEndian.Uint64(dictBlob[ptr : ptr+8])
		qtyDict[i] = float64(qRaw) / QtScale
		ptr += 8
	}

	if len(dictBlob) < ptr+4 {
		return 0, false
	}
	chunkCount := binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
	ptr += 4

	if uint64(ptr)+uint64(chunkCount)*4 > uint64(len(dictBlob)) {
		return 0, false
	}

	chunkOffsets := make([]uint32, chunkCount)
	for i := 0; i < int(chunkCount); i++ {
		chunkOffsets[i] = binary.LittleEndian.Uint32(dictBlob[ptr : ptr+4])
		ptr += 4
	}

	for _, off := range chunkOffsets {
		if uint64(off)+18 > uint64(len(rawBlob)) {
			return 0, false
		}
		chunk := rawBlob[off:]
		n := int(binary.LittleEndian.Uint16(chunk[0:2]))
		baseT := int64(binary.LittleEndian.Uint64(chunk[2:10]))
		baseP := int64(binary.LittleEndian.Uint64(chunk[10:18]))

		// Offsets in chunk
		pTime := 18
		pPrice := pTime + n*4
		pQty := pPrice + n*8 // int64 price deltas
		pMatches := pQty + n*2
		pSide := pMatches + n*2

		// Backward compatibility: If side is out of bounds, assume no matches stream
		hasMatches := true
		if pSide > len(chunk) {
			pSideLegacy := pQty + n*2
			if pSideLegacy <= len(chunk) {
				hasMatches = false
				pSide = pSideLegacy
			} else {
				return 0, false
			}
		}

		tDeltas := unsafe.Slice((*int32)(unsafe.Pointer(&chunk[pTime])), n)
		pDeltas := unsafe.Slice((*int64)(unsafe.Pointer(&chunk[pPrice])), n)
		qIDs := unsafe.Slice((*uint16)(unsafe.Pointer(&chunk[pQty])), n)

		var ms []uint16
		if hasMatches {
			ms = unsafe.Slice((*uint16)(unsafe.Pointer(&chunk[pMatches])), n)
		}

		sideBits := chunk[pSide:]
		if len(sideBits) < (n+7)/8 {
			return 0, false
		}

		lastT := baseT
		lastP := baseP

		for i := 0; i < n; i++ {
			lastT += int64(tDeltas[i])
			lastP += pDeltas[i]

			cols.Times = append(cols.Times, lastT)
			cols.Prices = append(cols.Prices, float64(lastP)/PxScale)

			qID := int(qIDs[i])
			if qID >= 0 && qID < len(qtyDict) {
				cols.Qtys = append(cols.Qtys, qtyDict[qID])
			} else {
				cols.Qtys = append(cols.Qtys, 0)
			}

			if hasMatches {
				cols.Matches = append(cols.Matches, ms[i])
			} else {
				cols.Matches = append(cols.Matches, 1)
			}

			bitByte := sideBits[i/8]
			isBuy := (bitByte & (1 << (i % 8))) != 0
			side := int8(-1)
			if isBuy {
				side = 1
			}
			cols.Sides = append(cols.Sides, side)
		}
	}

	cols.Count = len(cols.Prices)
	return cols.Count, true
}
```

// --- End File: common.go ---

// --- File: data.go ---

```go
package main

import (
	"archive/zip"
	"bufio"
	"bytes"
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"path/filepath"
	"strings"
	"sync"
	"sync/atomic"
	"time"
	"unsafe"
)

const (
	HostData   = "data.binance.vision"
	S3Prefix   = "data/futures/um"
	DataSet    = "aggTrades"
	FallbackDt = "2020-01-01"
)

var (
	httpClient *http.Client
	stopEvent  atomic.Bool
	dirLocks   sync.Map
)

var errNotFound = fmt.Errorf("404")

// --- Ingestion Memory Pool ---

type IngestBuffers struct {
	Ts       []int64
	Ps       []int64
	Qs       []uint64
	Ms       []uint16
	Buys     []bool
	TDeltas  []int32
	PDeltas  []int64
	QIDs     []uint16
	SideBits []byte
}

var ingestBufferPool = sync.Pool{
	New: func() any {
		const cap = 1_000_000
		return &IngestBuffers{
			Ts:       make([]int64, 0, cap),
			Ps:       make([]int64, 0, cap),
			Qs:       make([]uint64, 0, cap),
			Ms:       make([]uint16, 0, cap),
			Buys:     make([]bool, 0, cap),
			TDeltas:  make([]int32, GNCChunkSize),
			PDeltas:  make([]int64, GNCChunkSize),
			QIDs:     make([]uint16, GNCChunkSize),
			SideBits: make([]byte, (GNCChunkSize+7)/8),
		}
	},
}

func (b *IngestBuffers) Reset() {
	b.Ts = b.Ts[:0]
	b.Ps = b.Ps[:0]
	b.Qs = b.Qs[:0]
	b.Ms = b.Ms[:0]
	b.Buys = b.Buys[:0]
}

func init() {
	tr := &http.Transport{
		MaxIdleConns:        100,
		MaxIdleConnsPerHost: 100,
		IdleConnTimeout:     90 * time.Second,
	}
	httpClient = &http.Client{
		Transport: tr,
		Timeout:   30 * time.Second,
	}
}

func runData() {
	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, os.Interrupt)
	go func() {
		<-sigChan
		stopEvent.Store(true)
		fmt.Println("\n[warn] Stopping gracefully...")
	}()

	fmt.Printf("--- GNC-v2 Ingestion (Streaming) | Symbol: %s ---\n", Symbol())

	start, err := time.Parse("2006-01-02", FallbackDt)
	if err != nil {
		fmt.Printf("[fatal] invalid FallbackDt: %v\n", err)
		return
	}

	end := time.Now().UTC().AddDate(0, 0, -1)
	var days []time.Time
	for d := start; !d.After(end); d = d.AddDate(0, 0, 1) {
		days = append(days, d)
	}

	fmt.Printf("[job] Processing %d days using %d threads.\n", len(days), CPUThreads)

	jobs := make(chan time.Time, len(days))
	results := make(chan string, len(days))
	var wg sync.WaitGroup

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for d := range jobs {
				if stopEvent.Load() {
					return
				}
				results <- processDay(d)
			}
		}()
	}

	for _, d := range days {
		jobs <- d
	}
	close(jobs)
	wg.Wait()
	close(results)

	stats := make(map[string]int)
	for r := range results {
		key := strings.SplitN(r, " ", 2)[0]
		stats[key]++
	}
	fmt.Printf("\n[done] %v\n", stats)
}

func processDay(d time.Time) string {
	y, m, day := d.Year(), int(d.Month()), d.Day()

	dirPath := filepath.Join(BaseDir, Symbol(), fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	muAny, _ := dirLocks.LoadOrStore(dirPath, &sync.Mutex{})
	mu := muAny.(*sync.Mutex)

	mu.Lock()
	indexed := isIndexed(idxPath, day)
	mu.Unlock()

	if indexed {
		return "skip"
	}

	sym := Symbol()
	url := fmt.Sprintf("https://%s/%s/daily/%s/%s/%s-%s-%04d-%02d-%02d.zip",
		HostData, S3Prefix, DataSet, sym, sym, DataSet, y, m, day)

	// Download ZIP
	zipBytes, err := download(url)
	if err != nil {
		if err == errNotFound {
			return "missing"
		}
		return "error_dl"
	}

	// Stream Parse
	bufs := ingestBufferPool.Get().(*IngestBuffers)
	bufs.Reset()
	defer ingestBufferPool.Put(bufs)

	gncBlob, count, err := streamZipToGNCBlob(zipBytes, bufs)
	if err != nil {
		return "error_parse"
	}
	if count == 0 {
		return "empty"
	}

	sum := sha256.Sum256(gncBlob)
	cSum := binary.LittleEndian.Uint64(sum[:8])

	mu.Lock()
	defer mu.Unlock()

	if isIndexed(idxPath, day) {
		return "skip_race"
	}

	if err := os.MkdirAll(dirPath, 0755); err != nil {
		return "error_mkdir"
	}

	fData, err := os.OpenFile(dataPath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
	if err != nil {
		return "error_io"
	}

	stat, err := fData.Stat()
	if err != nil {
		fData.Close()
		return "error_stat"
	}
	offset := stat.Size()

	if _, err := fData.Write(gncBlob); err != nil {
		fData.Close()
		return "error_write"
	}
	fData.Close()

	if err := updateIndex(idxPath, day, offset, len(gncBlob), cSum); err != nil {
		return "error_idx"
	}

	return "ok"
}

// --- Streaming ZIP -> GNC Pipeline ---

func streamZipToGNCBlob(zipData []byte, bufs *IngestBuffers) ([]byte, uint64, error) {
	r, err := zip.NewReader(bytes.NewReader(zipData), int64(len(zipData)))
	if err != nil {
		return nil, 0, err
	}

	for _, f := range r.File {
		if !strings.HasSuffix(f.Name, ".csv") {
			continue
		}

		rc, err := f.Open()
		if err != nil {
			continue
		}

		// FIX: No defer inside loop. Explicit close after usage.
		count, err := scanCSVToBuffers(rc, bufs)
		rc.Close()

		if err != nil {
			return nil, 0, err
		}

		return encodeGNC(bufs, count)
	}
	return nil, 0, fmt.Errorf("no csv found")
}

func scanCSVToBuffers(r io.Reader, bufs *IngestBuffers) (int, error) {
	scanner := bufio.NewScanner(r)
	buf := make([]byte, 0, 64*1024)
	scanner.Buffer(buf, 1024*1024)

	var price int64
	var ts int64
	var qty uint64
	var firstId, lastId int64
	var isBuyerMaker bool

	count := 0
	firstLine := true

	for scanner.Scan() {
		line := scanner.Bytes()
		if len(line) == 0 {
			continue
		}

		if firstLine {
			firstLine = false
			if line[0] < '0' || line[0] > '9' {
				continue
			}
		}

		col := 0
		start := 0

		price = 0
		qty = 0
		ts = 0
		firstId = 0
		lastId = 0
		isBuyerMaker = false

		for i := 0; i <= len(line); i++ {
			var c byte
			if i < len(line) {
				c = line[i]
			} else {
				c = ','
			}

			if c == ',' {
				field := line[start:i]
				switch col {
				case 1:
					price = fastParseFixed(field)
				case 2:
					qty = uint64(fastParseFixed(field))
				case 3:
					firstId = fastParseInt(field)
				case 4:
					lastId = fastParseInt(field)
				case 5:
					ts = fastParseInt(field)
				case 6:
					if len(field) > 0 && (field[0] == 't' || field[0] == 'T') {
						isBuyerMaker = true
					}
				}
				col++
				start = i + 1
			}
		}

		bufs.Ps = append(bufs.Ps, price)
		bufs.Qs = append(bufs.Qs, qty)
		bufs.Ts = append(bufs.Ts, ts)
		bufs.Buys = append(bufs.Buys, !isBuyerMaker)

		matches := int64(1)
		if lastId >= firstId {
			matches = lastId - firstId + 1
		}
		if matches > 65535 {
			matches = 65535
		}
		bufs.Ms = append(bufs.Ms, uint16(matches))

		count++
	}

	return count, scanner.Err()
}

// encodeGNC and encodeChunk ... (Standard GNC logic)
func encodeGNC(bufs *IngestBuffers, count int) ([]byte, uint64, error) {
	if count == 0 {
		return nil, 0, nil
	}

	var buf bytes.Buffer
	buf.Grow(count * 20)

	baseTime := bufs.Ts[0]
	basePrice := bufs.Ps[0]

	buf.WriteString(GNCMagic)

	var scratch [8]byte
	binary.LittleEndian.PutUint32(scratch[:4], uint32(count))
	buf.Write(scratch[:4])

	binary.LittleEndian.PutUint64(scratch[:], uint64(baseTime))
	buf.Write(scratch[:])

	binary.LittleEndian.PutUint64(scratch[:], uint64(basePrice))
	buf.Write(scratch[:])

	footerOffsetPos := buf.Len()
	binary.LittleEndian.PutUint64(scratch[:], 0)
	buf.Write(scratch[:])

	qtyDict := make(map[uint64]uint16, 1000)
	var dictLog []uint64
	chunkOffsets := make([]uint32, 0)

	for i := 0; i < count; i += GNCChunkSize {
		end := i + GNCChunkSize
		if end > count {
			end = count
		}
		chunkOffsets = append(chunkOffsets, uint32(buf.Len()))
		if err := encodeChunk(&buf, bufs, i, end, qtyDict, &dictLog); err != nil {
			return nil, 0, err
		}
	}

	footerStart := buf.Len()

	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(dictLog)))
	buf.Write(scratch[:4])
	for _, q := range dictLog {
		binary.LittleEndian.PutUint64(scratch[:], q)
		buf.Write(scratch[:])
	}

	binary.LittleEndian.PutUint32(scratch[:4], uint32(len(chunkOffsets)))
	buf.Write(scratch[:4])
	for _, off := range chunkOffsets {
		binary.LittleEndian.PutUint32(scratch[:4], off)
		buf.Write(scratch[:4])
	}

	finalBytes := buf.Bytes()
	binary.LittleEndian.PutUint64(finalBytes[footerOffsetPos:], uint64(footerStart))

	return finalBytes, uint64(count), nil
}

func encodeChunk(w *bytes.Buffer, bufs *IngestBuffers, start, end int, dict map[uint64]uint16, log *[]uint64) error {
	count := end - start
	ts := bufs.Ts[start:end]
	ps := bufs.Ps[start:end]
	qs := bufs.Qs[start:end]
	ms := bufs.Ms[start:end]
	buys := bufs.Buys[start:end]

	tDeltas := bufs.TDeltas[:count]
	pDeltas := bufs.PDeltas[:count]
	qIDs := bufs.QIDs[:count]
	sideBits := bufs.SideBits[:(count+7)/8]

	for k := range sideBits {
		sideBits[k] = 0
	}

	chunkBaseT := ts[0]
	chunkBaseP := ps[0]
	var lastT, lastP int64 = chunkBaseT, chunkBaseP

	tDeltas[0] = 0
	pDeltas[0] = 0

	id, ok := dict[qs[0]]
	if !ok {
		if len(*log) >= 65535 {
			return fmt.Errorf("qty dict overflow")
		}
		id = uint16(len(*log))
		dict[qs[0]] = id
		*log = append(*log, qs[0])
	}
	qIDs[0] = id
	if buys[0] {
		sideBits[0] |= 1
	}

	for i := 1; i < count; i++ {
		dt := ts[i] - lastT
		dp := ps[i] - lastP

		if dt > 2147483647 || dt < -2147483648 {
			return fmt.Errorf("time delta overflow")
		}

		tDeltas[i] = int32(dt)
		lastT = ts[i]
		pDeltas[i] = dp
		lastP = ps[i]

		id, ok := dict[qs[i]]
		if !ok {
			if len(*log) >= 65535 {
				return fmt.Errorf("qty dict overflow")
			}
			id = uint16(len(*log))
			dict[qs[i]] = id
			*log = append(*log, qs[i])
		}
		qIDs[i] = id

		if buys[i] {
			sideBits[i/8] |= (1 << (i % 8))
		}
	}

	var head [18]byte
	binary.LittleEndian.PutUint16(head[0:], uint16(count))
	binary.LittleEndian.PutUint64(head[2:], uint64(chunkBaseT))
	binary.LittleEndian.PutUint64(head[10:], uint64(chunkBaseP))
	w.Write(head[:])

	w.Write(unsafeBytes(tDeltas))
	w.Write(unsafeBytes(pDeltas))
	w.Write(unsafeBytes(qIDs))
	w.Write(unsafeBytes(ms))
	w.Write(sideBits)

	return nil
}

func fastParseFixed(b []byte) int64 {
	var num int64
	var seenDot bool
	var dec int
	for _, c := range b {
		if c == '.' {
			seenDot = true
			continue
		}
		if c < '0' || c > '9' {
			continue
		}
		num = num*10 + int64(c-'0')
		if seenDot {
			dec++
		}
	}
	for dec < 8 {
		num *= 10
		dec++
	}
	return num
}

func fastParseInt(b []byte) int64 {
	var n int64
	for _, c := range b {
		if c < '0' || c > '9' {
			continue
		}
		n = n*10 + int64(c-'0')
	}
	return n
}

func unsafeBytes[T any](s []T) []byte {
	return unsafe.Slice((*byte)(unsafe.Pointer(unsafe.SliceData(s))), len(s)*int(unsafe.Sizeof(s[0])))
}

func download(url string) ([]byte, error) {
	var lastErr error
	for attempt := 0; attempt < 3; attempt++ {
		resp, err := httpClient.Get(url)
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		if resp.StatusCode == 404 {
			resp.Body.Close()
			return nil, errNotFound
		}
		if resp.StatusCode != 200 {
			resp.Body.Close()
			lastErr = fmt.Errorf("status %d", resp.StatusCode)
			time.Sleep(100 * time.Millisecond)
			continue
		}
		data, err := io.ReadAll(resp.Body)
		resp.Body.Close()
		if err != nil {
			lastErr = err
			time.Sleep(100 * time.Millisecond)
			continue
		}
		return data, nil
	}
	if lastErr == nil {
		lastErr = fmt.Errorf("unknown dl error")
	}
	return nil, lastErr
}

func isIndexed(idxPath string, day int) bool {
	f, err := os.Open(idxPath)
	if err != nil {
		return false
	}
	defer f.Close()
	return checkIndex(f, day)
}

func checkIndex(f *os.File, day int) bool {
	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return false
	}
	if string(hdr[0:4]) != IdxMagic {
		return false
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			return false
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return true
		}
	}
	return false
}

func updateIndex(idxPath string, day int, offset int64, length int, csum uint64) error {
	f, err := os.OpenFile(idxPath, os.O_CREATE|os.O_RDWR, 0644)
	if err != nil {
		return err
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return err
	}
	if stat.Size() == 0 {
		var hdr [16]byte
		copy(hdr[0:], IdxMagic)
		binary.LittleEndian.PutUint32(hdr[4:], uint32(IdxVersion))
		if _, err := f.Write(hdr[:]); err != nil {
			return err
		}
	}

	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	var count uint64
	if err := binary.Read(f, binary.LittleEndian, &count); err != nil {
		return err
	}

	if _, err := f.Seek(0, io.SeekEnd); err != nil {
		return err
	}
	var row [26]byte
	binary.LittleEndian.PutUint16(row[0:], uint16(day))
	binary.LittleEndian.PutUint64(row[2:], uint64(offset))
	binary.LittleEndian.PutUint64(row[10:], uint64(length))
	binary.LittleEndian.PutUint64(row[18:], csum)
	if _, err := f.Write(row[:]); err != nil {
		return err
	}

	if _, err := f.Seek(8, io.SeekStart); err != nil {
		return err
	}
	return binary.Write(f, binary.LittleEndian, count+1)
}
```

// --- End File: data.go ---

// --- File: main.go ---

```go
package main

import (
	"fmt"
	"os"
	"runtime"
	"runtime/debug"
	"time"
)

func main() {
	// Use all 7900X hardware threads.
	runtime.GOMAXPROCS(CPUThreads)

	// Hard memory limit: 24GB.
	const ramLimit = 24 * 1024 * 1024 * 1024
	debug.SetMemoryLimit(ramLimit)

	if len(os.Args) < 2 {
		printHelp()
		os.Exit(1)
	}

	start := time.Now()

	fmt.Printf("%s | Env: %s/%s | Threads: %d | RAM Limit: 24GB | GOGC: %s | GOAMD64: %s\n",
		runtime.Version(),
		runtime.GOOS, runtime.GOARCH,
		runtime.GOMAXPROCS(0),
		os.Getenv("GOGC"),
		os.Getenv("GOAMD64"),
	)

	cmd := os.Args[1]

	switch cmd {
	case "data":
		runData()
	case "build":
		runBuild()
	case "study":
		runStudy()
	case "sanity":
		runSanity()
	case "bench":
		runBench()
	default:
		fmt.Printf("Unknown command: %s\n", cmd)
		printHelp()
		os.Exit(1)
	}

	fmt.Printf("\n[sys] Execution Time: %s | Mem: %s\n", time.Since(start), getMemUsage())
}

func printHelp() {
	fmt.Println("Usage: quant.exe [command]")
	fmt.Println("  data   - Download raw aggTrades")
	fmt.Println("  build  - Run TFI Primitives (RWVI, VAI, etc) -> features")
	fmt.Println("  study  - Run IS/OOS backtest on features")
	fmt.Println("  sanity - Check data integrity")
	fmt.Println("  bench  - Run decode benchmark + inline pprof summaries")
}

func getMemUsage() string {
	var m runtime.MemStats
	runtime.ReadMemStats(&m)
	return fmt.Sprintf("%d MB", m.Alloc/1024/1024)
}
```

// --- End File: main.go ---

// --- File: metrics.go ---

```go
package main

import (
	"math"
	"slices"
)

type MetricStats struct {
	Count        int
	ICPearson    float64
	IC_TStat     float64
	Sharpe       float64
	HitRate      float64
	BreakevenBps float64
	AutoCorr     float64
	AutoCorrAbs  float64
	AvgSegLen    float64
	MaxSegLen    float64
}

type Moments struct {
	Count          float64
	SumSig         float64
	SumRet         float64
	SumProd        float64
	SumSqSig       float64
	SumSqRet       float64
	SumPnL         float64
	SumSqPnL       float64
	Hits           float64
	ValidHits      float64
	SumAbsDeltaSig float64
	SumProdLag     float64
	SumAbsSig      float64
	SumAbsProdLag  float64
	SegCount       float64
	SegLenTotal    float64
	SegLenMax      float64
}

func (m *Moments) Add(m2 Moments) {
	m.Count += m2.Count
	m.SumSig += m2.SumSig
	m.SumRet += m2.SumRet
	m.SumProd += m2.SumProd
	m.SumSqSig += m2.SumSqSig
	m.SumSqRet += m2.SumSqRet
	m.SumPnL += m2.SumPnL
	m.SumSqPnL += m2.SumSqPnL
	m.Hits += m2.Hits
	m.ValidHits += m2.ValidHits
	m.SumAbsDeltaSig += m2.SumAbsDeltaSig
	m.SumProdLag += m2.SumProdLag
	m.SumAbsSig += m2.SumAbsSig
	m.SumAbsProdLag += m2.SumAbsProdLag
	m.SegCount += m2.SegCount
	m.SegLenTotal += m2.SegLenTotal
	if m2.SegLenMax > m.SegLenMax {
		m.SegLenMax = m2.SegLenMax
	}
}

func CalcMomentsVectors(sigs, rets []float64) Moments {
	var m Moments
	n := len(sigs)
	if n == 0 {
		return m
	}
	var prevSig float64
	var prevSign float64
	var curSegLen float64

	for i := 0; i < n; i++ {
		s := sigs[i]
		r := rets[i]

		m.Count++
		m.SumSig += s
		m.SumRet += r
		m.SumSqSig += s * s
		m.SumSqRet += r * r
		m.SumProd += s * r

		pnl := s * r
		m.SumPnL += pnl
		m.SumSqPnL += pnl * pnl

		absS := s
		if absS < 0 {
			absS = -absS
		}
		m.SumAbsSig += absS

		if s != 0 && r != 0 {
			m.ValidHits++
			if (s > 0 && r > 0) || (s < 0 && r < 0) {
				m.Hits++
			}
		}

		if i > 0 {
			d := s - prevSig
			if d < 0 {
				d = -d
			}
			m.SumAbsDeltaSig += d
			m.SumProdLag += s * prevSig
			absPrev := prevSig
			if absPrev < 0 {
				absPrev = -absPrev
			}
			m.SumAbsProdLag += absS * absPrev
		}

		sign := 0.0
		if s > 0 {
			sign = 1.0
		} else if s < 0 {
			sign = -1.0
		}

		if sign != 0 {
			if prevSign == sign {
				curSegLen++
			} else {
				if curSegLen > 0 {
					m.SegCount++
					m.SegLenTotal += curSegLen
					if curSegLen > m.SegLenMax {
						m.SegLenMax = curSegLen
					}
				}
				curSegLen = 1
			}
		} else {
			if curSegLen > 0 {
				m.SegCount++
				m.SegLenTotal += curSegLen
				if curSegLen > m.SegLenMax {
					m.SegLenMax = curSegLen
				}
				curSegLen = 0
			}
		}
		prevSig = s
		prevSign = sign
	}

	if curSegLen > 0 {
		m.SegCount++
		m.SegLenTotal += curSegLen
		if curSegLen > m.SegLenMax {
			m.SegLenMax = curSegLen
		}
	}
	return m
}

func FinalizeMetrics(m Moments, dailyICs []float64) MetricStats {
	if m.Count <= 1 {
		return MetricStats{Count: int(m.Count)}
	}
	ms := MetricStats{Count: int(m.Count)}

	num := m.Count*m.SumProd - m.SumSig*m.SumRet
	denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
	denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
	if denX > 0 && denY > 0 {
		ms.ICPearson = num / math.Sqrt(denX*denY)
	}

	meanPnL := m.SumPnL / m.Count
	varPnL := (m.SumSqPnL / m.Count) - meanPnL*meanPnL
	if varPnL > 1e-18 {
		ms.Sharpe = meanPnL / math.Sqrt(varPnL)
	}

	if m.ValidHits > 0 {
		ms.HitRate = m.Hits / m.ValidHits
	}
	if m.SumAbsDeltaSig > 1e-18 {
		ms.BreakevenBps = (m.SumPnL / m.SumAbsDeltaSig) * 10000.0
	}

	meanSig := m.SumSig / m.Count
	covLag := (m.SumProdLag / m.Count) - meanSig*meanSig
	varSig := (m.SumSqSig / m.Count) - meanSig*meanSig
	if varSig > 1e-18 {
		ms.AutoCorr = covLag / varSig
	}

	if m.Count > 0 {
		meanAbs := m.SumAbsSig / m.Count
		covAbs := (m.SumAbsProdLag / m.Count) - meanAbs*meanAbs
		varAbs := (m.SumSqSig / m.Count) - meanAbs*meanAbs
		if varAbs > 1e-18 {
			ms.AutoCorrAbs = covAbs / varAbs
		}
	}

	if m.SegCount > 0 {
		ms.AvgSegLen = m.SegLenTotal / m.SegCount
	}
	ms.MaxSegLen = m.SegLenMax

	if len(dailyICs) > 1 {
		var sum, sumSq float64
		n := float64(len(dailyICs))
		for _, v := range dailyICs {
			sum += v
			sumSq += v * v
		}
		mean := sum / n
		variance := (sumSq / n) - mean*mean
		if variance > 1e-18 {
			stdDev := math.Sqrt(variance)
			ms.IC_TStat = mean / (stdDev / math.Sqrt(n))
		}
	}
	return ms
}

type BucketResult struct {
	ID        int
	AvgSig    float64
	AvgRetBps float64
	Count     int
}

func ComputeQuantilesStrided(sigs, rets []float64, numBuckets, stride int) []BucketResult {
	n := len(sigs)
	if n == 0 || numBuckets <= 0 {
		return nil
	}

	estSize := n / stride
	type pair struct{ s, r float64 }
	pairs := make([]pair, 0, estSize)

	for i := 0; i < n; i += stride {
		pairs = append(pairs, pair{s: sigs[i], r: rets[i]})
	}

	if len(pairs) == 0 {
		return nil
	}

	slices.SortFunc(pairs, func(a, b pair) int {
		if a.s < b.s {
			return -1
		}
		if a.s > b.s {
			return 1
		}
		return 0
	})

	subN := len(pairs)
	results := make([]BucketResult, numBuckets)
	bucketSize := subN / numBuckets
	if bucketSize == 0 {
		bucketSize = 1
	}

	for b := 0; b < numBuckets; b++ {
		start := b * bucketSize
		end := start + bucketSize
		if b == numBuckets-1 || end > subN {
			end = subN
		}

		var sumS, sumR float64
		count := 0
		for i := start; i < end; i++ {
			sumS += pairs[i].s
			sumR += pairs[i].r
			count++
		}
		if count > 0 {
			results[b] = BucketResult{
				ID:        b + 1,
				AvgSig:    sumS / float64(count),
				AvgRetBps: (sumR / float64(count)) * 10000.0,
				Count:     count * stride,
			}
		}
	}
	return results
}

type BucketAgg struct {
	Count     int
	SumSig    float64
	SumRetBps float64
}

func (ba *BucketAgg) Add(br BucketResult) {
	if br.Count <= 0 {
		return
	}
	ba.Count += br.Count
	ba.SumSig += br.AvgSig * float64(br.Count)
	ba.SumRetBps += br.AvgRetBps * float64(br.Count)
}

func (ba BucketAgg) Finalize(id int) BucketResult {
	if ba.Count == 0 {
		return BucketResult{ID: id}
	}
	den := float64(ba.Count)
	return BucketResult{
		ID:        id,
		AvgSig:    ba.SumSig / den,
		AvgRetBps: ba.SumRetBps / den,
		Count:     ba.Count,
	}
}
```

// --- End File: metrics.go ---

// --- File: sanity.go ---

```go
package main

import (
	"crypto/sha256"
	"encoding/binary"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sort"
	"sync"
	"text/tabwriter"
	"time"
)

// Global Report Accumulator
type SanityReport struct {
	Mu           sync.Mutex
	TotalMonths  int
	TotalDays    int
	TotalTrades  int64
	TotalBytes   int64
	CorruptFiles int
	MissingDays  []string // List of "YYYY-MM-DD" gaps
	Errors       []string
}

var report SanityReport

func runSanity() {
	start := time.Now()
	root := filepath.Join(BaseDir, Symbol())
	dirs, err := os.ReadDir(root)
	if err != nil {
		fmt.Printf("[sanity] ReadDir(%s): %v\n", root, err)
		return
	}

	var tasks []string
	// Discover all Month directories
	for _, y := range dirs {
		if !y.IsDir() {
			continue
		}
		yearPath := filepath.Join(root, y.Name())
		months, err := os.ReadDir(yearPath)
		if err != nil {
			fmt.Printf("[sanity] ReadDir(%s): %v\n", yearPath, err)
			continue
		}
		for _, m := range months {
			if m.IsDir() {
				tasks = append(tasks, filepath.Join(root, y.Name(), m.Name()))
			}
		}
	}

	fmt.Printf("--- SANITY CHECK: %s | %d Months Found ---\n", Symbol(), len(tasks))

	// Reset Report
	report = SanityReport{MissingDays: make([]string, 0), Errors: make([]string, 0)}
	report.TotalMonths = len(tasks)

	var wg sync.WaitGroup
	jobs := make(chan string, len(tasks))

	// Workers
	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for path := range jobs {
				validateMonth(path)
			}
		}()
	}

	for _, t := range tasks {
		jobs <- t
	}
	close(jobs)
	wg.Wait()

	printSummary(time.Since(start))
}

func validateMonth(dirPath string) {
	// Parse Year/Month from path for gap detection
	// Path ends in .../YYYY/MM
	_, mStr := filepath.Split(dirPath)
	yStr := filepath.Base(filepath.Dir(dirPath))
	year := fastAtoi(yStr)
	month := fastAtoi(mStr)

	idxPath := filepath.Join(dirPath, "index.quantdev")
	dataPath := filepath.Join(dirPath, "data.quantdev")

	// Local accumulators to minimize locking
	var lTrades int64
	var lBytes int64
	var lDays int
	var lCorrupt int
	lErrors := make([]string, 0)
	presentDays := make(map[int]bool)

	// 1. Check Files Exist
	fIdx, err := os.Open(idxPath)
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("MISSING IDX: %s", dirPath))
		report.Mu.Unlock()
		return
	}
	defer fIdx.Close()

	fData, err := os.Open(dataPath)
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("MISSING DATA: %s", dirPath))
		report.Mu.Unlock()
		return
	}
	defer fData.Close()

	dstat, err := fData.Stat()
	if err != nil {
		report.Mu.Lock()
		report.Errors = append(report.Errors, fmt.Sprintf("STAT FAIL: %s (%v)", dataPath, err))
		report.Mu.Unlock()
		return
	}

	// 2. Validate Index Header
	var hdr [16]byte
	if _, err := io.ReadFull(fIdx, hdr[:]); err != nil {
		lErrors = append(lErrors, fmt.Sprintf("BAD IDX HDR: %s", dirPath))
		mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, nil)
		return
	}
	if string(hdr[:4]) != IdxMagic {
		lErrors = append(lErrors, fmt.Sprintf("BAD MAGIC: %s", dirPath))
		mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, nil)
		return
	}

	count := binary.LittleEndian.Uint64(hdr[8:])

	// 3. Iterate Days
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(fIdx, row[:]); err != nil {
			lErrors = append(lErrors, fmt.Sprintf("IDX TRUNCATED: %s", dirPath))
			break
		}

		day := int(binary.LittleEndian.Uint16(row[0:]))
		offset := int64(binary.LittleEndian.Uint64(row[2:]))
		length := int64(binary.LittleEndian.Uint64(row[10:]))
		expSum := binary.LittleEndian.Uint64(row[18:])

		presentDays[day] = true
		lDays++
		lBytes += length

		// Validate Data Blob
		if length < 32 {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Corrupt Blob (Len<32): %s Day %d", dirPath, day))
			continue
		}

		if offset < 0 || length < 0 || offset+length > dstat.Size() {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Blob exceeds file size: %s Day %d", dirPath, day))
			continue
		}

		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			lCorrupt++
			continue
		}

		// Read Header only first to check magic/count
		var blobHeader [32]byte
		if _, err := io.ReadFull(fData, blobHeader[:]); err != nil {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Read Fail: %s Day %d", dirPath, day))
			continue
		}

		if string(blobHeader[0:4]) != GNCMagic {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Bad GNC Magic: %s Day %d", dirPath, day))
			continue
		}

		tradeCount := binary.LittleEndian.Uint32(blobHeader[4:8])
		lTrades += int64(tradeCount)

		// Full Checksum (Expensive but necessary for 'Sanity')
		// Rewind to read full blob
		if _, err := fData.Seek(offset, io.SeekStart); err != nil {
			lCorrupt++
			continue
		}

		// Safety check on length alloc
		if length > 256*1024*1024 { // Cap at 256MB per day chunk for sanity
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Huge Blob (%d MB): %s Day %d", length/1024/1024, dirPath, day))
			continue
		}

		blob := make([]byte, int(length))
		if _, err := io.ReadFull(fData, blob); err != nil {
			lCorrupt++
			continue
		}

		sum := sha256.Sum256(blob)
		if binary.LittleEndian.Uint64(sum[:8]) != expSum {
			lCorrupt++
			lErrors = append(lErrors, fmt.Sprintf("Checksum Mismatch: %s Day %d", dirPath, day))
		}
	}

	// 4. Gap Detection
	// Calculate valid days for this specific month/year
	expectedDays := daysInMonth(year, month)
	var missing []string

	// Don't check gaps for the current (incomplete) month if it matches today's month
	now := time.Now()
	isCurrentMonth := (now.Year() == year && int(now.Month()) == month)

	limit := expectedDays
	if isCurrentMonth {
		limit = now.Day() - 1 // Expect up to yesterday
	}

	for d := 1; d <= limit; d++ {
		if !presentDays[d] {
			missing = append(missing, fmt.Sprintf("%04d-%02d-%02d", year, month, d))
		}
	}

	mergeReport(lTrades, lBytes, lDays, lCorrupt, lErrors, missing)
}

func mergeReport(trades, bytes int64, days, corrupt int, errs []string, missing []string) {
	report.Mu.Lock()
	defer report.Mu.Unlock()

	report.TotalTrades += trades
	report.TotalBytes += bytes
	report.TotalDays += days
	report.CorruptFiles += corrupt
	if len(errs) > 0 {
		report.Errors = append(report.Errors, errs...)
	}
	if len(missing) > 0 {
		report.MissingDays = append(report.MissingDays, missing...)
	}
}

// --- Helpers ---

func daysInMonth(year, month int) int {
	// Days in month lookup
	if month == 2 {
		if isLeap(year) {
			return 29
		}
		return 28
	}
	if month == 4 || month == 6 || month == 9 || month == 11 {
		return 30
	}
	return 31
}

func isLeap(year int) bool {
	return year%4 == 0 && (year%100 != 0 || year%400 == 0)
}

func printSummary(duration time.Duration) {
	fmt.Println("\n=======================================================")
	fmt.Printf("   DATA INTEGRITY REPORT (%s)   \n", Symbol())
	fmt.Println("=======================================================")

	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	fmt.Fprintf(w, "Scan Duration:\t%s\n", duration)
	fmt.Fprintf(w, "Months Scanned:\t%d\n", report.TotalMonths)
	fmt.Fprintf(w, "Total Days:\t%d\n", report.TotalDays)
	fmt.Fprintf(w, "Total Trades:\t%s\n", fmtHumanInt(report.TotalTrades))
	fmt.Fprintf(w, "Total Size:\t%s\n", fmtHumanBytes(report.TotalBytes))
	fmt.Fprintf(w, "Corrupt files:\t%d\n", report.CorruptFiles)

	gapCount := len(report.MissingDays)
	fmt.Fprintf(w, "Missing Days:\t%d\n", gapCount)

	w.Flush()
	fmt.Println("-------------------------------------------------------")

	if gapCount > 0 {
		fmt.Println("GAPS FOUND (First 10):")
		sort.Strings(report.MissingDays)
		for i, gap := range report.MissingDays {
			if i >= 10 {
				fmt.Printf("... and %d more\n", gapCount-10)
				break
			}
			fmt.Printf(" - [MISSING] %s\n", gap)
		}
		fmt.Println("-------------------------------------------------------")
	}

	if len(report.Errors) > 0 {
		fmt.Println("CRITICAL ERRORS (First 10):")
		for i, err := range report.Errors {
			if i >= 10 {
				fmt.Printf("... and %d more\n", len(report.Errors)-10)
				break
			}
			fmt.Printf(" - [FAIL] %s\n", err)
		}
	} else if report.CorruptFiles == 0 && gapCount == 0 {
		fmt.Println(">> STATUS: GREEN (100% Integrity) <<")
	} else {
		fmt.Println(">> STATUS: AMBER (Gaps or Corruption Detected) <<")
	}
	fmt.Println("=======================================================")
}

func fmtHumanInt(n int64) string {
	s := fmt.Sprintf("%d", n)
	if n < 1000 {
		return s
	}
	// Simple comma insertion
	var res []byte
	for i, c := range s {
		if i > 0 && (len(s)-i)%3 == 0 {
			res = append(res, ',')
		}
		res = append(res, byte(c))
	}
	return string(res)
}

func fmtHumanBytes(b int64) string {
	const unit = 1024
	if b < unit {
		return fmt.Sprintf("%d B", b)
	}
	div, exp := int64(unit), 0
	for n := b / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.2f %cB", float64(b)/float64(div), "KMGTPE"[exp])
}
```

// --- End File: sanity.go ---

// --- File: study.go ---

```go
package main

import (
	"encoding/binary"
	"fmt"
	"io"
	"iter"
	"math"
	"os"
	"path/filepath"
	"slices"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"text/tabwriter"
	"time"
	"unsafe"
)

// --- Configuration ---

const (
	OOSDateStr     = "2024-01-01"
	NumBuckets     = 5
	QuantileStride = 10
)

var TimeHorizonsMS = []int{500, 1000, 2000, 5000, 10000}
var oosBoundaryYMD int

func init() {
	oosBoundaryYMD = parseOOSBoundary(OOSDateStr)
}

type DayResult struct {
	YMD       int
	Metrics   map[string][]Moments
	Quantiles map[string]map[int][]BucketResult
}

// --- Main Logic ---

func runStudy() {
	startT := time.Now()

	found := false
	for sym := range discoverFeatureSymbols() {
		found = true
		studySymbol(sym)
	}

	if !found {
		fmt.Printf("[study] No features found in %s/features\n", BaseDir)
	} else {
		fmt.Printf("[study] ALL COMPLETE in %s\n", time.Since(startT))
	}
}

func discoverFeatureSymbols() iter.Seq[string] {
	return func(yield func(string) bool) {
		featDir := filepath.Join(BaseDir, "features")
		entries, err := os.ReadDir(featDir)
		if err != nil {
			fmt.Printf("[study] ReadDir(%s): %v\n", featDir, err)
			return
		}
		for _, e := range entries {
			if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
				if !yield(e.Name()) {
					return
				}
			}
		}
	}
}

func studySymbol(sym string) {
	fmt.Printf("\n>>> STUDY: %s <<<\n", sym)
	featRoot := filepath.Join(BaseDir, "features", sym)

	entries, err := os.ReadDir(featRoot)
	if err != nil {
		return
	}
	var variants []string
	for _, e := range entries {
		if e.IsDir() && !strings.HasPrefix(e.Name(), ".") {
			variants = append(variants, e.Name())
		}
	}
	slices.Sort(variants)
	if len(variants) == 0 {
		return
	}

	var tasks []int
	for d := range discoverStudyDays(filepath.Join(featRoot, variants[0])) {
		tasks = append(tasks, d)
	}
	totalTasks := len(tasks)
	fmt.Printf("Variants: %d | Days: %d\n", len(variants), totalTasks)

	isAcc := make(map[string][]Moments)
	oosAcc := make(map[string][]Moments)
	isDailyIC := make(map[string]map[int][]float64)
	oosDailyIC := make(map[string]map[int][]float64)
	isBuckets := make(map[string]map[int][]BucketAgg)
	oosBuckets := make(map[string]map[int][]BucketAgg)

	var accMu sync.Mutex
	resultsChan := make(chan DayResult, 64)
	jobsChan := make(chan int, len(tasks))
	var wg sync.WaitGroup

	var completed atomic.Int64
	doneChan := make(chan bool)

	go func() {
		ticker := time.NewTicker(500 * time.Millisecond)
		defer ticker.Stop()
		start := time.Now()
		for {
			select {
			case <-doneChan:
				printProgress(int(completed.Load()), totalTasks, start)
				fmt.Println()
				return
			case <-ticker.C:
				printProgress(int(completed.Load()), totalTasks, start)
			}
		}
	}()

	for i := 0; i < CPUThreads; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			var sigBuf []float64
			var fileBuf []byte
			var retBuf []float64
			retsPerHBuf := make([][]float64, len(TimeHorizonsMS))
			var gncBuf []byte

			for idx := range jobsChan {
				dayInt := tasks[idx]
				doQuantiles := dayInt < oosBoundaryYMD
				res := processStudyDay(
					sym, dayInt, variants, featRoot,
					&sigBuf, &fileBuf, &retBuf, &retsPerHBuf, &gncBuf,
					doQuantiles,
				)
				resultsChan <- res
				completed.Add(1)
			}
		}()
	}

	for i := range tasks {
		jobsChan <- i
	}
	close(jobsChan)

	go func() {
		wg.Wait()
		close(resultsChan)
		close(doneChan)
	}()

	isDays, oosDays := 0, 0
	for res := range resultsChan {
		if len(res.Metrics) == 0 {
			continue
		}
		isOOS := res.YMD >= oosBoundaryYMD
		if isOOS {
			oosDays++
		} else {
			isDays++
		}

		accMu.Lock()
		for vName, moms := range res.Metrics {
			if _, ok := isAcc[vName]; !ok {
				isAcc[vName] = make([]Moments, len(TimeHorizonsMS))
				oosAcc[vName] = make([]Moments, len(TimeHorizonsMS))
				isDailyIC[vName] = make(map[int][]float64)
				oosDailyIC[vName] = make(map[int][]float64)
				isBuckets[vName] = make(map[int][]BucketAgg)
				oosBuckets[vName] = make(map[int][]BucketAgg)
			}

			tMoments := isAcc[vName]
			tDailyIC := isDailyIC[vName]
			tBuckets := isBuckets[vName]
			if isOOS {
				tMoments = oosAcc[vName]
				tDailyIC = oosDailyIC[vName]
				tBuckets = oosBuckets[vName]
			}

			for hIdx := range TimeHorizonsMS {
				m := moms[hIdx]
				if m.Count <= 0 {
					continue
				}
				tMoments[hIdx].Add(m)

				num := m.Count*m.SumProd - m.SumSig*m.SumRet
				denX := m.Count*m.SumSqSig - m.SumSig*m.SumSig
				denY := m.Count*m.SumSqRet - m.SumRet*m.SumRet
				ic := 0.0
				if denX > 0 && denY > 0 {
					ic = num / math.Sqrt(denX*denY)
				}
				tDailyIC[hIdx] = append(tDailyIC[hIdx], ic)

				if qMap, ok := res.Quantiles[vName]; ok {
					if qList, ok2 := qMap[hIdx]; ok2 {
						if len(tBuckets[hIdx]) == 0 {
							tBuckets[hIdx] = make([]BucketAgg, NumBuckets)
						}
						for i, bucket := range qList {
							if i < NumBuckets {
								tBuckets[hIdx][i].Add(bucket)
							}
						}
					}
				}
			}
		}
		accMu.Unlock()
	}

	var finalKeys []string
	for k := range isAcc {
		finalKeys = append(finalKeys, k)
	}
	sort.Strings(finalKeys)

	for hIdx, ms := range TimeHorizonsMS {
		printHorizonTable(ms, finalKeys, isAcc, oosAcc, isDailyIC, oosDailyIC, hIdx, isDays, oosDays)
		printMonotonicityTable(ms, finalKeys, isBuckets, hIdx)
		fmt.Println()
	}
}

func processStudyDay(
	sym string, dayInt int, variants []string, featRoot string,
	sigBuf *[]float64, fileBuf *[]byte, retBuf *[]float64,
	retsPerH *[][]float64, gncBuf *[]byte,
	doQuantiles bool,
) DayResult {

	y := dayInt / 10000
	m := (dayInt % 10000) / 100
	d := dayInt % 100

	res := DayResult{
		YMD:       dayInt,
		Metrics:   make(map[string][]Moments),
		Quantiles: make(map[string]map[int][]BucketResult),
	}

	colsAny := DayColumnPool.Get()
	cols := colsAny.(*DayColumns)
	cols.Reset()
	defer DayColumnPool.Put(cols)

	rowCount, ok := loadDayColumns(sym, y, m, d, cols, gncBuf)
	if !ok || rowCount == 0 {
		return res
	}
	n := rowCount

	p := cols.Prices
	tm := cols.Times
	dStr := fmt.Sprintf("%04d%02d%02d", y, m, d)

	featureNames := []string{
		"f01_OFI", "f02_TCI", "f03_Whale", "f04_Lumpiness",
		"f05_Sweep", "f06_Fragility", "f07_Magnet",
		"f08_Velocity", "f09_Accel", "f10_Gap",
		"f11_DGT", "f12_Absorb", "f13_Fractal",
	}

	for hIdx, ms := range TimeHorizonsMS {
		computeReturns(p, tm, n, ms, retBuf)
		target := (*retsPerH)[hIdx]
		if cap(target) < n {
			target = make([]float64, n+n/4)
			(*retsPerH)[hIdx] = target
		}
		target = target[:n]
		copy(target, (*retBuf)[:n])
	}

	for _, v := range variants {
		sigPath := filepath.Join(featRoot, v, dStr+".bin")

		rawSigs, byteSize, ok := fastLoadBytes(sigPath, fileBuf)
		if !ok || byteSize == 0 {
			continue
		}

		dims := byteSize / (n * FeatBytes)
		if dims < 1 || dims > FeatDims {
			continue
		}

		if n > cap(*sigBuf) {
			*sigBuf = make([]float64, n+n/4)
		}

		for dim := 0; dim < dims; dim++ {
			target := (*sigBuf)[:n]

			decodeFeatureDim(rawSigs, n, dims, dim, target)

			key := v
			if dims > 1 {
				suffix := fmt.Sprintf("_d%d", dim+1)
				if dim < len(featureNames) {
					suffix = "_" + featureNames[dim]
				}
				key = v + suffix
			}

			moms := make([]Moments, len(TimeHorizonsMS))
			var qMap map[int][]BucketResult
			if doQuantiles {
				qMap = make(map[int][]BucketResult)
			}

			for hIdx := range TimeHorizonsMS {
				rets := (*retsPerH)[hIdx][:n]
				moms[hIdx] = CalcMomentsVectors(target, rets)
				if doQuantiles {
					qMap[hIdx] = ComputeQuantilesStrided(target, rets, NumBuckets, QuantileStride)
				}
			}

			res.Metrics[key] = moms
			if doQuantiles && len(qMap) > 0 {
				res.Quantiles[key] = qMap
			}
		}
	}
	return res
}

func discoverStudyDays(vDir string) iter.Seq[int] {
	return func(yield func(int) bool) {
		files, err := os.ReadDir(vDir)
		if err != nil {
			return
		}
		var days []int
		for _, f := range files {
			if strings.HasSuffix(f.Name(), ".bin") {
				if val := fastAtoi(strings.TrimSuffix(f.Name(), ".bin")); val > 0 {
					days = append(days, val)
				}
			}
		}
		sort.Ints(days)
		for _, d := range days {
			if !yield(d) {
				return
			}
		}
	}
}

func loadDayColumns(sym string, y, m, d int, cols *DayColumns, gncBuf *[]byte) (int, bool) {
	dir := filepath.Join(BaseDir, sym, fmt.Sprintf("%04d", y), fmt.Sprintf("%02d", m))
	idxPath := filepath.Join(dir, "index.quantdev")
	dataPath := filepath.Join(dir, "data.quantdev")

	offset, length := findBlobOffset(idxPath, d)
	if length == 0 {
		return 0, false
	}

	f, err := os.Open(dataPath)
	if err != nil {
		return 0, false
	}
	defer f.Close()

	stat, err := f.Stat()
	if err != nil {
		return 0, false
	}

	off64 := int64(offset)
	len64 := int64(length)

	if off64 < 0 || len64 <= 0 || off64+len64 > stat.Size() {
		return 0, false
	}
	// Sanity cap: 512MB
	if len64 > 512*1024*1024 {
		return 0, false
	}

	need := int(len64)
	if cap(*gncBuf) < need {
		*gncBuf = make([]byte, need)
	}
	raw := (*gncBuf)[:need]

	if _, err := f.Seek(off64, io.SeekStart); err != nil {
		return 0, false
	}
	if _, err := io.ReadFull(f, raw); err != nil {
		return 0, false
	}

	return inflateGNCToColumns(raw, cols)
}

func findBlobOffset(idxPath string, day int) (uint64, uint64) {
	f, err := os.Open(idxPath)
	if err != nil {
		return 0, 0
	}
	defer f.Close()

	var hdr [16]byte
	if _, err := io.ReadFull(f, hdr[:]); err != nil {
		return 0, 0
	}
	if string(hdr[0:4]) != IdxMagic {
		return 0, 0
	}
	count := binary.LittleEndian.Uint64(hdr[8:])
	var row [26]byte
	for i := uint64(0); i < count; i++ {
		if _, err := io.ReadFull(f, row[:]); err != nil {
			break
		}
		if int(binary.LittleEndian.Uint16(row[0:])) == day {
			return binary.LittleEndian.Uint64(row[2:]), binary.LittleEndian.Uint64(row[10:])
		}
	}
	return 0, 0
}

func fastLoadBytes(path string, fileBuf *[]byte) ([]byte, int, bool) {
	f, err := os.Open(path)
	if err != nil {
		return nil, 0, false
	}
	defer f.Close()
	fi, err := f.Stat()
	if err != nil {
		return nil, 0, false
	}
	size := int(fi.Size())
	if size == 0 {
		return nil, 0, false
	}
	if cap(*fileBuf) < size {
		*fileBuf = make([]byte, size)
	}
	buf := (*fileBuf)[:size]
	if _, err := io.ReadFull(f, buf); err != nil {
		return nil, 0, false
	}
	return buf, size, true
}

func decodeFeatureDim(raw []byte, n, dims, dim int, out []float64) {
	// FIX: Added explicit bounds check fallback
	minBytes := n * dims * FeatBytes
	if len(raw) < minBytes || dim < 0 || dim >= dims {
		for i := 0; i < n; i++ {
			offset := (i*dims + dim) * FeatBytes
			if offset+4 > len(raw) {
				out[i] = 0
				continue
			}
			bits := binary.LittleEndian.Uint32(raw[offset:])
			out[i] = float64(math.Float32frombits(bits))
		}
		return
	}

	f32s := unsafe.Slice((*float32)(unsafe.Pointer(&raw[0])), len(raw)/4)
	for i := 0; i < n; i++ {
		out[i] = float64(f32s[i*dims+dim])
	}
}

func computeReturns(p []float64, tm []int64, n int, horizonMS int, outBuf *[]float64) {
	if n > cap(*outBuf) {
		*outBuf = make([]float64, n+n/4)
	}
	outSlice := (*outBuf)[:n]
	hVal := int64(horizonMS)
	right := 0
	for left := 0; left < n; left++ {
		targetTime := tm[left] + hVal
		if right < left {
			right = left
		}
		for right < n && tm[right] < targetTime {
			right++
		}
		if right >= n {
			for k := left; k < n; k++ {
				outSlice[k] = 0
			}
			return
		}
		pStart := p[left]
		pEnd := p[right]
		if pStart > 0 {
			outSlice[left] = (pEnd - pStart) / pStart
		} else {
			outSlice[left] = 0
		}
	}
}

func parseOOSBoundary(d string) int {
	return fastAtoi(d[0:4])*10000 + fastAtoi(d[5:7])*100 + fastAtoi(d[8:10])
}

func fastAtoi(s string) int {
	n := 0
	for i := 0; i < len(s); i++ {
		c := s[i]
		if c >= '0' && c <= '9' {
			n = n*10 + int(c-'0')
		}
	}
	return n
}

func printProgress(curr, total int, start time.Time) {
	if total == 0 {
		return
	}
	const barWidth = 40
	percent := float64(curr) / float64(total)
	if percent > 1.0 {
		percent = 1.0
	}
	filled := int(percent * float64(barWidth))
	empty := barWidth - filled
	bar := strings.Repeat("=", filled) + strings.Repeat("-", empty)
	if filled > 0 && filled < barWidth {
		bar = bar[:filled-1] + ">" + bar[filled:]
	}
	elapsed := time.Since(start).Seconds()
	rate := 0.0
	if elapsed > 0 {
		rate = float64(curr) / elapsed
	}
	fmt.Printf("\r[%s] %.1f%% (%d/%d) | %.1f days/s  ", bar, percent*100, curr, total, rate)
}

func printHorizonTable(hMS int, keys []string, isAcc, oosAcc map[string][]Moments, isDailyIC, oosDailyIC map[string]map[int][]float64, hIdx, isDays, oosDays int) {
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 2, ' ', 0)
	sec := float64(hMS) / 1000.0
	fmt.Fprintf(w, "== Horizon %.3fs [IS: %d | OOS: %d] ==\n", sec, isDays, oosDays)
	fmt.Fprintln(w, "FEATURE\tIS_IC\tIS_T\tOOS_IC\tOOS_T\tAC1\t|AC1|\tAVG_SEG\tMAX_SEG\tIS_BPS/TR\tOOS_BPS/TR")
	for _, k := range keys {
		var isICSlice, oosICSlice []float64
		if m, ok := isDailyIC[k]; ok {
			isICSlice = m[hIdx]
		}
		if m, ok := oosDailyIC[k]; ok {
			oosICSlice = m[hIdx]
		}
		isStats := FinalizeMetrics(isAcc[k][hIdx], isICSlice)
		oosStats := FinalizeMetrics(oosAcc[k][hIdx], oosICSlice)
		fmt.Fprintf(w, "%s\t%.4f\t%.2f\t%.4f\t%.2f\t%.3f\t%.3f\t%.2f\t%.1f\t%.2f\t%.2f\n",
			k,
			isStats.ICPearson, isStats.IC_TStat,
			oosStats.ICPearson, oosStats.IC_TStat,
			isStats.AutoCorr, isStats.AutoCorrAbs,
			isStats.AvgSegLen, isStats.MaxSegLen,
			isStats.BreakevenBps, oosStats.BreakevenBps,
		)
	}
	w.Flush()
}

func printMonotonicityTable(hMS int, keys []string, isBuckets map[string]map[int][]BucketAgg, hIdx int) {
	sec := float64(hMS) / 1000.0
	fmt.Printf("\n-- Monotonicity Check (IS) Horizon %.3fs --\n", sec)
	w := tabwriter.NewWriter(os.Stdout, 0, 0, 1, ' ', 0)
	fmt.Fprintln(w, "FEATURE\tMONO\tB1(Sell)\tB2\tB3\tB4\tB5(Buy)")
	for _, k := range keys {
		aggs, ok := isBuckets[k][hIdx]
		if !ok || len(aggs) < NumBuckets {
			continue
		}
		brets := make([]float64, NumBuckets)
		for i := 0; i < NumBuckets; i++ {
			br := aggs[i].Finalize(i + 1)
			brets[i] = br.AvgRetBps
		}
		mono := bucketMonotonicity(brets)
		fmt.Fprintf(w, "%s\t%.3f", k, mono)
		for i := 0; i < NumBuckets; i++ {
			fmt.Fprintf(w, "\t%.1f", brets[i])
		}
		fmt.Fprintln(w, "")
	}
	w.Flush()
}

func bucketMonotonicity(rets []float64) float64 {
	n := len(rets)
	if n == 0 {
		return 0
	}
	var sumX, sumY, sumXY, sumX2, sumY2 float64
	nf := float64(n)
	for i := 0; i < n; i++ {
		x := float64(i + 1)
		y := rets[i]
		sumX += x
		sumY += y
		sumXY += x * y
		sumX2 += x * x
		sumY2 += y * y
	}
	num := nf*sumXY - sumX*sumY
	denX := nf*sumX2 - sumX*sumX
	denY := nf*sumY2 - sumY*sumY
	if denX <= 0 || denY <= 0 {
		return 0
	}
	return num / math.Sqrt(denX*denY)
}
```

// --- End File: study.go ---

